{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chp11_part02_sequence_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdJZBSz4ZDFM"
      },
      "source": [
        "# Chp11: Part 2 Sequence Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwlcxspiZDFP"
      },
      "source": [
        "### 11.3.3 Processing words as a sequence: the Sequence Model approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHG_JLkuG_nc"
      },
      "source": [
        "def HR():\n",
        "    # print char * numeric\n",
        "    print('-' * 80)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAlblBTx9hmo",
        "outputId": "c32570a5-8e75-416e-9420-286fca49087e"
      },
      "source": [
        "# Downloading the GloVe word embeddings\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/MyDrive/data"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yHxyMQZZDFR",
        "outputId": "a680af2b-a26d-4ca7-f2af-57d3cdf75159"
      },
      "source": [
        "def listing11_12():\n",
        "    import os\n",
        "\n",
        "    dirpath = 'aclImdb'\n",
        "    if not os.path.isdir(dirpath):\n",
        "        print(f'{dirpath} not found, creating directory')\n",
        "        HR()\n",
        "        try:\n",
        "            !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "            !tar -xf aclImdb_v1.tar.gz\n",
        "            !rm -r aclImdb/train/unsup\n",
        "        except Exception as ex:\n",
        "            print(f\"Not able to create directory due to error {ex}\")\n",
        "            \n",
        "listing11_12()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Finished creating aclImdb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtusvntPLSZk"
      },
      "source": [
        "# dict\n",
        "options = {\n",
        "    'batch_size': 32,\n",
        "    'max_length' : 600,\n",
        "    'max_tokens' : 20_000\n",
        "}\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJL-lF01ZDFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1abd98b-b934-492c-ccce-8e2060e657bd"
      },
      "source": [
        "def listing11_13():\n",
        "    import os, pathlib, shutil, random\n",
        "    from tensorflow import keras\n",
        "\n",
        "    dirpath = 'aclImdb/val'\n",
        "    if os.path.isdir(dirpath):\n",
        "        print(f\"{dirpath} already exists\")\n",
        "    else:\n",
        "        print(f\"Prepare a validation set by setting apart 20% of the training text files in a new directory, {dirpath}\")\n",
        "        base_dir = pathlib.Path(\"aclImdb\")\n",
        "        val_dir = base_dir / \"val\"\n",
        "        train_dir = base_dir / \"train\"\n",
        "        for category in (\"neg\", \"pos\"):\n",
        "            os.makedirs(val_dir / category, exist_ok=True)\n",
        "            files = os.listdir(train_dir / category)\n",
        "\n",
        "            # Shuffle the list of training files using a seed, to ensure\n",
        "            # we get the same validation set every time we run the code\n",
        "            random.Random(1337).shuffle(files)\n",
        "\n",
        "            # Take 20% of the training files to use for validation\n",
        "            num_val_samples = int(0.2 * len(files))\n",
        "            val_files = files[-num_val_samples:]\n",
        "\n",
        "            # Move the files to aclImdb/val/neg and aclImdb/val/pos\n",
        "            for fname in val_files:\n",
        "                shutil.move(train_dir / category / fname,\n",
        "                            val_dir / category / fname)\n",
        "\n",
        "# This should be its own function, since the action is conditional\n",
        "# and we want to be able to treat it as so via control-flow eventually\n",
        "\n",
        "listing11_13()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb/val already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yU0tSP-KyVE",
        "outputId": "e7fa95b2-5e05-4c15-f532-8c4e1923b359"
      },
      "source": [
        "def listing11_14():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/train\", batch_size=options['batch_size']\n",
        "    )\n",
        "    val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/val\", batch_size=options['batch_size']\n",
        "    )\n",
        "    test_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/test\", batch_size=options['batch_size']\n",
        "    )\n",
        "    text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "\n",
        "    # Preparing integer sequence datasets\n",
        "    text_vectorization = layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=options['max_tokens'],\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=options['max_length'],\n",
        "    )\n",
        "    text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "    int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "    int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "    int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "    HR()\n",
        "    print(type(int_train_ds))\n",
        "    print(int_train_ds)\n",
        "    HR()\n",
        "    return int_train_ds, int_val_ds, int_test_ds\n",
        "\n",
        "int_train_ds, int_val_ds, int_test_ds = listing11_14()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
            "<MapDataset shapes: ((None, 600), (None,)), types: (tf.int64, tf.int32)>\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKUeUCO5Kphf"
      },
      "source": [
        "# This runs VERY SLOWLY\n",
        "def listing11_15(int_train_ds, int_val_ds, int_test_ds):\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    # A sequence model built on top of one-hot encoded vector sequences\n",
        "    # import tensorflow as tf\n",
        "\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    embedded = tf.one_hot(inputs, depth=options['max_tokens'])\n",
        "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    \n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # Training a first basic sequence model\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                        save_best_only=True)\n",
        "    ]\n",
        "    model.fit(\n",
        "        int_train_ds, \n",
        "        validation_data=int_val_ds, \n",
        "        #epochs=10, \n",
        "        epochs=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "    print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "# listing11_15(int_train_ds, int_val_ds, int_test_ds)\n",
        "\n",
        "# 625/625 [==============================] - 482s 766ms/step - loss: 0.5439 - accuracy: 0.7372 - val_loss: 0.7386 - val_accuracy: 0.7542\n",
        "# 782/782 [==============================] - 338s 431ms/step - loss: 0.7305 - accuracy: 0.7550\n",
        "# Test acc: 0.755\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_3Z7veDHGHj",
        "outputId": "936776ec-395e-46f5-acfd-82ed261345dd"
      },
      "source": [
        "# Listing 11.18 Model that uses an Embedding layer trained from scratch\n",
        "# This should train much faster than the one-hot model (since the LSTM only \n",
        "# has to process 256-dimensional vectors instead of 20,000-dimensional).\n",
        "def listing11_18(int_train_ds, int_val_ds, int_test_ds):\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    # Understanding word embeddings\n",
        "    # Learning word embeddings with the Embedding layer\n",
        "    # Instantiating an Embedding layer\n",
        "    embedding_layer = layers.Embedding(\n",
        "        input_dim=options['max_tokens'], \n",
        "        output_dim=256\n",
        "    )\n",
        "\n",
        "    # Model that uses an Embedding layer trained from scratch\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    embedded = layers.Embedding(input_dim=options['max_tokens'], output_dim=256)(inputs)\n",
        "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            \"embeddings_bidir_gru.keras\",\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        int_train_ds, \n",
        "        validation_data=int_val_ds, \n",
        "        #epochs=10, \n",
        "        epochs=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "    print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "\n",
        "listing11_18(int_train_ds, int_val_ds, int_test_ds)\n",
        "\n",
        "# 625/625 [==============================] - 162s 253ms/step - loss: 0.4543 - accuracy: 0.8001 - val_loss: 0.3996 - val_accuracy: 0.8496\n",
        "# 782/782 [==============================] - 72s 91ms/step - loss: 0.4129 - accuracy: 0.8429\n",
        "# Test acc: 0.843\n",
        "\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# input_7 (InputLayer)         [(None, None)]            0         \n",
        "# _________________________________________________________________\n",
        "# embedding_8 (Embedding)      (None, None, 256)         5120000   \n",
        "# _________________________________________________________________\n",
        "# bidirectional_6 (Bidirection (None, 64)                73984     \n",
        "# _________________________________________________________________\n",
        "# dropout_6 (Dropout)          (None, 64)                0         \n",
        "# _________________________________________________________________\n",
        "# dense_6 (Dense)              (None, 1)                 65        \n",
        "# =================================================================\n",
        "# Total params: 5,194,049\n",
        "# Trainable params: 5,194,049\n",
        "# Non-trainable params: 0\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_8 (Embedding)      (None, None, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 64)                73984     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "625/625 [==============================] - 420s 668ms/step - loss: 0.4641 - accuracy: 0.7930 - val_loss: 0.3617 - val_accuracy: 0.8738\n",
            "782/782 [==============================] - 106s 134ms/step - loss: 0.3825 - accuracy: 0.8596\n",
            "Test acc: 0.860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOFkvWceP4LG",
        "outputId": "f3de49e0-27cf-4121-c23c-c20e80d3ddd4"
      },
      "source": [
        "#  Listing 11.19 Model that uses an Embedding layer trained from scratch, with masking enabled\n",
        "def listing11_19(int_train_ds, int_val_ds, int_test_ds):\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    # Understanding padding & masking\n",
        "    # Model that uses an Embedding layer trained from scratch, with masking enabled\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    embedded = layers.Embedding(\n",
        "        input_dim=options['max_tokens'], output_dim=256, mask_zero=True)(inputs)\n",
        "    \n",
        "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                        save_best_only=True)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        int_train_ds, \n",
        "        validation_data=int_val_ds, \n",
        "        #epochs=10, \n",
        "        epochs=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "    print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")    \n",
        "\n",
        "listing11_19(int_train_ds, int_val_ds, int_test_ds)\n",
        "\n",
        "# 625/625 [==============================] - 200s 262ms/step - loss: 0.4168 - accuracy: 0.8129 - val_loss: 0.3127 - val_accuracy: 0.8696\n",
        "# 782/782 [==============================] - 79s 97ms/step - loss: 0.3221 - accuracy: 0.8604\n",
        "# Test acc: 0.860\n",
        "\n",
        "# _________________________________________________________________\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# input_8 (InputLayer)         [(None, None)]            0         \n",
        "# _________________________________________________________________\n",
        "# embedding_9 (Embedding)      (None, None, 256)         5120000   \n",
        "# _________________________________________________________________\n",
        "# bidirectional_7 (Bidirection (None, 64)                73984     \n",
        "# _________________________________________________________________\n",
        "# dropout_7 (Dropout)          (None, 64)                0         \n",
        "# _________________________________________________________________\n",
        "# dense_7 (Dense)              (None, 1)                 65        \n",
        "# =================================================================\n",
        "# Total params: 5,194,049\n",
        "# Trainable params: 5,194,049\n",
        "# Non-trainable params: 0"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_9 (Embedding)      (None, None, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 64)                73984     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "625/625 [==============================] - 543s 857ms/step - loss: 0.4053 - accuracy: 0.8130 - val_loss: 0.2981 - val_accuracy: 0.8686\n",
            "782/782 [==============================] - 132s 166ms/step - loss: 0.3281 - accuracy: 0.8545\n",
            "Test acc: 0.854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1cH3_5AZDFc"
      },
      "source": [
        "---\n",
        "# Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jr2CjNLZDFc"
      },
      "source": [
        "# def listing11_20():\n",
        "#     import os\n",
        "\n",
        "#     filepath = \"glove.6B.100d.txt\"\n",
        "#     if not os.path.isfile(filepath):\n",
        "#         try:\n",
        "#             # OLD: https://nlp.stanford.edu/projects/glove/\n",
        "#             # 822 MB zip file\n",
        "#             #!wget https://web.archive.org/web/20181130213045/https://nlp.stanford.edu/data/glove.6B.zip --no-check-certificate\n",
        "#             !unzip -q glove.6B.zip\n",
        "#         except Exception as ex:\n",
        "#             print(f\"Encountered error: {ex}\")\n",
        "\n",
        "# listing11_20()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi9oFq885Jem",
        "outputId": "30308327-41a9-4138-c1d7-f276ce0c7ae0"
      },
      "source": [
        "# csv.QUOTE_MINIMAL means only when required, for example, when a\n",
        "#     field contains either the quotechar or the delimiter\n",
        "# csv.QUOTE_ALL means that quotes are always placed around fields.\n",
        "# csv.QUOTE_NONNUMERIC means that quotes are always placed around\n",
        "#     fields which do not parse as integers or floating point\n",
        "#     numbers.\n",
        "# csv.QUOTE_NONE means that quotes are never placed around fields.\n",
        "\n",
        "# Get error 'EOF inside string', as this line has a string that contains\n",
        "# within it a single quote mark:\n",
        "# \" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 \n",
        "# You have to add this line to fix it: quoting=csv.QUOTE_NONE\n",
        "\n",
        "# GB: Examining file contents with pandas\n",
        "\n",
        "data_pathway = '/content/drive/MyDrive/data/glove.6B.100d.txt'\n",
        "\n",
        "def test():\n",
        "    import pandas as pd\n",
        "    import csv\n",
        "\n",
        "    df = pd.read_csv(data_pathway, sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
        "    print(df.info())\n",
        "    print()\n",
        "    print(df.head().T)\n",
        "\n",
        "test()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400000 entries, 0 to 399999\n",
            "Columns: 101 entries, 0 to 100\n",
            "dtypes: float64(100), object(1)\n",
            "memory usage: 308.2+ MB\n",
            "None\n",
            "\n",
            "            0         1         2        3         4\n",
            "0         the         ,         .       of        to\n",
            "1   -0.038194  -0.10767  -0.33979  -0.1529   -0.1897\n",
            "2    -0.24487   0.11053   0.20941 -0.24279  0.050024\n",
            "3     0.72812   0.59812   0.46348  0.89837   0.19084\n",
            "4    -0.39961  -0.54361  -0.64792  0.16996 -0.049184\n",
            "..        ...       ...       ...      ...       ...\n",
            "96   -0.51058   0.61214   0.31802 -0.34839  -0.41548\n",
            "97   -0.52028  -0.35111  -0.39242 -0.56094 -0.038175\n",
            "98    -0.1459  -0.83155  -0.23394   -0.591  -0.39804\n",
            "99     0.8278   0.45293   0.47298   1.0039   0.47647\n",
            "100   0.27062  0.082577 -0.028803  0.20664  -0.15983\n",
            "\n",
            "[101 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ5Qq8XdZDFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ce1462-8eb1-4b54-a419-35ed8343f075"
      },
      "source": [
        "# Listing 11.20 Parsing the GloVe word-embeddings file\n",
        "def listing11_20():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    import numpy as np\n",
        "\n",
        "    data_pathway = '/content/drive/MyDrive/data/glove.6B.100d.txt'\n",
        "    path_to_glove_file = data_pathway\n",
        "\n",
        "    embeddings_index = {}\n",
        "    with open(path_to_glove_file) as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    print(f\"Found {len(embeddings_index)} word vectors.\")\n",
        "    print()\n",
        "\n",
        "    #####\n",
        "\n",
        "    # Setup\n",
        "    train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/train\", batch_size=options['batch_size']\n",
        "    )\n",
        "    val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/val\", batch_size=options['batch_size']\n",
        "    )\n",
        "    test_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/test\", batch_size=options['batch_size']\n",
        "    )\n",
        "    \n",
        "    HR()\n",
        "\n",
        "    text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "\n",
        "    # Preparing integer sequence datasets\n",
        "    text_vectorization = layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=options['max_tokens'],\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=options['max_length'],\n",
        "    )\n",
        "    text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "    int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "    int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "    int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "    ####\n",
        "\n",
        "    # Loading the GloVe embeddings in the model\n",
        "    # Preparing the GloVe word-embeddings matrix\n",
        "\n",
        "    embedding_dim = 100\n",
        "\n",
        "    vocabulary = text_vectorization.get_vocabulary()\n",
        "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "    embedding_matrix = np.zeros((options['max_tokens'], embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if i < options['max_tokens']:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "    embedding_layer = layers.Embedding(\n",
        "        options['max_tokens'],\n",
        "        embedding_dim,\n",
        "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "        trainable=False,\n",
        "        mask_zero=True,\n",
        "    )\n",
        "\n",
        "    #####\n",
        "\n",
        "    # Training a simple bidirectional LSTM on top of the GloVe embeddings\n",
        "    # Model that uses a pretrained Embedding layer\n",
        "\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    embedded = embedding_layer(inputs)\n",
        "    x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            \"glove_embeddings_sequence_model.keras\",\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    model.fit(\n",
        "        int_train_ds, \n",
        "        validation_data=int_val_ds, \n",
        "        #epochs=10, \n",
        "        epochs=1,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "    model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "    \n",
        "    print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "    #####\n",
        "    print(\"Done..\")\n",
        "\n",
        "listing11_20()\n",
        "\n",
        "# 625/625 [==============================] - 352s 552ms/step - loss: 0.5749 - accuracy: 0.6982 - val_loss: 0.4861 - val_accuracy: 0.7674\n",
        "# 782/782 [==============================] - 100s 125ms/step - loss: 0.4894 - accuracy: 0.7669\n",
        "# Test acc: 0.767\n",
        "\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# input_9 (InputLayer)         [(None, None)]            0         \n",
        "# _________________________________________________________________\n",
        "# embedding_10 (Embedding)     (None, None, 100)         2000000   \n",
        "# _________________________________________________________________\n",
        "# bidirectional_8 (Bidirection (None, 64)                34048     \n",
        "# _________________________________________________________________\n",
        "# dropout_8 (Dropout)          (None, 64)                0         \n",
        "# _________________________________________________________________\n",
        "# dense_8 (Dense)              (None, 1)                 65        \n",
        "# =================================================================\n",
        "# Total params: 2,034,113\n",
        "# Trainable params: 34,113\n",
        "# Non-trainable params: 2,000,000\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "--------------------------------------------------------------------------------\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_10 (Embedding)     (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 64)                34048     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "625/625 [==============================] - 357s 561ms/step - loss: 0.5757 - accuracy: 0.6897 - val_loss: 0.5552 - val_accuracy: 0.7080\n",
            "782/782 [==============================] - 102s 128ms/step - loss: 0.5578 - accuracy: 0.7037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFMKWA2BHN-V"
      },
      "source": [
        "### Up to Listing 11.22 Model that uses a pretrained Embedding layer, pg.382"
      ]
    }
  ]
}