{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chp11_part03_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrH3XT3Ma_-l"
      },
      "source": [
        "# Chp 11: 11.4 Part 3 The Transformer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXVleEeTVmmg",
        "outputId": "201e6bd0-9b50-4daa-af4b-a2f1e28c4796"
      },
      "source": [
        "def setup():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "\n",
        "    # every layer uses a 16-bit compute dtype and float32 variable dtype by default.\n",
        "    # most of the forward pass of the model will be done in float16,\n",
        "    # (with the exception of numerically unstable operations like softmax),\n",
        "    # while the weights of the model will be stored and updated in float32.\n",
        "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "    print(tf.keras.mixed_precision.global_policy())\n",
        "    \n",
        "setup()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Policy \"mixed_float16\">\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2ZhhTXAVdgW"
      },
      "source": [
        "def HR():\n",
        "    # print char * numeric\n",
        "    print('-' * 80)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OlAlFyPVKUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9dc36f-9077-43f2-8fff-e18f166b27bd"
      },
      "source": [
        "def listing11_12():\n",
        "    import os\n",
        "\n",
        "    dirpath = 'aclImdb'\n",
        "    if not os.path.isdir(dirpath):\n",
        "        print(f'{dirpath} not found, creating directory')\n",
        "        HR()\n",
        "        try:\n",
        "            !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "            !tar -xf aclImdb_v1.tar.gz\n",
        "            !rm -r aclImdb/train/unsup\n",
        "        except Exception as ex:\n",
        "            print(f\"Not able to create directory due to error {ex}\")\n",
        "            \n",
        "listing11_12()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb not found, creating directory\n",
            "--------------------------------------------------------------------------------\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  64.8M      0  0:00:01  0:00:01 --:--:-- 64.8M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NiI6LIiVkz0"
      },
      "source": [
        "# dict\n",
        "options = {\n",
        "    'batch_size': 32,\n",
        "    'max_length' : 600,\n",
        "    'max_tokens' : 20_000\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgeCwuMKVzZP",
        "outputId": "3711f984-8f2f-4db2-97e3-e0896d1e163e"
      },
      "source": [
        "# Preparing the data\n",
        "def listing11_13():\n",
        "    import os, pathlib, shutil, random\n",
        "    from tensorflow import keras\n",
        "\n",
        "    dirpath = 'aclImdb/val'\n",
        "    if os.path.isdir(dirpath):\n",
        "        print(f\"{dirpath} already exists\")\n",
        "    else:\n",
        "        print(f\"Prepare a validation set by setting apart 20% of the training text files in a new directory, {dirpath}\")\n",
        "        base_dir = pathlib.Path(\"aclImdb\")\n",
        "        val_dir = base_dir / \"val\"\n",
        "        train_dir = base_dir / \"train\"\n",
        "        for category in (\"neg\", \"pos\"):\n",
        "            os.makedirs(val_dir / category, exist_ok=True)\n",
        "            files = os.listdir(train_dir / category)\n",
        "\n",
        "            # Shuffle the list of training files using a seed, to ensure\n",
        "            # we get the same validation set every time we run the code\n",
        "            random.Random(1337).shuffle(files)\n",
        "\n",
        "            # Take 20% of the training files to use for validation\n",
        "            num_val_samples = int(0.2 * len(files))\n",
        "            val_files = files[-num_val_samples:]\n",
        "\n",
        "            # Move the files to aclImdb/val/neg and aclImdb/val/pos\n",
        "            for fname in val_files:\n",
        "                shutil.move(train_dir / category / fname,\n",
        "                            val_dir / category / fname)\n",
        "\n",
        "# This should be its own function, since the action is conditional\n",
        "# and we want to be able to treat it as so via control-flow eventually\n",
        "\n",
        "listing11_13()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepare a validation set by setting apart 20% of the training text files in a new directory, aclImdb/val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoXBjJPgWEAq",
        "outputId": "ac8d6022-2876-4774-c84e-d7fd5688257e"
      },
      "source": [
        "# Vectorizing the data\n",
        "def listing11_14():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/train\", batch_size=options['batch_size']\n",
        "    )\n",
        "    val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/val\", batch_size=options['batch_size']\n",
        "    )\n",
        "    test_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "        \"aclImdb/test\", batch_size=options['batch_size']\n",
        "    )\n",
        "    text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "\n",
        "\n",
        "    # Preparing integer sequence datasets\n",
        "    text_vectorization = layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=options['max_tokens'],\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=options['max_length'],\n",
        "    )\n",
        "    text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "    int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "    int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "    int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "    HR()\n",
        "    print(type(int_train_ds))\n",
        "    print(int_train_ds)\n",
        "    HR()\n",
        "    return int_train_ds, int_val_ds, int_test_ds\n",
        "\n",
        "int_train_ds, int_val_ds, int_test_ds = listing11_14()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "--------------------------------------------------------------------------------\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n",
            "<MapDataset shapes: ((None, 600), (None,)), types: (tf.int64, tf.int32)>\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spXem72qa_-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d62b04f-1f3a-439a-bd75-125c6c3896ef"
      },
      "source": [
        "# Listing 11.23 Transformer encoder implemented as a subclassed Layer\n",
        "# p. 392\n",
        "\n",
        "# The encoder part can be used for text classification — it’s a very generic \n",
        "# module that ingests a sequence and learns to turn it into a more useful \n",
        "# representation. Implement a Transformer encoder and try it on the movie \n",
        "# review sentiment classification task.\n",
        "\n",
        "def listing11_23():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    class TransformerEncoder(layers.Layer):\n",
        "        def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "\n",
        "            # Size of the input token vectors\n",
        "            self.embed_dim = embed_dim\n",
        "            # Size of the inner dense layer\n",
        "            self.dense_dim = dense_dim\n",
        "            # Number of attention heads\n",
        "            self.num_heads = num_heads\n",
        "            self.attention = layers.MultiHeadAttention(\n",
        "                num_heads=num_heads, key_dim=embed_dim)\n",
        "            self.dense_proj = keras.Sequential(\n",
        "                [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),]\n",
        "            )\n",
        "\n",
        "            # The normalization layers we’re using here aren’t BatchNormalization \n",
        "            # layers like those you’ve used before in image models. That’s \n",
        "            # because BatchNormalization doesn’t work well for sequence data. \n",
        "            # Instead, we’re using the LayerNormalization layer, which normalizes \n",
        "            # each sequence independently from other sequences in the batch.\n",
        "            self.layernorm_1 = layers.LayerNormalization()\n",
        "            self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "        # Computation goes in call\n",
        "        def call(self, inputs, mask=None):\n",
        "            # The mask that will be generated by the Embedding layer will be \n",
        "            # 2D, but the attention layer expects to be 3D or 4D, so we expand \n",
        "            # its rank.\n",
        "            if mask is not None:\n",
        "                mask = mask[:, tf.newaxis, :]\n",
        "            attention_output = self.attention(\n",
        "                inputs, inputs, attention_mask=mask)\n",
        "            proj_input = self.layernorm_1(inputs + attention_output)\n",
        "            proj_output = self.dense_proj(proj_input)\n",
        "            return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "        # Serialization so we can save the model\n",
        "        def get_config(self):\n",
        "            config = super().get_config()\n",
        "            config.update({\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"dense_dim\": self.dense_dim,\n",
        "            })\n",
        "            return config\n",
        "\n",
        "    return TransformerEncoder\n",
        "\n",
        "TransformerEncoder = listing11_23()\n",
        "\n",
        "print(type(TransformerEncoder))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'type'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t4uI0mT8Qt_"
      },
      "source": [
        "---\n",
        "\n",
        "This next section is about \"sequence models\". \n",
        "\n",
        "Word order is important, and the Transformer was a sequence-processing architecture, originally developed for machine translation. \n",
        "\n",
        "However, the Transformer encoder here isn't a sequence model at all.\n",
        "\n",
        "It’s composed of dense layers, which process sequence tokens independently from each other, and an attention layer, which looks at the tokens as a set. \n",
        "\n",
        "You could change the order of the tokens in a sequence, and you’d get the exact same pairwise attention scores and the exact same context-aware representations. \n",
        "\n",
        "If you were to completely scramble the words in every movie review, the model wouldn’t notice, and you’d still get the exact same accuracy. \n",
        "\n",
        "Self-attention is a set-processing mechanism, focused on the relationships between pairs of sequence elements (see figure 11.10) — it’s blind to whether these elements occur at the beginning, at the end, or in the middle of a sequence. \n",
        "\n",
        "So, why do we say that Transformer is a sequence model, then? And how could it possibly be good for machine translation if it doesn’t look at word order? \n",
        "\n",
        "The Transformer is a hybrid approach, that is technically order-agnostic, but that manually injects order information in the representations it processes. This is the missing ingredient! It’s called \"positional encoding\". It has both word-order awareness and context awareness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1t9ATcxa_-w",
        "outputId": "a6aa894e-41ae-42ee-8f41-56e70b324531"
      },
      "source": [
        "# Listing 11.24 Text classification model that combines the Transformer encoder \n",
        "# and a pooling layer\n",
        "# We can use the TransformerEncoder to assemble a text-classification model \n",
        "# similar to the GRU-based one seen previously.\n",
        "# p.394\n",
        "\n",
        "# This example is flawed, as noted above.\n",
        "\n",
        "def listing11_24():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    vocab_size = 20000\n",
        "    embed_dim = 256\n",
        "    num_heads = 2\n",
        "    dense_dim = 32\n",
        "\n",
        "\n",
        "    # NOTE: This Transformer encoder is NOT a sequence model at all.\n",
        "    # It’s composed of dense layers, which process sequence tokens \n",
        "    # independently from each other, and an attention layer, which looks at \n",
        "    # the tokens as a set. You could change the order of the tokens in a \n",
        "    # sequence, and you’d get the exact same pairwise attention scores and \n",
        "    # the exact same context-aware representations. If you were to completely \n",
        "    # scramble the words in every movie review, the model wouldn’t notice, \n",
        "    # and you’d still get the exact same accuracy. \n",
        "\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "    # Since TransformerEncoder returns full sequences, we need to reduce each \n",
        "    # sequence to a single vector for classification, via a global pooling layer.\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "    # Training and evaluating the Transformer encoder based model\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                        save_best_only=True)\n",
        "    ]\n",
        "    \n",
        "    model.fit(\n",
        "        int_train_ds, \n",
        "        validation_data=int_val_ds, \n",
        "        # epochs=20,\n",
        "        epochs=5, \n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "    model = keras.models.load_model(\n",
        "        \"transformer_encoder.keras\",\n",
        "        custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "    print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "listing11_24()\n",
        "\n",
        "\n",
        "# Model: \"model\"\n",
        "# _________________________________________________________________\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# input_1 (InputLayer)         [(None, None)]            0         \n",
        "# _________________________________________________________________\n",
        "# embedding (Embedding)        (None, None, 256)         5120000   \n",
        "# _________________________________________________________________\n",
        "# transformer_encoder (Transfo (None, None, 256)         543776    \n",
        "# _________________________________________________________________\n",
        "# global_max_pooling1d (Global (None, 256)               0         \n",
        "# _________________________________________________________________\n",
        "# dropout (Dropout)            (None, 256)               0         \n",
        "# _________________________________________________________________\n",
        "# dense_2 (Dense)              (None, 1)                 257       \n",
        "# =================================================================\n",
        "# Total params: 5,664,033\n",
        "# Trainable params: 5,664,033\n",
        "# Non-trainable params: 0\n",
        "# _________________________________________________________________\n",
        "# Epoch 1/5\n",
        "# 625/625 [==============================] - 45s 68ms/step - loss: 0.4814 - accuracy: 0.7789 - val_loss: 0.3244 - val_accuracy: 0.8620\n",
        "# Epoch 2/5\n",
        "# 625/625 [==============================] - 42s 67ms/step - loss: 0.3147 - accuracy: 0.8652 - val_loss: 0.2880 - val_accuracy: 0.8804\n",
        "# Epoch 3/5\n",
        "# 625/625 [==============================] - 41s 66ms/step - loss: 0.2370 - accuracy: 0.9049 - val_loss: 0.2856 - val_accuracy: 0.8880\n",
        "# Epoch 4/5\n",
        "# 625/625 [==============================] - 42s 67ms/step - loss: 0.1827 - accuracy: 0.9291 - val_loss: 0.3538 - val_accuracy: 0.8814\n",
        "# Epoch 5/5\n",
        "# 625/625 [==============================] - 41s 65ms/step - loss: 0.1511 - accuracy: 0.9452 - val_loss: 0.3209 - val_accuracy: 0.8860\n",
        "# 782/782 [==============================] - 19s 24ms/step - loss: 0.3057 - accuracy: 0.8763\n",
        "\n",
        "# Test acc: 0.876"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, None, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "transformer_encoder_4 (Trans (None, None, 256)         543776    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_4 (Glob (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,664,033\n",
            "Trainable params: 5,664,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 44s 67ms/step - loss: 0.4775 - accuracy: 0.7764 - val_loss: 0.3251 - val_accuracy: 0.8618\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.3188 - accuracy: 0.8666 - val_loss: 0.2675 - val_accuracy: 0.8906\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 42s 68ms/step - loss: 0.2512 - accuracy: 0.8977 - val_loss: 0.2588 - val_accuracy: 0.8878\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 41s 66ms/step - loss: 0.1978 - accuracy: 0.9232 - val_loss: 0.2721 - val_accuracy: 0.8950\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 41s 65ms/step - loss: 0.1623 - accuracy: 0.9390 - val_loss: 0.2820 - val_accuracy: 0.8956\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.2828 - accuracy: 0.8817\n",
            "Test acc: 0.882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATqlfys69L3i"
      },
      "source": [
        "---\n",
        "# USING POSITIONAL ENCODING TO REINJECT ORDER INFORMATION, p.395\n",
        "\n",
        "The idea behind positional encoding is very simple: to give the model access to word order information, we’re going to add to each word embedding the word’s position in the sentence. \n",
        "\n",
        "Our input word embeddings will have two components: \n",
        "\n",
        "1. The usual **word vector**, which represents the word independently of any specific context.\n",
        "\n",
        "2. The **position vector**, which represents the position of the word in the current sentence. Hopefully, the model will then figure out how to best leverage this additional information.\n",
        "\n",
        "The simplest scheme you could come up with would be to concatenate the word’s position to its embedding vector. You’d add a \"position\" axis to the vector, and fill it with 0 for the first word in the sequence, 1 for the second one, and so on.\n",
        "\n",
        "That may not be ideal, however, because your positions can potentially be very large integers, which will disrupt the range of values in the embedding vector. As you know, neural networks don’t like very large input values, or discrete input distributions.\n",
        "\n",
        "The original \"Attention is all you need paper\" used an interesting trick to encode word positions: it added to the word embeddings a vector containing values in the range [-1, 1] that varied cyclically depending on the position (it used cosine functions to achieve this). This trick offers a way to uniquely characterize any integer in a large range via of vector of small values. \n",
        "\n",
        "It’s clever, but it’s not what we’re going to use in our case. We’ll do something simpler and more effective: we’ll just learn position embedding vectors, just the same way we learn to embed word indices. We’ll then proceed to add our position embeddings to the corresponding word embeddings, to obtain a position-aware word embedding. This technique is called \"positional embedding\". Let’s implement it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzUJ4B6Ka_-z"
      },
      "source": [
        "# Using positional encoding to reinject order information\n",
        "# Implementing positional embedding as a subclassed layer\n",
        "# Listing 11.26 Implementing positional embedding as a subclassed layer\n",
        "\n",
        "def listing11_26():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    # Use this PositionEmbedding layer just like a regular Embedding layer.\n",
        "    class PositionalEmbedding(layers.Layer):\n",
        "        # A downside of position embeddings is that the sequence length needs to be known in advance.\n",
        "        def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            # Embedding layer for the token indices\n",
        "            self.token_embeddings = layers.Embedding(\n",
        "                input_dim=input_dim, output_dim=output_dim)\n",
        "            # Embedding layer for the token positions\n",
        "            self.position_embeddings = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=output_dim)\n",
        "            self.sequence_length = sequence_length\n",
        "            self.input_dim = input_dim\n",
        "            self.output_dim = output_dim\n",
        "\n",
        "        def call(self, inputs):\n",
        "            length = tf.shape(inputs)[-1]\n",
        "            positions = tf.range(start=0, limit=length, delta=1)\n",
        "            embedded_tokens = self.token_embeddings(inputs)\n",
        "            embedded_positions = self.position_embeddings(positions)\n",
        "            # Add both embedding vectors together\n",
        "            return embedded_tokens + embedded_positions\n",
        "\n",
        "        # Like the Embedding layer, this layer should be able to generate a mask \n",
        "        # so we can ignore padding 0s in the inputs. The compute_mask method \n",
        "        # will called automatically by the framework and the mask will get \n",
        "        # propagated to the next layer.\n",
        "        def compute_mask(self, inputs, mask=None):\n",
        "            return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "        # Implement serialization so we can save the model.\n",
        "        # Note on saving custom layers\n",
        "        # When you write custom layers, make sure to implement the get_config \n",
        "        # method: this enables the layer to be reinstantiated from its config \n",
        "        # dict, which is useful during model saving and loading. The method \n",
        "        # should return a Python dict that contains the values of the \n",
        "        # constructor arguments used to create the layer.\n",
        "        def get_config(self):\n",
        "            config = super().get_config()\n",
        "            config.update({\n",
        "                \"output_dim\": self.output_dim,\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"input_dim\": self.input_dim,\n",
        "            })\n",
        "            return config\n",
        "\n",
        "    return PositionalEmbedding\n",
        "\n",
        "PositionalEmbedding = listing11_26()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7HQ14V6a_-0",
        "outputId": "f84c38a8-6fe3-4b8a-fcc7-885e5a7b400b"
      },
      "source": [
        "# Listing 11.27 Text classification model that combines positional embedding, \n",
        "# the Transformer encoder, and a pooling layer\n",
        "# Putting it all together: a text-classification Transformer\n",
        "# p.397\n",
        "\n",
        "def listing11_27():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "\n",
        "    vocab_size = 20_000\n",
        "    sequence_length = 600\n",
        "    embed_dim = 256\n",
        "    num_heads = 2\n",
        "    dense_dim = 32\n",
        "\n",
        "\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "    \n",
        "    # Using our new positional embedding layer\n",
        "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "    \n",
        "    x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    callbacks = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            \"full_transformer_encoder.keras\",\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    model.fit(\n",
        "        int_train_ds, \n",
        "        validation_data=int_val_ds, \n",
        "        #epochs=20, \n",
        "        epochs = 5,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "    model = keras.models.load_model(\n",
        "        \"full_transformer_encoder.keras\",\n",
        "        custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                        \"PositionalEmbedding\": PositionalEmbedding})\n",
        "    \n",
        "    HR()\n",
        "    print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
        "\n",
        "listing11_27()\n",
        "\n",
        "\n",
        "# Model: \"model_6\"\n",
        "# _________________________________________________________________\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# input_7 (InputLayer)         [(None, None)]            0         \n",
        "# _________________________________________________________________\n",
        "# positional_embedding_2 (Posi (None, None, 256)         5273600   \n",
        "# _________________________________________________________________\n",
        "# transformer_encoder_6 (Trans (None, None, 256)         543776    \n",
        "# _________________________________________________________________\n",
        "# global_max_pooling1d_6 (Glob (None, 256)               0         \n",
        "# _________________________________________________________________\n",
        "# dropout_6 (Dropout)          (None, 256)               0         \n",
        "# _________________________________________________________________\n",
        "# dense_28 (Dense)             (None, 1)                 257       \n",
        "# =================================================================\n",
        "# Total params: 5,817,633\n",
        "# Trainable params: 5,817,633\n",
        "# Non-trainable params: 0\n",
        "# _________________________________________________________________\n",
        "# Epoch 1/5\n",
        "# 625/625 [==============================] - 46s 70ms/step - loss: 0.4845 - accuracy: 0.7732 - val_loss: 0.2747 - val_accuracy: 0.8906\n",
        "# Epoch 2/5\n",
        "# 625/625 [==============================] - 46s 73ms/step - loss: 0.2388 - accuracy: 0.9100 - val_loss: 0.2596 - val_accuracy: 0.8880\n",
        "# Epoch 3/5\n",
        "# 625/625 [==============================] - 47s 75ms/step - loss: 0.1819 - accuracy: 0.9337 - val_loss: 0.2595 - val_accuracy: 0.8988\n",
        "# Epoch 4/5\n",
        "# 625/625 [==============================] - 47s 75ms/step - loss: 0.1508 - accuracy: 0.9442 - val_loss: 0.2965 - val_accuracy: 0.8960\n",
        "# Epoch 5/5\n",
        "# 625/625 [==============================] - 47s 75ms/step - loss: 0.1266 - accuracy: 0.9530 - val_loss: 0.3151 - val_accuracy: 0.8796\n",
        "# 782/782 [==============================] - 21s 27ms/step - loss: 0.2965 - accuracy: 0.8804\n",
        "\n",
        "# Test acc: 0.880\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "positional_embedding_2 (Posi (None, None, 256)         5273600   \n",
            "_________________________________________________________________\n",
            "transformer_encoder_6 (Trans (None, None, 256)         543776    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_6 (Glob (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,817,633\n",
            "Trainable params: 5,817,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "625/625 [==============================] - 46s 70ms/step - loss: 0.4845 - accuracy: 0.7732 - val_loss: 0.2747 - val_accuracy: 0.8906\n",
            "Epoch 2/5\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.2388 - accuracy: 0.9100 - val_loss: 0.2596 - val_accuracy: 0.8880\n",
            "Epoch 3/5\n",
            "625/625 [==============================] - 47s 75ms/step - loss: 0.1819 - accuracy: 0.9337 - val_loss: 0.2595 - val_accuracy: 0.8988\n",
            "Epoch 4/5\n",
            "625/625 [==============================] - 47s 75ms/step - loss: 0.1508 - accuracy: 0.9442 - val_loss: 0.2965 - val_accuracy: 0.8960\n",
            "Epoch 5/5\n",
            "625/625 [==============================] - 47s 75ms/step - loss: 0.1266 - accuracy: 0.9530 - val_loss: 0.3151 - val_accuracy: 0.8796\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.2965 - accuracy: 0.8804\n",
            "Test acc: 0.880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RQNK6JeETH6"
      },
      "source": [
        "We get around 88.3% test accuracy, a solid improvement that clearly demonstrates the value of word order information for text classification. This is our best sequence model so far! \n",
        "\n",
        "However, it’s still one notch below the bag-of-words approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8euMmOxZa_-1"
      },
      "source": [
        "---\n",
        "# Up to 11.4.4 When to use sequence models over bag-of-words models?"
      ]
    }
  ]
}