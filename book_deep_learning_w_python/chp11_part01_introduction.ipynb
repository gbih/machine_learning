{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls1Ryd_cWprg"
   },
   "source": [
    "# Chp 11: Part 1\n",
    "\n",
    "Modern NLP is about using machine learning and large datasets to give computers the ability — not to understand language, which is a more lofty goal — but to ingest a piece of language as input and return something useful, like predicting:\n",
    "\n",
    "    \"What’s the topic of this text?\" (text classification)\n",
    "    \"Does this text contain abuse?\" (content filtering)\n",
    "    \"Does this text sound positive or negative?\" (sentiment analysis)\n",
    "    \"What should be the next word in this incomplete sentence?\" (language modeling) \"How would you say this in German?\" (translation)\n",
    "    \"How would you summarize this article in one paragraph?\" (summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHF2DhuWWprm"
   },
   "source": [
    "### Using the `TextVectorization` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ly7gDDBujLS_"
   },
   "outputs": [],
   "source": [
    "def HR():\n",
    "    # print char * numeric\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_IARuj_QS_3",
    "outputId": "1a8b8a85-2039-4671-a7e6-9ed7225c685c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "global policy: <Policy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "def set_mixed_precision():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        print(\"GPU mode - switch to mixed precision.\")\n",
    "        print(\"Every layer will use a 16-bit compute dtype and float32 variable dtype by default.\")\n",
    "        keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    HR()\n",
    "    print(\"global policy:\", tf.keras.mixed_precision.global_policy())\n",
    "\n",
    "set_mixed_precision()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgksWZz5Wprm",
    "outputId": "31d9a09b-20a9-4199-d695-17a62fc1e847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n",
      "\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "# listing11_2_4, p.359\n",
    "# This is just for demonstration purposes, as it is not performant.\n",
    "# In actuality, it is better to use the Keras TextVectorization layer,\n",
    "# which is fast and efficient, and can be dropped directly into a \n",
    "# tf.data pipeline or a Keras model.\n",
    "\n",
    "def listing11_2_4():\n",
    "        \n",
    "    import string\n",
    "\n",
    "    class Vectorizer:\n",
    "        def standardize(self, text):\n",
    "            text = text.lower()\n",
    "            return \"\".join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "        def tokenize(self, text):\n",
    "            text = self.standardize(text)\n",
    "            return text.split()\n",
    "\n",
    "        def make_vocabulary(self, dataset):\n",
    "            self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "            for text in dataset:\n",
    "                text = self.standardize(text)\n",
    "                tokens = self.tokenize(text)\n",
    "                for token in tokens:\n",
    "                    if token not in self.vocabulary:\n",
    "                        self.vocabulary[token] = len(self.vocabulary)\n",
    "            self.inverse_vocabulary = dict(\n",
    "                (v, k) for k, v in self.vocabulary.items())\n",
    "\n",
    "        def encode(self, text):\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "        def decode(self, int_sequence):\n",
    "            return \" \".join(\n",
    "                self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "\n",
    "    #####################\n",
    "        \n",
    "    vectorizer = Vectorizer()\n",
    "    dataset = [\n",
    "        \"I write, erase, rewrite\",\n",
    "        \"Erase again, and then\",\n",
    "        \"A poppy blooms.\",\n",
    "    ]\n",
    "    vectorizer.make_vocabulary(dataset)\n",
    "\n",
    "\n",
    "    # test Haiku-like sentence\n",
    "    test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "    encoded_sentence = vectorizer.encode(test_sentence)\n",
    "    print(encoded_sentence)\n",
    "    print()\n",
    "    \n",
    "    decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "    print(decoded_sentence)\n",
    "\n",
    "listing11_2_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zDhlY4-Wprp",
    "outputId": "f2143c05-f06a-44fb-bf6d-f56cdeb8fd03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']\n",
      "\n",
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "\n",
      "i write rewrite and [UNK] rewrite again\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the the Keras TextVectorization layer, p.360\n",
    "# This can be dropped directly into a tf.data pipeline or a Keras model.\n",
    "# We can provide custom functions for standardization and tokenization, \n",
    "# which means the layer is flexible enough to handle any use case.\n",
    "# Such custom functions should operate on tf.string tensors, \n",
    "# not regular Python strings.\n",
    "\n",
    "def listing11_2_5():\n",
    "    import re\n",
    "    import string\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "    def custom_standardization_fn(string_tensor):\n",
    "        # Convert strings to lowercase\n",
    "        lowercase_string = tf.strings.lower(string_tensor)\n",
    "        # Replace punctuation characters with the empty string\n",
    "        return tf.strings.regex_replace(\n",
    "            lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "    def custom_split_fn(string_tensor):\n",
    "        # Split strings on whitespace\n",
    "        return tf.strings.split(string_tensor)\n",
    "\n",
    "    # Configures the layer to return sequences of words encoded as integer indices.\n",
    "    text_vectorization = TextVectorization(\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization_fn,\n",
    "        split=custom_split_fn,\n",
    "    )\n",
    "\n",
    "\n",
    "    # To index the vocabulary of a text corpus, call the adapt() method \n",
    "    # of the layer with a Dataset object that yields strings, or just with \n",
    "    # on list of Python strings:\n",
    "    dataset = [\n",
    "        \"I write, erase, rewrite\",\n",
    "        \"Erase again, and then\",\n",
    "        \"A poppy blooms.\",\n",
    "    ]\n",
    "    text_vectorization.adapt(dataset)\n",
    "\n",
    "    \n",
    "    # Displaying the vocabulary.\n",
    "    # We can retrieve the computed vocabulary via get_vocabulary().\n",
    "    # This is useful if you need to convert text encoded as integer sequences back into words.\n",
    "    # The first two entries in the vocabulary are the mask token (index 0) and and the OOV token (index 1).\n",
    "    # Entries in the vocabulary list are sorted by frequency.\n",
    "    print(text_vectorization.get_vocabulary())\n",
    "    print()\n",
    "\n",
    "    \n",
    "    # Encode and then decode an example sentence.\n",
    "    vocabulary = text_vectorization.get_vocabulary()\n",
    "    test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "    encoded_sentence = text_vectorization(test_sentence)\n",
    "    print(encoded_sentence)\n",
    "    print()\n",
    "\n",
    "    inverse_vocab = dict(enumerate(vocabulary))\n",
    "    decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "    print(decoded_sentence)\n",
    "    print()\n",
    "\n",
    "listing11_2_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiJ5PMofWpru"
   },
   "source": [
    "---\n",
    "## 11.3 Two approaches for representing groups of words: sets and sequences, P.362\n",
    "\n",
    "How to represent word order is the pivotal question from which different kinds of NLP architectures spring. The simplest thing you could do is just discard order and treat text as an unordered set of words—this gives you bag-of-words models. You could also decide that words should be processed strictly in the order in which they appear, one at a time, like steps in a timeseries—you could then leverage the recurrent models from last chapter. Finally, a hybrid approach is also possible: the Transformer architecture is technically order-agnostic, yet it injects word-position information into the representations it processes, which enables it to simultaneously look at different parts of a sentence (unlike RNNs) while still being order-aware. Because they take into account word order, both RNNs and Transformers are called sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h61cJSekWpru"
   },
   "source": [
    "### 11.3.1 Preparing the IMDB movie reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rLLlVTvXoUX",
    "outputId": "3cf53faf-f239-441d-d676-500ac4ed1897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb not found, creating directory\n",
      "--------------------------------------------------------------------------------\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  37.5M      0  0:00:02  0:00:02 --:--:-- 37.5M\n",
      "\n",
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "def listing11_3_1():\n",
    "    import os\n",
    "\n",
    "    dirpath = 'aclImdb'\n",
    "    if not os.path.isdir(dirpath):\n",
    "        print(f'{dirpath} not found, creating directory')\n",
    "        HR()\n",
    "        try:\n",
    "            !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "            !tar -xf aclImdb_v1.tar.gz\n",
    "            !rm -r aclImdb/train/unsup\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(f\"Not able to create directory due to error {ex}\")\n",
    "\n",
    "    print()\n",
    "    !cat aclImdb/train/pos/4077_10.txt\n",
    "\n",
    "listing11_3_1()\n",
    "\n",
    "# This creates this folder structure, where pos is positive, neg is negative\n",
    "# aclImdb/\n",
    "# ...train/\n",
    "# ......pos/\n",
    "# ......neg/\n",
    "# ...test/\n",
    "# ......pos/\n",
    "# ......neg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVlF4sIBWprw",
    "outputId": "051f9444-3175-46db-8968-28cea9fa1a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare a validation set by setting apart 20% of the training text files in a new directory, aclImdb/val\n"
     ]
    }
   ],
   "source": [
    "def listing11_3_2():\n",
    "    import os, pathlib, shutil, random\n",
    "    from tensorflow import keras\n",
    "\n",
    "    dirpath = 'aclImdb/val'\n",
    "    if os.path.isdir(dirpath):\n",
    "        print(f\"{dirpath} already exists\")\n",
    "    else:\n",
    "        print(f\"Prepare a validation set by setting apart 20% of the training text files in a new directory, {dirpath}\")\n",
    "        base_dir = pathlib.Path(\"aclImdb\")\n",
    "        val_dir = base_dir / \"val\"\n",
    "        train_dir = base_dir / \"train\"\n",
    "        \n",
    "        for category in (\"neg\", \"pos\"):\n",
    "            os.makedirs(val_dir / category, exist_ok=True)\n",
    "            files = os.listdir(train_dir / category)\n",
    "\n",
    "            # Shuffle the list of training files using a seed, to ensure\n",
    "            # we get the same validation set every time we run the code\n",
    "            random.Random(1337).shuffle(files)\n",
    "\n",
    "            # Take 20% of the training files to use for validation\n",
    "            num_val_samples = int(0.2 * len(files))\n",
    "            val_files = files[-num_val_samples:]\n",
    "\n",
    "            # Move the files to aclImdb/val/neg and aclImdb/val/pos\n",
    "            for fname in val_files:\n",
    "                shutil.move(train_dir / category / fname,\n",
    "                            val_dir / category / fname)\n",
    "    \n",
    "listing11_3_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OTgsXkm7AEX",
    "outputId": "98692650-d7b2-4fa7-9222-fa59467fec39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pympler\n",
      "  Downloading Pympler-0.9.tar.gz (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 5.2 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pympler\n",
      "  Building wheel for pympler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pympler: filename=Pympler-0.9-py3-none-any.whl size=164823 sha256=70639f90d7ba70e1f55a8fd7e9cf44922187b1674d1ce0aab248ac4d3983be0d\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/f3/d8/35d5614ea4ddd295ffb9372a5f2f9570d9593d1ea4be33ec6d\n",
      "Successfully built pympler\n",
      "Installing collected packages: pympler\n",
      "Successfully installed pympler-0.9\n"
     ]
    }
   ],
   "source": [
    "# https://realpython.com/python-namedtuple/\n",
    "# development tool to measure, monitor and analyze the memory behavior of Python objects.\n",
    "!pip install pympler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ym81GvEU7sNJ",
    "outputId": "4810435a-6993-4ba7-92f2-564af794702d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "from collections import namedtuple\n",
    "\n",
    "# One possibility, use namedtuple for data object.\n",
    "\n",
    "DATA = namedtuple(\"DATA\", [\n",
    "    'train_ds'\n",
    "    'val_ds',\n",
    "    'test_ds',\n",
    "    'binary_1gram_train_ds',\n",
    "    'binary_1gram_val_ds',\n",
    "    'binary_1gram_test_ds',\n",
    "    'text_only_train_ds'\n",
    "])\n",
    "print(asizeof.asizeof(DATA))\n",
    "\n",
    "#data_c = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGGxH5FxDu6P",
    "outputId": "2da9628d-c96f-469f-807c-084a62d44418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.DATACLASS_C'>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import astuple\n",
    "\n",
    "# Create dataclass\n",
    "# https://realpython.com/python-data-classes/\n",
    "\n",
    "# create immutable dataclass\n",
    "@dataclass(frozen=True)\n",
    "class DATACLASS_C:\n",
    "    #train_ds: tensorflow.data\n",
    "    train_ds: object\n",
    "    val_ds: object\n",
    "    test_ds: object\n",
    "    binary_1gram_train_ds: object\n",
    "    binary_1gram_val_ds: object\n",
    "    binary_1gram_test_ds: object\n",
    "    text_only_train_ds: object\n",
    "\n",
    "    # How to create iterable dataclass?\n",
    "    # def __iter__(self):\n",
    "    #     return iter(astuple(self))\n",
    "\n",
    "print(DATACLASS_C)\n",
    "print(asizeof.asizeof(DATACLASS_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qI2ZudCXotlT",
    "outputId": "fe2f34f0-f1ef-4436-e37b-8aee81e05b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "\n",
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"if you didn't live in the 90's or didn't listen to rapper EVER!! this movie might be OK for you, but any for any fan or any single person who ever listened to rap this movie was boring and there was no point in the movie where i said thats interesting or i didn't know that. another thing that bugged me was it made it look like anything in his life he did was very easy there was no struggle he made jail look easy, selling drugs, and even rapping it wasn't realistic. i think if the movie where released in about 15 years from now it might have more of an impact maybe!!! good rap movies hustle and flow, get rich or die trying not notorious\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n",
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n",
      "--------\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Size of data:\t665496\n",
      "Size of data_t:\t664768\n",
      "Size of data_c:\t665344\n",
      "--------------------------------------------------------------------------------\n",
      "Size of data_c: 665344\n",
      "Testing data_c\n",
      "DATACLASS_C(train_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, val_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, test_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, binary_1gram_train_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, binary_1gram_val_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, binary_1gram_test_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, text_only_train_ds=<MapDataset shapes: (None,), types: tf.string>)\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Size of data 2: 665496\n",
      "<class 'dict'>\n",
      "train_ds <BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "val_ds <BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "test_ds <BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "binary_1gram_train_ds <MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>\n",
      "binary_1gram_val_ds <MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>\n",
      "binary_1gram_test_ds <MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>\n",
      "text_only_train_ds <MapDataset shapes: (None,), types: tf.string>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# In chapter 8, we used the utility image_dataset_from_directory to create a \n",
    "# batched Dataset of images and their labels for a directory structure.  \n",
    "# We can do the same thing for text files using the utility text_dataset_from_directory.\n",
    "# We create three Dataset objects, for training, validation, and testing.\n",
    "# p.364\n",
    "\n",
    "# Organize this project into 2 main aspects:\n",
    "# 1. Data engineering\n",
    "# 2. Model building\n",
    "\n",
    "# Create and process data (\"data-engineering\")\n",
    "def listing11_3_2b():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "        \"aclImdb/train\", batch_size=batch_size\n",
    "    )\n",
    "    val_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "        \"aclImdb/val\", batch_size=batch_size\n",
    "    )\n",
    "    test_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "        \"aclImdb/test\", batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Displaying the shapes and dtypes of the first batch\n",
    "    for inputs, targets in train_ds:\n",
    "        print(\"inputs.shape:\", inputs.shape)\n",
    "        print(\"inputs.dtype:\", inputs.dtype)\n",
    "        print(\"targets.shape:\", targets.shape)\n",
    "        print(\"targets.dtype:\", targets.dtype)\n",
    "        print(\"inputs[0]:\", inputs[0])\n",
    "        print(\"targets[0]:\", targets[0])\n",
    "        break\n",
    "\n",
    "\n",
    "    # Processing words as a set: the bag-of-words approach\n",
    "    # Single words (unigrams) with binary encoding\n",
    "    # Preprocessing our datasets with a TextVectorization layer\n",
    "    text_vectorization = TextVectorization(\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"binary\",\n",
    "    )\n",
    "    text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "    text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "    binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "\n",
    "    # Inspecting the output of our binary unigram dataset\n",
    "    # These datasets yield inputs that are TensorFlow tf.string tensors, \n",
    "    # and targets that are int32 tensors encoding the value \"0\" or \"1\"\n",
    "    for inputs, targets in binary_1gram_train_ds:\n",
    "        print(\"inputs.shape:\", inputs.shape)\n",
    "        print(\"inputs.dtype:\", inputs.dtype)\n",
    "        print(\"targets.shape:\", targets.shape)\n",
    "        print(\"targets.dtype:\", targets.dtype)\n",
    "        print(\"inputs[0]:\", inputs[0])\n",
    "        print(\"targets[0]:\", targets[0])\n",
    "        print('--------')\n",
    "        break\n",
    "    \n",
    "\n",
    "    HR()\n",
    "    print(type(train_ds))\n",
    "\n",
    "    HR()\n",
    "\n",
    "    data = {\n",
    "        'train_ds': train_ds,\n",
    "        'val_ds': val_ds,\n",
    "        'test_ds': test_ds,\n",
    "        'binary_1gram_train_ds': binary_1gram_train_ds,\n",
    "        'binary_1gram_val_ds': binary_1gram_val_ds,\n",
    "        'binary_1gram_test_ds': binary_1gram_test_ds,\n",
    "        'text_only_train_ds': text_only_train_ds,\n",
    "    }\n",
    "\n",
    "    data_t = DATA(\n",
    "        train_ds = train_ds,\n",
    "        val_ds = val_ds,\n",
    "        test_ds = test_ds,\n",
    "        binary_1gram_train_ds = binary_1gram_train_ds,\n",
    "        binary_1gram_val_ds = binary_1gram_val_ds,\n",
    "        binary_1gram_test_ds = binary_1gram_test_ds,\n",
    "        text_only_train_ds = text_only_train_ds\n",
    "    )\n",
    "\n",
    "    data_c = DATACLASS_C(\n",
    "        train_ds,\n",
    "        val_ds = val_ds,\n",
    "        test_ds = test_ds,\n",
    "        binary_1gram_train_ds = binary_1gram_train_ds,\n",
    "        binary_1gram_val_ds = binary_1gram_val_ds,\n",
    "        binary_1gram_test_ds = binary_1gram_test_ds,\n",
    "        text_only_train_ds = text_only_train_ds\n",
    "    )\n",
    "\n",
    "\n",
    "    HR()\n",
    "    print(f\"Size of data:\\t{asizeof.asizeof(data)}\")\n",
    "    print(f\"Size of data_t:\\t{asizeof.asizeof(data_t)}\")\n",
    "    print(f\"Size of data_c:\\t{asizeof.asizeof(data_c)}\")\n",
    "    HR()\n",
    "\n",
    "\n",
    "    print(f\"Size of data_c: {asizeof.asizeof(data_c)}\")\n",
    "    print(\"Testing data_c\")\n",
    "    print(data_c)\n",
    "    print(data_c.train_ds)\n",
    "    print(data_c.test_ds)\n",
    "    print(data_c.val_ds)\n",
    "    print()\n",
    "    HR()\n",
    "\n",
    "\n",
    "\n",
    "    return data, data_c\n",
    "    \n",
    "\n",
    "\n",
    "data, data_c = listing11_3_2b()\n",
    "\n",
    "print(f\"Size of data 2: {asizeof.asizeof(data)}\")\n",
    "\n",
    "print(type(data))\n",
    "\n",
    "for x, y in data.items():\n",
    "    print(x, y)\n",
    "\n",
    "print(data['train_ds'])\n",
    "\n",
    "# print(type(binary_1gram_train_ds))\n",
    "# print(binary_1gram_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bzdCQjJ8gHI",
    "outputId": "6b49c784-5272-4ecf-836b-f221b02083b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data_t: 663248\n",
      "DATA(train_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, val_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, test_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, binary_1gram_train_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, binary_1gram_val_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, binary_1gram_test_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, text_only_train_ds=<MapDataset shapes: (None,), types: tf.string>)\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>\n",
      "<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>\n",
      "<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>\n",
      "<MapDataset shapes: (None,), types: tf.string>\n",
      "--------------------------------------------------------------------------------\n",
      "Size of data_c: 665344\n",
      "DATACLASS_C(train_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, val_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, test_ds=<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>, binary_1gram_train_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, binary_1gram_val_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, binary_1gram_test_ds=<MapDataset shapes: ((None, 20000), (None,)), types: (tf.float32, tf.int32)>, text_only_train_ds=<MapDataset shapes: (None,), types: tf.string>)\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int32)>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of data_t: {asizeof.asizeof(data_t)}\")\n",
    "print(data_t)\n",
    "print(data_t.train_ds)\n",
    "print(data_t.test_ds)\n",
    "print(data_t.val_ds)\n",
    "print()\n",
    "\n",
    "for x in data_t:\n",
    "    print(x)\n",
    "HR()\n",
    "\n",
    "print(f\"Size of data_c: {asizeof.asizeof(data_c)}\")\n",
    "print(data_c)\n",
    "print(data_c.train_ds)\n",
    "print(data_c.test_ds)\n",
    "print(data_c.val_ds)\n",
    "print()\n",
    "\n",
    "# for x in data_c:\n",
    "#     print(x)\n",
    "# HR()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MO9wcfJqX7i",
    "outputId": "962de0d3-72d1-4e8f-ecf3-853b26ef9465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 22s 31ms/step - loss: 0.4313 - accuracy: 0.8160 - val_loss: 0.3078 - val_accuracy: 0.8808\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2936 - accuracy: 0.8927 - val_loss: 0.2990 - val_accuracy: 0.8846\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2639 - accuracy: 0.9061 - val_loss: 0.3197 - val_accuracy: 0.8860\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2525 - accuracy: 0.9124 - val_loss: 0.3220 - val_accuracy: 0.8836\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2442 - accuracy: 0.9165 - val_loss: 0.3230 - val_accuracy: 0.8838\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2410 - accuracy: 0.9187 - val_loss: 0.3330 - val_accuracy: 0.8824\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2359 - accuracy: 0.9193 - val_loss: 0.3483 - val_accuracy: 0.8782\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2268 - accuracy: 0.9219 - val_loss: 0.3620 - val_accuracy: 0.8796\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2354 - accuracy: 0.9230 - val_loss: 0.3772 - val_accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2337 - accuracy: 0.9233 - val_loss: 0.3834 - val_accuracy: 0.8754\n",
      "--------------------------------------------------------------------------------\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.3063 - accuracy: 0.8798\n",
      "Test acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "# Processing words as a set: the bag-of-words approach\n",
    "# The simplest way to encode a piece of text for processing by a machine learning \n",
    "# model is to discard order and treat it as a set (a \"bag\") of tokens. You could \n",
    "# either look at individual words (unigrams), or try to recover some local order \n",
    "# information by looking at groups of consecutive token (N-grams).\n",
    " \n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def listing11_7():\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    binary_1gram_train_ds = data['binary_1gram_train_ds']\n",
    "    binary_1gram_val_ds = data['binary_1gram_val_ds']\n",
    "    binary_1gram_test_ds = data['binary_1gram_test_ds']\n",
    "\n",
    "   \n",
    "    #############################\n",
    "    \n",
    "    # Listing 11.8 Training and testing the binary unigram model\n",
    "    model = get_model()\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            \"binary_1gram.keras\",\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        # Call .cache() on the datasets to cache them in memory: this way, we will only \n",
    "        # do the preprocessing once during the first epoch, and we’ll reuse the \n",
    "        # preprocessed texts for the following epochs. This can only be done if the \n",
    "        # data is small enough to fit in memory.\n",
    "        binary_1gram_train_ds.cache(),\n",
    "        validation_data=binary_1gram_val_ds.cache(),\n",
    "        epochs=10,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "\n",
    "    HR()\n",
    "\n",
    "    print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")\n",
    "\n",
    "listing11_7()\n",
    "\n",
    "# Test acc: 0.879"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAUXYCa5qvYp",
    "outputId": "a2597778-8ab8-46fb-e83f-e36cacb7f932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 32s 51ms/step - loss: 0.3841 - accuracy: 0.8395 - val_loss: 0.2722 - val_accuracy: 0.8938\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2446 - accuracy: 0.9135 - val_loss: 0.2759 - val_accuracy: 0.8958\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2112 - accuracy: 0.9316 - val_loss: 0.2808 - val_accuracy: 0.9028\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1944 - accuracy: 0.9394 - val_loss: 0.2974 - val_accuracy: 0.8966\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1876 - accuracy: 0.9420 - val_loss: 0.3168 - val_accuracy: 0.8988\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1789 - accuracy: 0.9456 - val_loss: 0.3214 - val_accuracy: 0.8978\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1839 - accuracy: 0.9474 - val_loss: 0.3292 - val_accuracy: 0.8938\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1814 - accuracy: 0.9492 - val_loss: 0.3354 - val_accuracy: 0.8970\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1735 - accuracy: 0.9513 - val_loss: 0.3432 - val_accuracy: 0.8950\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.1698 - accuracy: 0.9512 - val_loss: 0.3603 - val_accuracy: 0.8958\n",
      "--------------------------------------------------------------------------------\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2702 - accuracy: 0.8962\n",
      "Test acc: 0.896\n"
     ]
    }
   ],
   "source": [
    "def listing11_10():\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "    # Bigrams with binary encoding\n",
    "    \n",
    "    # Of course, discarding word order is very reductive, because even atomic concepts \n",
    "    # can be expressed via multiple words: the term \"United States\" conveys a concept \n",
    "    # that is quite distinct from the meaning of the words \"states\" and \"united\" taken \n",
    "    # separately. For this reason, you will usually end up re-injecting local order \n",
    "    # information into your bag-of-words representation by looking at N-grams rather \n",
    "    # than single words (most commonly, bigrams).\n",
    "    \n",
    "    binary_1gram_train_ds = data['binary_1gram_train_ds']\n",
    "    binary_1gram_val_ds = data['binary_1gram_val_ds']\n",
    "    binary_1gram_test_ds = data['binary_1gram_test_ds']\n",
    "    text_only_train_ds = data['text_only_train_ds']\n",
    "    train_ds = data['train_ds']\n",
    "    val_ds = data['val_ds']\n",
    "    test_ds = data['test_ds']\n",
    "\n",
    "    # Configuring the TextVectorization layer to return bigrams\n",
    "    text_vectorization = TextVectorization(\n",
    "        ngrams=2,\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"binary\",\n",
    "    )\n",
    "\n",
    "    # Test how model would perform when trained on such binary-encoded bags of bigrams.\n",
    "    # Listing 11.10 Training and testing the binary bigram model\n",
    "    text_vectorization.adapt(text_only_train_ds)\n",
    "    binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "    # Listing 11.8 Training and testing the binary unigram model\n",
    "    model = get_model()\n",
    "\n",
    "    model.summary()\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            \"binary_2gram.keras\",\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    "    model.fit(\n",
    "        binary_2gram_train_ds.cache(),\n",
    "        validation_data=binary_2gram_val_ds.cache(),\n",
    "        epochs=10,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "\n",
    "    HR()\n",
    "\n",
    "    print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")\n",
    "\n",
    "listing11_10()\n",
    "\n",
    "# Test acc: 0.887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZp78dUqWprx",
    "outputId": "e12a47fc-7944-47af-df55-d19f178d5ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 22s 34ms/step - loss: 0.5779 - accuracy: 0.7638 - val_loss: 0.3557 - val_accuracy: 0.8608\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3588 - accuracy: 0.8483 - val_loss: 0.2985 - val_accuracy: 0.8930\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3115 - accuracy: 0.8681 - val_loss: 0.3333 - val_accuracy: 0.8794\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2771 - accuracy: 0.8819 - val_loss: 0.3295 - val_accuracy: 0.8916\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2599 - accuracy: 0.8893 - val_loss: 0.3415 - val_accuracy: 0.8732\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2433 - accuracy: 0.9029 - val_loss: 0.4153 - val_accuracy: 0.8706\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2296 - accuracy: 0.9069 - val_loss: 0.3527 - val_accuracy: 0.8904\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2296 - accuracy: 0.9098 - val_loss: 0.3532 - val_accuracy: 0.8846\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2225 - accuracy: 0.9128 - val_loss: 0.3785 - val_accuracy: 0.8694\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2135 - accuracy: 0.9171 - val_loss: 0.3587 - val_accuracy: 0.8918\n",
      "--------------------------------------------------------------------------------\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.2952 - accuracy: 0.8953\n",
      "Test acc: 0.895\n"
     ]
    }
   ],
   "source": [
    "def listing11_13():\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "    # Bigrams with TF-IDF encoding\n",
    "    \n",
    "    # Add a bit more information to this representation by counting how many times \n",
    "    # each word or N-gram occurs, that is to say, by taking the histogram of the \n",
    "    # words over the text.\n",
    "    # {\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
    "    # \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n",
    "    \n",
    "    # If you’re doing text classification, knowing how many times a word occurs in a \n",
    "    # sample is critical: any sufficiently long movie review may contain the word \n",
    "    # \"terrible\" regardless of sentiment, but a review that contains many instances \n",
    "    # of the word \"terrible\" is likely a negative one.\n",
    "    \n",
    "\n",
    "    binary_1gram_train_ds = data['binary_1gram_train_ds']\n",
    "    binary_1gram_val_ds = data['binary_1gram_val_ds']\n",
    "    binary_1gram_test_ds = data['binary_1gram_test_ds']\n",
    "    text_only_train_ds = data['text_only_train_ds']\n",
    "    train_ds = data['train_ds']\n",
    "    val_ds = data['val_ds']\n",
    "    test_ds = data['test_ds']\n",
    "\n",
    "\n",
    "    # Configuring the TextVectorization layer to return token counts\n",
    "    text_vectorization = TextVectorization(\n",
    "        ngrams=2,\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"count\"\n",
    "    )\n",
    "\n",
    "    # Count bigram occurrences with the TextVectorization layer:\n",
    "    # Configuring the TextVectorization layer to return TF-IDF-weighted outputs\n",
    "    text_vectorization = TextVectorization(\n",
    "        ngrams=2,\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"tf-idf\", # TF-IDF normalization\n",
    "    )\n",
    "\n",
    "    # Training and testing the TF-IDF bigram model\n",
    "    text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "    tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "    model = get_model()\n",
    "    model.summary()\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                        save_best_only=True)\n",
    "    ]\n",
    "    model.fit(\n",
    "        tfidf_2gram_train_ds.cache(),\n",
    "        validation_data=tfidf_2gram_val_ds.cache(),\n",
    "        epochs=10,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "    \n",
    "    HR()\n",
    "\n",
    "    print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n",
    "\n",
    "listing11_13()\n",
    "\n",
    "# Test acc: 0.888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdcFO4KRMgiB",
    "outputId": "f97ebb3c-fa6e-48e6-f96b-deacb3bb7186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 22s 33ms/step - loss: 0.5356 - accuracy: 0.7713 - val_loss: 0.3209 - val_accuracy: 0.8638\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3465 - accuracy: 0.8521 - val_loss: 0.3012 - val_accuracy: 0.8804\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3101 - accuracy: 0.8654 - val_loss: 0.3196 - val_accuracy: 0.8848\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2932 - accuracy: 0.8727 - val_loss: 0.3490 - val_accuracy: 0.8866\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2719 - accuracy: 0.8910 - val_loss: 0.3294 - val_accuracy: 0.8718\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2554 - accuracy: 0.8964 - val_loss: 0.3573 - val_accuracy: 0.8774\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2444 - accuracy: 0.8987 - val_loss: 0.3292 - val_accuracy: 0.8788\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2350 - accuracy: 0.9043 - val_loss: 0.3410 - val_accuracy: 0.8808\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2335 - accuracy: 0.9036 - val_loss: 0.3465 - val_accuracy: 0.8662\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.2244 - accuracy: 0.9087 - val_loss: 0.3439 - val_accuracy: 0.8864\n",
      "--------------------------------------------------------------------------------\n",
      "782/782 [==============================] - 20s 25ms/step - loss: 0.2993 - accuracy: 0.8815\n",
      "Test acc: 0.882\n",
      "\n",
      "Exporting a model that processes raw strings.\n",
      "raw_text_data tf.Tensor([[b'That was an excellent movie, I loved it.']], shape=(1, 1), dtype=string)\n",
      "\n",
      "95.00 percent positive\n",
      "\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_6 (TextVe (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "model_3 (Functional)         (None, 1)                 320033    \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def listing_sidebar():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "    # Bigrams with TF-IDF encoding\n",
    "    \n",
    "    # Add a bit more information to this representation by counting how many times \n",
    "    # each word or N-gram occurs, that is to say, by taking the histogram of the \n",
    "    # words over the text.\n",
    "    # {\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\n",
    "    # \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\n",
    "    \n",
    "    # If you’re doing text classification, knowing how many times a word occurs in a \n",
    "    # sample is critical: any sufficiently long movie review may contain the word \n",
    "    # \"terrible\" regardless of sentiment, but a review that contains many instances \n",
    "    # of the word \"terrible\" is likely a negative one.\n",
    "    \n",
    "\n",
    "    binary_1gram_train_ds = data['binary_1gram_train_ds']\n",
    "    binary_1gram_val_ds = data['binary_1gram_val_ds']\n",
    "    binary_1gram_test_ds = data['binary_1gram_test_ds']\n",
    "    text_only_train_ds = data['text_only_train_ds']\n",
    "    train_ds = data['train_ds']\n",
    "    val_ds = data['val_ds']\n",
    "    test_ds = data['test_ds']\n",
    "\n",
    "\n",
    "    # Configuring the TextVectorization layer to return token counts\n",
    "    text_vectorization = TextVectorization(\n",
    "        ngrams=2,\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"count\"\n",
    "    )\n",
    "\n",
    "    # Count bigram occurrences with the TextVectorization layer:\n",
    "    # Configuring the TextVectorization layer to return TF-IDF-weighted outputs\n",
    "    text_vectorization = TextVectorization(\n",
    "        ngrams=2,\n",
    "        max_tokens=20000,\n",
    "        output_mode=\"tf-idf\", # TF-IDF normalization\n",
    "    )\n",
    "\n",
    "    # Training and testing the TF-IDF bigram model\n",
    "    text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "    tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "    tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "    model = get_model()\n",
    "\n",
    "    model.summary()\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                        save_best_only=True)\n",
    "    ]\n",
    "    model.fit(\n",
    "        tfidf_2gram_train_ds.cache(),\n",
    "        validation_data=tfidf_2gram_val_ds.cache(),\n",
    "        epochs=10,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "    \n",
    "    HR()\n",
    "\n",
    "    print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    ###########################\n",
    "\n",
    "    print(\"Exporting a model that processes raw strings.\")\n",
    "\n",
    "    # Exporting a model that processes raw strings\n",
    "    inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    processed_inputs = text_vectorization(inputs)\n",
    "    outputs = model(processed_inputs)\n",
    "    inference_model = keras.Model(inputs, outputs)\n",
    "\n",
    "    raw_text_data = tf.convert_to_tensor([\n",
    "        [\"That was an excellent movie, I loved it.\"],\n",
    "    ])\n",
    "    print(\"raw_text_data\", raw_text_data)\n",
    "    print()\n",
    "\n",
    "    predictions = inference_model(raw_text_data)\n",
    "    print(f\"{float(predictions[0] * 100):.2f} percent positive\")\n",
    "    print()\n",
    "\n",
    "    inference_model.summary()\n",
    "\n",
    "listing_sidebar()\n",
    "\n",
    "# Test acc: 0.888\n",
    "\n",
    "# ORIGINAL MODEL that processes data in separate pipeline\n",
    "# Model: \"model_10\"\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# input_11 (InputLayer)        [(None, 20000)]           0         \n",
    "# _________________________________________________________________\n",
    "# dense_14 (Dense)             (None, 16)                320016    \n",
    "# _________________________________________________________________\n",
    "# dropout_7 (Dropout)          (None, 16)                0         \n",
    "# _________________________________________________________________\n",
    "# dense_15 (Dense)             (None, 1)                 17        \n",
    "# =================================================================\n",
    "# Total params: 320,033\n",
    "# Trainable params: 320,033\n",
    "# Non-trainable params: 0\n",
    "\n",
    "\n",
    "# NEW MODEL that processes input as part of model\n",
    "# Model: \"model_11\"\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# input_12 (InputLayer)        [(None, 1)]               0         \n",
    "# _________________________________________________________________\n",
    "# text_vectorization_20 (TextV (None, 20000)             0         \n",
    "# _________________________________________________________________\n",
    "# model_10 (Functional)        (None, 1)                 320033    \n",
    "# =================================================================\n",
    "# Total params: 320,033\n",
    "# Trainable params: 320,033\n",
    "# Non-trainable params: 0\n",
    "# _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihe5QEX9GP44"
   },
   "source": [
    "### Next is 11.3.3 Processing words as a sequence: the Sequence Model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1LXrJIMGSj0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chp11_part01_introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
