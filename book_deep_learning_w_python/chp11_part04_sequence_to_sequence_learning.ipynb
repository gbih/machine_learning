{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chp11_part04_sequence_to_sequence_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEJY4Dt-i02u"
      },
      "source": [
        "# Chp 11: Part 4\n",
        "\n",
        "11.4.4 When to use sequence models over bag-of-words models?\n",
        "\n",
        "You may sometimes hear that bag-of-words methods are outdated, and that Transformer-based sequence models are the way to go, no matter what task or dataset you’re looking at. This is definitely not the case: a small stack of Dense layers on top of a bag-of-bigrams remains a perfectly valid and relevant approach in many cases. In fact, among the various techniques that we’ve tried on the IMDB dataset throughout this chapter, the best performing so far was the bag-of-bigrams!\n",
        "\n",
        "So, when you should prefer one approach over the other?\n",
        "\n",
        "In 2017, my team and I ran a systematic analysis of the performance of various text classification techniques across many different types of text datasets, and we discovered a remarkable and surprising rule of thumb for deciding whether to go with a bag-of-words model or a sequence model. A golden constant of sorts.\n",
        "\n",
        "It turns out that, when approaching a new text classification task, you should pay close attention to the ratio between the (number of samples in your training data) and the (mean number of words per sample). \n",
        "\n",
        "    ratio = (number of samples in your training data) / (mean number of words per sample)\n",
        "\n",
        "1. If that ratio is small (less than 1,500) then the bag-of-bigrams model will perform better (and as a bonus, it will be much faster to train and to iterate on). \n",
        "\n",
        "2. If that ratio is higher than 1,500, then you should go with a sequence model. \n",
        "\n",
        "**In other words, sequence models work best when lots of training data is available and when each sample is relatively short.**\n",
        "\n",
        "* So if you’re classifying 1,000-word long documents, and you have 100,000 of them, you should go with a bigram model (ratio: 100). 100_000 / 1000 = 100\n",
        "\n",
        "\n",
        "* If you’re classifying tweets that are 40-word long on average, and you have 50,000 of them, you should also go with a bigram model (ratio: 1,250). 50_000 / 40 = 1250.0\n",
        " \n",
        "* But if you increase your dataset size to 500,000 tweets, then go with a Transformer encoder (ratio: 12,500). 500_000 / 40 = 12500.0\n",
        "\n",
        "What about the IMDB movie review classification task? We had 20,000 training samples and an average word count of 233, so our rule of thumb points towards a bigram model—which confirms what we found out in practice.\n",
        "\n",
        "    ratio = 20_000 / 223 = 89.67\n",
        "\n",
        "This intuitively makes sense: the input of a sequence models represents a richer and more complex space, and thus it takes more data to map out that space—meanwhile, a plain set of terms is a space so simple that you can train a logistic regression on top using just a few hundreds or thousands of samples. In addition, the shorter a sample is, the less the model can afford to discard any of the information it contains—in particular, word order becomes more important, and discarding it can create ambiguity. The sentences \"this movie is the bomb\" and \"this movie was a bomb\" have very close unigram representations, which could confuse a bag-of-words model, but a sequence model could tell which one is negative and which one is positive. With a longer sample, word statistics would become more reliable and the topic or sentiment would be more apparent from the word histogram alone.\n",
        "\n",
        "Now, keep in kind that this heuristic rule was developed specifically for text classification, it may not necessarily hold for other NLP tasks—when it comes to machine translation, for instance, Transformer shines especially for very long sequences, compared to RNNs. Our heuristic is also just a rule of thumb, rather than a scientific law, so expect it to work most of the time, but not necessarily every time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86OyVtMGJJE"
      },
      "source": [
        "---\n",
        "## 11.5 Beyond text classification: sequence-to-sequence learning, pg.400"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koyxfIOeCd5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6b6dc3-eb77-4f62-c0a0-dcc95cd1794e"
      },
      "source": [
        "# treat as globals for now\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "\n",
        "def setup():\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "\n",
        "    # every layer uses a 16-bit compute dtype and float32 variable dtype by default.\n",
        "    # most of the forward pass of the model will be done in float16,\n",
        "    # (with the exception of numerically unstable operations like softmax),\n",
        "    # while the weights of the model will be stored and updated in float32.\n",
        "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "    \n",
        "setup()\n",
        "\n",
        "\n",
        "def HR():\n",
        "    # print char * numeric\n",
        "    print('-' * 80)\n",
        "\n",
        "\n",
        "def listing11_5_1():\n",
        "    import os\n",
        "\n",
        "    dirpath = 'spa-eng'\n",
        "    if not os.path.isdir(dirpath):\n",
        "        print(f'{dirpath} not found, creating directory')\n",
        "        HR()\n",
        "        try:\n",
        "            !curl -O http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "            !unzip -q spa-eng.zip\n",
        "        except Exception as ex:\n",
        "            print(f\"Not able to create directory due to error {ex}\")\n",
        "            \n",
        "listing11_5_1()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
            "  Tesla P4, compute capability 6.1\n",
            "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
            "spa-eng not found, creating directory\n",
            "--------------------------------------------------------------------------------\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2576k  100 2576k    0     0  10.1M      0 --:--:-- --:--:-- --:--:-- 10.1M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au2UY-RLi020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6277e4a5-61d4-430c-fcec-6e75d4dbb14d"
      },
      "source": [
        "# Create global to carry the original data, text_pairs\n",
        "\n",
        "def listing11_5_2():\n",
        "    import random\n",
        "\n",
        "    # The text file contains one example per line: an English sentence, \n",
        "    # followed by a tab character, followed by the corresponding Spanish sentence.\n",
        "    \n",
        "    text_file = \"spa-eng/spa.txt\"\n",
        "    with open(text_file) as f:\n",
        "        lines = f.read().split(\"\\n\")[:-1]\n",
        "    text_pairs = []\n",
        "\n",
        "    # Iterate over the lines in the file.\n",
        "    for line in lines:\n",
        "        # Each line contains an English phrase and its Spanish translation, tab-separated.\n",
        "        english, spanish = line.split(\"\\t\")\n",
        "        # prepend \"[start]\" and append \"[end]\" to the Spanish sentence\n",
        "        spanish = \"[start] \" + spanish + \" [end]\"\n",
        "        text_pairs.append((english, spanish))\n",
        "\n",
        "    print(random.choice(text_pairs))\n",
        "\n",
        "    return text_pairs\n",
        "\n",
        "text_pairs = listing11_5_2()\n",
        "print(len(text_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('He neglects his studies.', '[start] Él descuida sus estudios. [end]')\n",
            "118964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy-khYWJi021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a20dd6-5690-4eed-8ed4-bbe8c8da8a25"
      },
      "source": [
        "# p.402\n",
        "\n",
        "def listing11_28():\n",
        "    import random\n",
        "\n",
        "    random.shuffle(text_pairs)\n",
        "    num_val_samples = int(0.15 * len(text_pairs))\n",
        "    num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "\n",
        "    train_pairs = text_pairs[:num_train_samples]\n",
        "    val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "    test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "    print(\"total text_pairs length:\", len(text_pairs))\n",
        "    print(\"train_pairs length:\", len(train_pairs))\n",
        "    print(\"val_pairs lenth:\", len(val_pairs))\n",
        "    print(\"test_pairs:\", len(test_pairs))\n",
        "    print()\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "    # Listing 11.28 Vectorizing the English and Spanish text pairs\n",
        "\n",
        "    import tensorflow as tf\n",
        "    import string\n",
        "    import re\n",
        "\n",
        "    strip_chars = string.punctuation + \"¿\"\n",
        "    strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "    strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "    def custom_standardization(input_string):\n",
        "        lowercase = tf.strings.lower(input_string)\n",
        "        return tf.strings.regex_replace(\n",
        "            lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "\n",
        "    source_vectorization = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=sequence_length,\n",
        "    )\n",
        "    target_vectorization = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=sequence_length + 1,\n",
        "        standardize=custom_standardization,\n",
        "    )\n",
        "    train_english_texts = [pair[0] for pair in train_pairs]\n",
        "    train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "    source_vectorization.adapt(train_english_texts)\n",
        "    target_vectorization.adapt(train_spanish_texts)\n",
        "\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "    # Listing 11.29 Preparing training and validation datasets for the translation task\n",
        "\n",
        "    batch_size = 64\n",
        "\n",
        "    def format_dataset(eng, spa):\n",
        "        eng = source_vectorization(eng)\n",
        "        spa = target_vectorization(spa)\n",
        "        return ({\n",
        "            \"english\": eng,\n",
        "            \"spanish\": spa[:, :-1],\n",
        "        }, spa[:, 1:])\n",
        "\n",
        "    def make_dataset(pairs):\n",
        "        eng_texts, spa_texts = zip(*pairs)\n",
        "        eng_texts = list(eng_texts)\n",
        "        spa_texts = list(spa_texts)\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.map(format_dataset)\n",
        "        return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "    train_ds = make_dataset(train_pairs)\n",
        "    val_ds = make_dataset(val_pairs)\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "    for inputs, targets in train_ds.take(1):\n",
        "        print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "        print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "        print(f\"targets.shape: {targets.shape}\")\n",
        "\n",
        "    return train_ds, val_ds, target_vectorization, source_vectorization, test_pairs\n",
        "\n",
        "train_ds, val_ds, target_vectorization, source_vectorization, test_pairs = listing11_28()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total text_pairs length: 118964\n",
            "train_pairs length: 83276\n",
            "val_pairs lenth: 17844\n",
            "test_pairs: 17844\n",
            "\n",
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79HAfS13Ilru"
      },
      "source": [
        "---\n",
        "# 11.5.2 Sequence-to-sequence learning with RNNs\n",
        "\n",
        "Recurrent neural networks dominated sequence-to-sequence learning from 2015 to 2017, before being overtaken by Transformer. They were the basis for many real-world machine translation systems—as mentioned in chapter 10, Google Translate circa 2017 was powered by a stack of seven large LSTM layers. It’s still worth learning about this approach today, as it provides an easy entry point to understand sequence-to-sequence models.\n",
        "\n",
        "\n",
        "The simplest, naive way to use RNNs to turn a sequence into another sequence is to keep the output of the RNN at each time step—in Keras, like this:\n",
        "\n",
        "    inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
        "    x = layers.LSTM(32, return_sequences=True)(x)\n",
        "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "However, there are two major issues with this approach:\n",
        "\n",
        "1. The target sequence must always be the same length as the source sequence. In practice, this is rarely the case. Technically, this isn’t critical, as you could always pad either the source sequence or the target sequence to make their lengthes match.\n",
        "2. Due to the step-by-step nature of RNNs, the model would only be looking at tokens 0...N in the source sequence in order to predict token N in the target sequence. This constraint makes this setup unsuitable for most tasks, in particular translation. Consider translating \"The weather is nice today\" to French—that would be \"Il fait beau aujourd’hui\". You’d need to be able to predict \"Il\" from just \"The\", \"Il fait\" from just \"The weather\", etc., which is simply impossible.\n",
        "\n",
        "If you’re a human translator, you’d start by reading the entire source sentence before starting to translate it. This is especially important if you’re dealing with languages that have wildly different word ordering, like English and Japanese. And that’s exactly what standard sequence-to-sequence models do.\n",
        "\n",
        "\n",
        "In a proper sequence-to-sequence setup (see figure 11.13), you would first use a RNN (the encoder) to turn the entire source sequence into a single vector (or set of vectors). This could be the last output of the RNN, or alternatively, its final internal state vectors. Then you would use this vector (or vectors) as the initial state of another RNN (the decoder), which would look at elements 0...​N in the target sequence, and try to predict step N+1 in the target sequence.\n",
        "\n",
        "Let’s implement this in Keras with GRU-based encoders and decoders. The choice of GRU rather than LSTM makes things a bit simpler, since GRU only has a single state vector, whereas LSTM has multiple. Let’s start with the encoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDvVGkI2i028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92243bfe-8863-45ee-b0b5-b17b1aa65c6c"
      },
      "source": [
        "# Listing 11.30 GRU-based encoder\n",
        "# Sequence-to-sequence learning with RNNs\n",
        "# GRU-based encoder\n",
        "\n",
        "def listing11_30():\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    import numpy as np\n",
        "    import random\n",
        "    \n",
        "    embed_dim = 256\n",
        "    latent_dim = 1024\n",
        "    source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "    encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "    # 11.31 GRU-based decoder and the end-to-end model\n",
        "    past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "    decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "    x = decoder_gru(x, initial_state=encoded_source)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "    seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "    # Listing 11.32 Training our recurrent sequence-to-sequence model\n",
        "    # This takes a long time\n",
        "    seq2seq_rnn.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "    seq2seq_rnn.fit(\n",
        "        train_ds, \n",
        "        #epochs=3, \n",
        "        epochs=1, \n",
        "        validation_data=val_ds\n",
        "    )\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "    # Using our model for inference\n",
        "    # Translating new sentences with our RNN encoder and decoder\n",
        "\n",
        "    spa_vocab = target_vectorization.get_vocabulary()\n",
        "    spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "    max_decoded_sentence_length = 20\n",
        "\n",
        "    def decode_sequence(input_sentence):\n",
        "        tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "        decoded_sentence = \"[start]\"\n",
        "        for i in range(max_decoded_sentence_length):\n",
        "            tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "            next_token_predictions = seq2seq_rnn.predict(\n",
        "                [tokenized_input_sentence, tokenized_target_sentence])\n",
        "            sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "            sampled_token = spa_index_lookup[sampled_token_index]\n",
        "            decoded_sentence += \" \" + sampled_token\n",
        "            if sampled_token == \"[end]\":\n",
        "                break\n",
        "        return decoded_sentence\n",
        "\n",
        "    test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "    for _ in range(20):\n",
        "        input_sentence = random.choice(test_eng_texts)\n",
        "        print(\"-\")\n",
        "        print(input_sentence)\n",
        "        print(decode_sequence(input_sentence))\n",
        "\n",
        "\n",
        "listing11_30()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1302/1302 [==============================] - 206s 145ms/step - loss: nan - accuracy: 0.4184 - val_loss: 1.2766 - val_accuracy: 0.5070\n",
            "-\n",
            "This might not be enough.\n",
            "[start] esto no puede ser tan [end]\n",
            "-\n",
            "Don't forget to pick me up at 6 o'clock tomorrow.\n",
            "[start] no me va a la mañana mañana [end]\n",
            "-\n",
            "Tom put out the fire.\n",
            "[start] tom se [UNK] la luz [end]\n",
            "-\n",
            "That's no longer possible.\n",
            "[start] eso no es más que yo [end]\n",
            "-\n",
            "I hope things change.\n",
            "[start] espero que [UNK] [end]\n",
            "-\n",
            "I need a good dictionary.\n",
            "[start] necesito un buen trabajo [end]\n",
            "-\n",
            "Do you like to study?\n",
            "[start] te gusta ir [end]\n",
            "-\n",
            "Tom doesn't know the reason why Mary is absent.\n",
            "[start] tom no sabe la verdad de tom está haciendo [end]\n",
            "-\n",
            "I live in the house.\n",
            "[start] yo en la casa [end]\n",
            "-\n",
            "I will see him after I get back.\n",
            "[start] me lo vi cuando yo me [UNK] [end]\n",
            "-\n",
            "Tom was nice to everyone.\n",
            "[start] tom estaba muy tarde a las seis [end]\n",
            "-\n",
            "He is not likely to succeed.\n",
            "[start] Él no es miedo [end]\n",
            "-\n",
            "I thought I knew them.\n",
            "[start] pensé que yo te había ido [end]\n",
            "-\n",
            "I am trying to learn English.\n",
            "[start] estoy de acuerdo de sus amigos [end]\n",
            "-\n",
            "Tom picked up a book, opened it, and started reading it.\n",
            "[start] tom se puso un libro y se lo [UNK] [end]\n",
            "-\n",
            "Welcome home.\n",
            "[start] en casa [end]\n",
            "-\n",
            "Where shall I wait for you?\n",
            "[start] dónde te puedo ir [end]\n",
            "-\n",
            "That guy has a screw loose!\n",
            "[start] esa chica es una buena [UNK] [end]\n",
            "-\n",
            "I got a letter from a friend of mine in Japan.\n",
            "[start] me una una casa en una casa en una casa [end]\n",
            "-\n",
            "She waited and waited, but he never came back.\n",
            "[start] ella se y y se había ido [end]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk_91K60KEzE"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgWC_eEei03B"
      },
      "source": [
        "# 11.5.3 Sequence-to-sequence learning with Transformer, p.409\n",
        "# Listing 11.35 The TransformerDecoder, p.411\n",
        "\n",
        "# The Transformer Decoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "    # Listing 11.36 TransformerDecoder method that generates a \"causal mask\"\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "    # Listing 11.37 The forward pass of the TransformerDecoder\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "###########################################################\n",
        "\n",
        "# Putting it all together: a Transformer for machine translation\n",
        "# PositionalEmbedding layer\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "###########################################################\n",
        "\n",
        "# End-to-end Transformer\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCFxZO3PcyDV",
        "outputId": "cf6ebd42-712f-4fc1-89bd-0a0c85e56db8"
      },
      "source": [
        "# Listing 11.38 End-to-end Transformer\n",
        "\n",
        "def listing11_38():\n",
        "    import random\n",
        "    \n",
        "    embed_dim = 256\n",
        "    dense_dim = 2048\n",
        "    num_heads = 8\n",
        "\n",
        "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "\n",
        "    encoder_outputs = TransformerEncoder(\n",
        "        embed_dim, dense_dim, num_heads\n",
        "        )(x)\n",
        "\n",
        "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "    x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "    x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "    transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\n",
        "    ###########################################################\n",
        "\n",
        "\n",
        "    # Listing 11.39 Training the sequence-to-sequence Transformer\n",
        "    # Training the sequence-to-sequence Transformer\n",
        "    transformer.compile(\n",
        "        optimizer=\"rmsprop\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "    transformer.fit(\n",
        "        train_ds, \n",
        "        # epochs=30,\n",
        "        epochs=1, \n",
        "        validation_data=val_ds\n",
        "    )\n",
        "\n",
        "\n",
        "    # Translating new sentences with our Transformer model\n",
        "    import numpy as np\n",
        "    spa_vocab = target_vectorization.get_vocabulary()\n",
        "    spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "    max_decoded_sentence_length = 20\n",
        "\n",
        "    def decode_sequence(input_sentence):\n",
        "        tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "        decoded_sentence = \"[start]\"\n",
        "        for i in range(max_decoded_sentence_length):\n",
        "            tokenized_target_sentence = target_vectorization(\n",
        "                [decoded_sentence])[:, :-1]\n",
        "            predictions = transformer(\n",
        "                [tokenized_input_sentence, tokenized_target_sentence])\n",
        "            sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "            sampled_token = spa_index_lookup[sampled_token_index]\n",
        "            decoded_sentence += \" \" + sampled_token\n",
        "            if sampled_token == \"[end]\":\n",
        "                break\n",
        "        return decoded_sentence\n",
        "\n",
        "    test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "    for _ in range(20):\n",
        "        input_sentence = random.choice(test_eng_texts)\n",
        "        print(\"-\")\n",
        "        print(input_sentence)\n",
        "        print(decode_sequence(input_sentence))\n",
        "\n",
        "listing11_38()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1302/1302 [==============================] - 176s 131ms/step - loss: nan - accuracy: 0.4293 - val_loss: 1.2792 - val_accuracy: 0.5143\n",
            "-\n",
            "You're small.\n",
            "[start] eres un poco [end]\n",
            "-\n",
            "His behavior never ceases to surprise me.\n",
            "[start] su nunca le [UNK] para que me [UNK] [end]\n",
            "-\n",
            "My mother was sick for two days.\n",
            "[start] mi madre estaba dos días [end]\n",
            "-\n",
            "We got there at the same time.\n",
            "[start] nos hemos estado en el tiempo [end]\n",
            "-\n",
            "When do we start?\n",
            "[start] cuándo nos queremos nosotros [end]\n",
            "-\n",
            "I don't feel anything.\n",
            "[start] no me siento nada [end]\n",
            "-\n",
            "Don't you know that you are the laughingstock of the whole town?\n",
            "[start] no quieres que hacer el tiempo para la ciudad [end]\n",
            "-\n",
            "Is that you?\n",
            "[start] es lo que es [end]\n",
            "-\n",
            "Do I have to wear a tie at work?\n",
            "[start] tengo que ser un trabajo para hacer trabajo [end]\n",
            "-\n",
            "Knock it off, Tom.\n",
            "[start] [UNK] a tom [end]\n",
            "-\n",
            "Tom was whistling a song his mother had taught him.\n",
            "[start] tom estaba acostumbrado a una historia que había oído que él le había hecho su madre [end]\n",
            "-\n",
            "What's your favorite yoga pose?\n",
            "[start] cuál es tu comida [UNK] [end]\n",
            "-\n",
            "How many times do I have to tell you?\n",
            "[start] cuánto veces vi a tengo que hacer algo [end]\n",
            "-\n",
            "The rich grow richer and the poor grow poorer.\n",
            "[start] los dientes se [UNK] y los dientes y los dientes [end]\n",
            "-\n",
            "The entire city was without electricity.\n",
            "[start] los hombres más de los personas y él estaba acostumbrado [end]\n",
            "-\n",
            "There are no houses around here.\n",
            "[start] no hay nada en aquí [end]\n",
            "-\n",
            "Her cat had another four kittens.\n",
            "[start] su gato había sido un gran [UNK] [end]\n",
            "-\n",
            "The letter was written by Tom.\n",
            "[start] la carta estaba acostumbrado a tom [end]\n",
            "-\n",
            "I could kill you.\n",
            "[start] podría estar acostumbrado [end]\n",
            "-\n",
            "Tom wondered why Mary didn't like John.\n",
            "[start] tom se le dio a mary por qué john quería hablar con john [end]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swr08QkVaRO8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}