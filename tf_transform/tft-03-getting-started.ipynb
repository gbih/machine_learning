{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e413456-c0ea-4d64-942b-e9ad527c9a60",
   "metadata": {},
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Get Started with TensorFlow Transform\n",
    "\n",
    "[Source](https://www.tensorflow.org/tfx/transform/get_started)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf10ff1-2fa8-4cba-9d32-8d63a458aeff",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/gbih/ml-notes/blob/main/tf_transform/tft-03-getting-started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf2a73-42fc-4c22-ad11-18e9636564f4",
   "metadata": {},
   "source": [
    "1. [Setup](#setup)\n",
    "2. [Introduction](#2.0)\n",
    "3. [Define a preprocessing function](#3.0)\n",
    "    * [3.1 Preprocessing function example](#3.1)\n",
    "    * [3.2 Batching](#3.2)\n",
    "4. [Apache Beam Implementation](#4.0)\n",
    "    * [4.1 \"instance dict\" TFT Beam tf.Transform implementation](#4.1)\n",
    "5. [Data Formats and Schema](#5.0)\n",
    "    * [5.1 The \"instance dict\" format](#5.1)\n",
    "    * [5.2 The TFXIO format](#5.2)\n",
    "        - [5.2.1 RecordBatch with pyarrow.record_batch()](#5.2.1)\n",
    "        - [5.2.2 RecordBatch with from_pandas()](#5.2.2)\n",
    "        - [5.2.3 RecordBatch with from_arrays()](#5.2.3)\n",
    "        - [5.2.4 RecordBatch with from_struct_array (struct of lists)](#5.2.4)\n",
    "        - [5.2.5 RecordBatch with from_struct_array (struct of scalars)](#5.2.5)\n",
    "        - [5.2.6 tfxio.TensorRepresentative type alias](#5.2.6)\n",
    "6. [Compatibility with TensorFlow](#6.0)\n",
    "7. [Input and output with Apache Beam](#7.0)\n",
    "    * [7.1 Pre-canned PCollection Sources (TFXIO)](#7.1)\n",
    "8. [Example: \"Census Income dataset\"](#8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2be924-2681-4fd7-92e0-173ef61fc125",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='2.0'></a><a name='2.0'></a>\n",
    "# 2. Imports / Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d5a84f-42fc-4ad9-b0b2-b0c80542e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded libraries.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Module level imports for tensorflow_transform.beam.\n",
    "# https://github.com/tensorflow/transform/blob/master/tensorflow_transform/beam/__init__.py\n",
    "# https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "\n",
    "# In-memory representation of all metadata associated with a dataset.\n",
    "# https://www.tensorflow.org/tfx/transform/api_docs/python/tft/DatasetMetadata\n",
    "# https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_metadata.py\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "\n",
    "# Utilities for using the tf.Metadata Schema within TensorFlow\n",
    "# https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/schema_utils.py\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "# Module level imports for tfx_bsl.public.tfxio.\n",
    "# TFXIO defines a common in-memory data representation shared by all TFX libraries\n",
    "# and components, as well as an I/O abstraction layer to produce such representations.\n",
    "# https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio\n",
    "# https://github.com/tensorflow/community/blob/master/rfcs/20191017-tfx-standardized-inputs.md\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# global seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "tf.get_logger().propagate = False\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel('ERROR') # DEBUG, INFO, WARN, ERROR, or FATAL\n",
    "\n",
    "\n",
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "def dir_ex(obj):\n",
    "    result = [x for x in dir(obj) if not x.startswith('_')]\n",
    "    print(type(obj))\n",
    "    print()\n",
    "    for x in result:\n",
    "        print(f'{x:<40}', end=\"\")\n",
    "\n",
    "print(\"Loaded libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527976a9-8698-4503-97d5-c7f4b907f78d",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='2.0'></a><a name='2.0'></a>\n",
    "# 2. Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "This guide introduces the basic concepts of `tf.Transform` and how to use them. It will:\n",
    "\n",
    "* Define a *preprocessing function*, a logical description of the pipeline that transforms the raw data into the data used to train a machine learning model.\n",
    "\n",
    "* Show the Apache Beam implementation used to transform data by converting the *preprocessing function* into a *Beam pipeline*.\n",
    "\n",
    "* Show additional usage example.\n",
    "\n",
    "## Transform library for TFX and non-TFX users \n",
    "\n",
    "The `tft` module documentation is the only module that is relevant to TFX users. The `tft_beam` module is relevant only when using Transform as a standalone library. Typically, a TFX user constructs a `preprocessing_fn`, and the rest of the Transform library calls are made by the [TFX Transform component](https://www.tensorflow.org/tfx/guide/transform)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f9c5e-30e6-4189-96ff-063de66352ad",
   "metadata": {},
   "source": [
    "## Notes on tfxio\n",
    "\n",
    "https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio\n",
    "\n",
    "<sub>\n",
    "    \n",
    "**Classes**\n",
    "\n",
    "* BeamRecordCsvTFXIO:  TFXIO implementation for CSV records in pcoll[bytes].\n",
    "* CsvTFXIO:  TFXIO implementation for CSV.\n",
    "* RecordBatchToExamplesEncoder:  Encodes pa.RecordBatch as a list of serialized tf.Examples.\n",
    "* RecordBatchesOptions:  Options for TFXIO's RecordBatches.\n",
    "* TFExampleBeamRecord:  TFXIO implementation for serialized tf.Examples in pcoll[bytes].\n",
    "* TFExampleRecord:  TFXIO implementation for tf.Example on TFRecord.\n",
    "* TFGraphRecordDecoder:  Base class for decoders that turns a list of bytes to (composite) tensors.\n",
    "* TFSequenceExampleBeamRecord:  TFXIO implementation for serialized tf.SequenceExamples in pcoll[bytes].\n",
    "* TFSequenceExampleRecord:  TFXIO implementation for tf.SequenceExample on TFRecord.\n",
    "* TFXIO:  Abstract basic class of all TFXIO API implementations.\n",
    "* TensorAdapter:  A TensorAdapter converts a RecordBatch to a collection of TF Tensors.\n",
    "* TensorAdapterConfig:  Config to a TensorAdapter.\n",
    "* TensorFlowDatasetOptions:  Options for TFXIO's TensorFlowDataset.\n",
    "    \n",
    "</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73237c72-7f01-4045-9831-244d90ae3504",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='3.0'></a><a name='3.0'></a>\n",
    "# 3. Define a preprocessing function\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "The *preprocessing function* is the most important concept of `tf.Transform`. The preprocessing function is a logical description of a transformation of the dataset. The preprocessing function accepts and returns a dictionary of tensors, where a *tensor* means `Tensor` or `SparseTensor`. There are two kinds of functions used to define the preprocessing function:\n",
    "\n",
    "1. Any function that accepts and returns tensors. These add TensorFlow operations to the graph that transform raw data into transformed data.\n",
    "\n",
    "2. Any of the *analyzers* provided by `tf.Transform`. Analyzers also accept and return tensors, but unlike TensorFlow functions, they do not add operations to the graph. Instead, analyzers cause `tf.Transform` to compute a full-pass operation outside of TensorFlow. They use the input tensor values over the entire dataset to generate a constant tensor that is returned as the output. For example, `tft.min` computes the minimum of a tensor over the dataset. `tf.Transform` provides a fixed set of analyzers, but this will be extended in future versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d732cde-8fa3-4026-b764-e3173a44fb20",
   "metadata": {},
   "source": [
    "<a id='3.1'></a><a name='3.1'></a>\n",
    "## 3.1 Preprocessing function example\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "By combining analyzers and regular TensorFlow functions, users can create flexible pipelines for transforming data. The following preprocessing function transforms each of the three features in different ways, and combines two of the features.\n",
    "\n",
    "[skipped: specific explanation about this function]\n",
    "\n",
    "The preprocessing function defines a pipeline of operations on a dataset. In order to apply the pipeline, we rely on a concrete implementation of the `tf.Transform` API. The Apache Beam implementation provides `PTransform` which applies a user's preprocessing function to data. The typical workflow of a `tf.Transform` user will construct a preprocessing function, then incorporate this into a larger Beam pipeline, creating the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526cd276-b84f-4049-8e1f-7d77920e0c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    x = inputs['x']\n",
    "    y = inputs['y']\n",
    "    s = inputs['s']\n",
    "    \n",
    "    # Cannot use this, get this error:\n",
    "    # ValueError: tf.function only supports singleton tf.Variables created on \n",
    "    # the first call. Make sure the tf.Variable is only created once or created\n",
    "    # outside tf.function.\n",
    "    # See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n",
    "    # @tf.function\n",
    "\n",
    "    # input_data = x\n",
    "    # layer = tf.keras.layers.Normalization(axis=None)\n",
    "    # test = layer(input_data)\n",
    "\n",
    "    x_centered = x - tft.mean(x)\n",
    "    y_normalized = tft.scale_to_0_1(y)\n",
    "    s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "    x_centered_times_y_normalized = x_centered * y_normalized\n",
    "    \n",
    "    return {\n",
    "        'x_centered': x_centered,\n",
    "        'y_normalized': y_normalized,\n",
    "        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "        's_integerized': s_integerized\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9a87e-24a9-4041-b314-5cc53296353a",
   "metadata": {},
   "source": [
    "<a id='3.2'></a><a name='3.2'></a>\n",
    "## 3.2 Batching\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Batching is an important part of TensorFlow. Since one of the goals of `tf.Transform` is to provide a TensorFlow graph for preprocessing that can be incorporated into the serving graph (and, optionally, the training graph), batching is also an important concept in `tf.Transform`.\n",
    "\n",
    "While not obvious in the example above, the user defined preprocessing function is passed tensors representing *batches* and not individual instances, as happens during training and serving with TensorFlow. \n",
    "\n",
    "One the other hand, analyzers perform a computation over the entire dataset that returns a single value and not a batch of values. \n",
    "\n",
    "`x` is a `Tensor` with a shape of `(batch_size,)` while `tft.mean(x)` is a `Tensor` with a shape of `()`. \n",
    "\n",
    "The subtraction `x - tft.mean(x)` broadcasts where the value of `tft.mean(x)` is subtracted from every element of the batch represented by `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e14de17-ab97-4168-a59f-77b85908b6cc",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='4.0'></a><a name='4.0'></a>\n",
    "# 4. Apache Beam Implementation\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "While the *preprocessing function* is intended as a logical description of a *preprocessing pipeline* implemented on multiple data processing frameworks, `tf.Transform` provides a canonical implementation used on Apache Beam. This implementation demonstrates the functionality required from an implementation. There is no formal API for this functionality, so each implementation can use an API that is idiomatic for its particular data processing framework.\n",
    "\n",
    "Note: **canonical** in this context means \"standard approved format\", \"normalized\", \"standardized\".\n",
    "\n",
    "The Apache Beam implementation provides two different `PTransform` functions to process data for a preprocessing function: \n",
    "\n",
    "1. The **\"instance dict\"** format, an intuitive format and is suitable for small datasets \n",
    "\n",
    "2. **TFXIO** (Apache Arrow) format, which provides improved performance and is suitble for large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f3b0d-ecf6-4583-80b7-f8ac2aa62a82",
   "metadata": {},
   "source": [
    "<a id='4.1'></a><a name='4.1'></a>\n",
    "## 4.1 \"instance dict\" TFT Beam tf.Transform implementation\n",
    "<a href=\"#top\">[back to top]</a>  \n",
    "\n",
    "This is an intuitive format and is suitable for small datasets.\n",
    "\n",
    "The following shows the usage for the composite function `tft_beam.AnalyzeAndTransformDataset`.\n",
    "\n",
    "Caution: The \"instance dict\" format used with `DatasetMetadata` \n",
    "is much less efficient than TFXIO. For any serious workloads you should \n",
    "use TFXIO with a `tfxio.TensorAdapterConfig` instance as the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda9d92a-c9d0-4d50-bd13-64fa15c2d619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 00:19:25.997094: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/gb/Desktop/python-3.8.12/env/lib/python3.8/site-packages/ipykernel_launcher.py', '-f', '/Users/gb/Library/Jupyter/runtime/kernel-a703c614-e716-490a-9399-f0a685569999.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "2022-07-28 00:19:29.518142: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "raw_data = [\n",
    "    {'x': 1, 'y': 1, 's': 'hello'},\n",
    "    {'x': 2, 'y': 2, 's': 'world'},\n",
    "    {'x': 3, 'y': 2, 's': 'hello'},\n",
    "]\n",
    "\n",
    "# tft.tf_metadata.dataset_metadata.DatasetMetadata\n",
    "# Metadata about a dataset used for the \"instance dict\" format.\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "        's': tf.io.FixedLenFeature([], tf.string),\n",
    "    })\n",
    ")\n",
    "\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    transformed_dataset, transform_fn = (\n",
    "        (raw_data, raw_data_metadata) |\n",
    "        tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a06c34-f4a4-4ebd-a1ac-1a4d04390d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data, transformed_metadata = transformed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78479e-184e-4c88-ba16-e3b10aa10ff7",
   "metadata": {},
   "source": [
    "The `transformed_data` content is shown below and contains the transformed columns in the same format as the raw data:\n",
    "\n",
    "1. In particular, the values of `s_integerized` are `[0, 1, 0]`. These values depend on how the words `hello` and `world` were mapped to integers, which is deterministic. \n",
    "\n",
    "2. For the column `x_centered`, we subtracted the mean, so the values of the column `x`, which were `[1.0, 2.0, 3.0]` became `[-1.0, 0.0, 1.0]`. Similarly, the rest of the columns  match their expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607dbc90-3c2d-44d3-b2cf-a1850a3820be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s_integerized': 0,\n",
       "  'x_centered': -1.0,\n",
       "  'x_centered_times_y_normalized': -0.0,\n",
       "  'y_normalized': 0.0},\n",
       " {'s_integerized': 1,\n",
       "  'x_centered': 0.0,\n",
       "  'x_centered_times_y_normalized': 0.0,\n",
       "  'y_normalized': 1.0},\n",
       " {'s_integerized': 0,\n",
       "  'x_centered': 1.0,\n",
       "  'x_centered_times_y_normalized': 1.0,\n",
       "  'y_normalized': 1.0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40feacb4-d7e0-4398-a81d-17a905b496ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': 1, 'y': 1, 's': 'hello'},\n",
       " {'x': 2, 'y': 2, 's': 'world'},\n",
       " {'x': 3, 'y': 2, 's': 'hello'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original dataset\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244a5b1-1fc3-4dd5-9757-065458472f71",
   "metadata": {},
   "source": [
    "Both `raw_data` and `transformed_data` are datasets. \n",
    "\n",
    "**The next two sections show how the Beam implementation represents datasets, and how to read and write data to disk.** The other return value, `transform_fn`, represents the transformation applied to the data, convered in detail below.\n",
    "\n",
    "The `tft_beam.AnalyzeAndTransformDataset` class is the composition of the two fundamental transforms provided by the implementation `tft_beam.AnalyzeDataset` and `tft_beam.TransformDataset`. So the following two code snippets are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2d6e00-0c70-45f1-ae9f-eb454f834ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = (raw_data, raw_data_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b56ac08-203f-4b38-8792-fb126433f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/gb/Desktop/python-3.8.12/env/lib/python3.8/site-packages/ipykernel_launcher.py', '-f', '/Users/gb/Library/Jupyter/runtime/kernel-a703c614-e716-490a-9399-f0a685569999.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    }
   ],
   "source": [
    "# Version 1\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    transformed_data, transform_fn = (\n",
    "        my_data | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f16d2e-008b-4a61-867b-25237cb23b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/gb/Desktop/python-3.8.12/env/lib/python3.8/site-packages/ipykernel_launcher.py', '-f', '/Users/gb/Library/Jupyter/runtime/kernel-a703c614-e716-490a-9399-f0a685569999.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/Users/gb/Desktop/python-3.8.12/env/lib/python3.8/site-packages/ipykernel_launcher.py', '-f', '/Users/gb/Library/Jupyter/runtime/kernel-a703c614-e716-490a-9399-f0a685569999.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    }
   ],
   "source": [
    "# Version 2\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    transform_fn_2 = my_data | tft_beam.AnalyzeDataset(preprocessing_fn)\n",
    "    transformed_data_2 = (my_data, transform_fn_2) | tft_beam.TransformDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87fd9504-9077-4db5-986d-063668f84b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (transformed_data == transformed_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399434cf-fd30-4546-b8c6-0ffb08f48f63",
   "metadata": {},
   "source": [
    "`transform_fn` is a pure function that represents an operation that is applied to each row of the dataset. In particular, the analyzer values are already computed and treated as constants. In the example, the `transform_fn` contains these constants:\n",
    "* mean of column `x`\n",
    "* min and max of column `y`\n",
    "* vocabulary used to map the strings to integers\n",
    "\n",
    "An important feature of `tf.Transform` is that `transform_fn` represents a map over *rows*. It is a pure function applied to each row separately. All of the computation for aggregating rows is done in `AnalyzeDataset`. Furthermore, the `transform_fn` is represented as a TensorFlow `Graph` which can be embedded into the serving graph.\n",
    "\n",
    "`AnalyzeAndTransformDataset` is provided for optimizations in this special case. This is the same pattern used in scikit-learn, providing the `fit`, `transform`, and `fit_transform` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87004a3-8c33-46f8-82bf-f5753ad98436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeamDatasetMetadata(dataset_metadata={'_schema': feature {\n",
       "  name: \"s_integerized\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"x_centered\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"x_centered_times_y_normalized\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"y_normalized\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "}, deferred_metadata=[{'_schema': feature {\n",
       "  name: \"s_integerized\"\n",
       "  type: INT\n",
       "  int_domain {\n",
       "    min: -1\n",
       "    max: 1\n",
       "    is_categorical: true\n",
       "  }\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"x_centered\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"x_centered_times_y_normalized\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"y_normalized\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "}], asset_map={'vocab_compute_and_apply_vocabulary_vocabulary': 'vocab_compute_and_apply_vocabulary_vocabulary'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_fn[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00456849-62ad-4411-953b-d069651fbbf0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='5.0'></a><a name='5.0'></a>\n",
    "# 5. Data Formats and Schema\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "TFT Beam implementation accepts two different input data formats:\n",
    "1. The **\"instance dict\"** format (as seen in the example above) is an intuitive format and suitable for small datasets. \n",
    "2. The **TFXIO** (Apache Arrow) format provides improved performance and is suitable for large datasets.\n",
    "\n",
    "The \"metadata\" accompanying the `PCollection` tells the Beam implementation the format of the `PCollection`.\n",
    "\n",
    "1. If `raw_data_metadata` is a `dataset_metadata.DatasetMetadata`, then `raw_data` is expected to be in the \"instance dict\" format.\n",
    "\n",
    "2. If `raw_data_metadata` is a `tfxio.TensorAdapterConfig`, then `raw_data` is expected to be in the TFXIO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5445630-9b1c-4400-ab30-207fccae2f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_data_metadata is a dataset_metadata.DatasetMetadata\n",
      "----------------------------------------\n",
      "raw_data:\n",
      "\n",
      "[ {'s': 'hello', 'x': 1, 'y': 1},\n",
      "  {'s': 'world', 'x': 2, 'y': 2},\n",
      "  {'s': 'hello', 'x': 3, 'y': 2}]\n",
      "----------------------------------------\n",
      "A raw_data row:\n",
      "\n",
      "{'x': 1, 'y': 1, 's': 'hello'}\n",
      "----------------------------------------\n",
      "Check that raw_data rows is in the \"instance dict\" format:\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check type of raw_data_metadata\n",
    "try:\n",
    "    assert(type(raw_data_metadata) == tft.tf_metadata.dataset_metadata.DatasetMetadata)\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    print(\"raw_data_metadata is a dataset_metadata.DatasetMetadata\")\n",
    "    HR()\n",
    "\n",
    "\n",
    "# We can confirm this is of type dataset_metadata.DatasetMetadata,\n",
    "# hence raw_data needs to be in the \"instance_dict\" format.\n",
    "print(\"raw_data:\\n\")\n",
    "pp.pprint(raw_data)\n",
    "HR()\n",
    "\n",
    "print(\"A raw_data row:\\n\")\n",
    "print(raw_data[0])\n",
    "HR()\n",
    "\n",
    "print('Check that raw_data rows is in the \"instance dict\" format:\\n')\n",
    "print(isinstance(raw_data[0], (dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c399a0-06c1-4efe-9006-a2d99a2b87f7",
   "metadata": {},
   "source": [
    "<a id='5.1'></a><a name='5.1'></a>\n",
    "## 5.1 The \"instance dict\" format\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "The previous code examples used this format. \n",
    "\n",
    "The metadata contains the schema that defines the layout of the data, and how it is read from and written to various formats. \n",
    "\n",
    "However, this in-memory format is not self-describing and requires the schema in order to be interpreted as tensors.\n",
    "\n",
    "Again, here is the definition of the schema for the example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f65a7cb6-08ce-48f2-8e56-fb3240ecf7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature {\n",
       "  name: \"s\"\n",
       "  type: BYTES\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"x\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"y\"\n",
       "  type: FLOAT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "        's': tf.io.FixedLenFeature([], tf.string),\n",
    "    })\n",
    ")\n",
    "\n",
    "raw_data_metadata.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0b356-f5f5-4f5c-a33a-911fa870f3ee",
   "metadata": {},
   "source": [
    "The `Schema` proto contains the information needed to parse the data from its on-disk or in-memory format, into tensors.\n",
    "\n",
    "It is typically constructed by calling `schema_utils.schema_from_feature_spec` with a dict mapping feature keys to these parsing configurations:\n",
    "1. `tf.io.FixedLenFeature`: Fixed-length input feature\n",
    "2. `tf.io.VarLenFeature`: Variable-length input feature\n",
    "3. `tf.io.SparseFeature`: Sparse input feature from an Example\n",
    "\n",
    "`tf.parse.example` parses Example protos into a dict of tensors. [See the documentation for more details](https://www.tensorflow.org/api_docs/python/tf/io/parse_example).\n",
    "\n",
    "Above, we use `tf.io.FixedLenFeature` to indicate that each feature contains a fixed number of values, in this case a single scalar value. Because `tf.Transform` batches instances, the actual `Tensor` representing the feature will have shape `(None,)` where the unknown dimension is the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0a12f-895d-4b25-8356-45f26503f387",
   "metadata": {},
   "source": [
    "<a id='5.2'></a><a name='5.2'></a>\n",
    "## 5.2 The TFXIO format\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "With this format, the data is expected to be contained in a **[pyarrow.RecordBatch](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatch.html)**.\n",
    "\n",
    "For tabular data, our Apache Beam implementation accepts Arrow `RecordBatch`es that consist of columns of the following types:\n",
    "\n",
    "1.  `pa.list_(<primitive>)` where `<primitive>` is: \n",
    "    * `pa.int64()`\n",
    "    * `pa.float32()`\n",
    "    * `pa.binary()`\n",
    "    * `pa.large_binary()`\n",
    "2. `pa.large_list(<primitive>)`\n",
    "\n",
    "\n",
    "The toy input dataset we used looks like these listings (5.2.1 - 5.2.5) when represented as a `RecordBatch`.\n",
    "\n",
    "---\n",
    "\n",
    "Notes:\n",
    "\n",
    "Similar to the `dataset_metadata.DatasetMetadata` instance that accompanies the \"instance dict\" format, a `tfxio.TensorAdapterConfig` must accompany the `RecordBatch`. It consists of the Arrow schema of the `RecordBatch`, and `tfxio.TensorRepresentations` to uniquely determine how columns in `RecordBatch`es can be interpreted as TensorFlow Tensors (including but not limited to `tf.Tensor`, `tf.SparseTensor`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af231a2e-2962-4108-a1d8-086b9928dbb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='5.2.1'></a><a name='5.2.1'></a>\n",
    "### 5.2.1 RecordBatch with pyarrow.record_batch()\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "[Create a pyarrow.RecordBatch from another Python data structure or sequence of arrays](https://arrow.apache.org/docs/python/generated/pyarrow.record_batch.html#pyarrow.record_batch)\n",
    "\n",
    "    pyarrow.record_batch(\n",
    "        data, \n",
    "        names=None, \n",
    "        schema=None, \n",
    "        metadata=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c719719-a844-448c-ad7c-cbf605f29590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow version: 5.0.0\n",
      "----------------------------------------\n",
      "pyarrow.RecordBatch\n",
      "x: list<item: float>\n",
      "  child 0, item: float\n",
      "y: list<item: float>\n",
      "  child 0, item: float\n",
      "s: list<item: binary>\n",
      "  child 0, item: binary\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[b'world']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x      y           s\n",
       "0  [1.0]  [1.0]  [b'hello']\n",
       "1  [2.0]  [2.0]  [b'world']\n",
       "2  [3.0]  [3.0]  [b'hello']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "# Note: If we use TFX, we are limited to pyarrow v5\n",
    "print(f\"pyarrow version: {pa.__version__}\")\n",
    "HR()\n",
    "\n",
    "# Constructing a RecordBatch from arrays:\n",
    "raw_data_record_batch = [\n",
    "    \n",
    "    # Create a pyarrow.RecordBatch from another Python data structure or sequence of arrays.\n",
    "    # https://arrow.apache.org/docs/python/generated/pyarrow.record_batch.html#pyarrow.record_batch\n",
    "    pa.record_batch(\n",
    "        # A DataFrame or list of arrays or chunked arrays.\n",
    "        data = [\n",
    "            # pa.array: Create pyarrow.Array instance from a Python object.\n",
    "            # pa.list_: Create ListType instance from child data type or field.\n",
    "            # pa.float32: Create single-precision floating point type.\n",
    "            pa.array([[1], [2], [3]], pa.list_(pa.float32())),\n",
    "            pa.array([[1], [2], [3]], pa.list_(pa.float32())),\n",
    "            pa.array([['hello'], ['world'], ['hello']], pa.list_(pa.binary()))\n",
    "        ],\n",
    "        names=['x', 'y', 's']\n",
    "    )\n",
    "]\n",
    "\n",
    "print(raw_data_record_batch[0])\n",
    "HR()\n",
    "raw_data_record_batch[0].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f97a6f-1c84-420c-8623-715eeda8cb3b",
   "metadata": {},
   "source": [
    "<a id='5.2.2'></a><a name='5.2.2'></a>\n",
    "### 5.2.2 RecordBatch with from_pandas()\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "[Convert pandas.DataFrame to an Arrow RecordBatch](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatch.html#pyarrow.RecordBatch.from_pandas)\n",
    "\n",
    "    from_pandas(\n",
    "        type cls, \n",
    "        df, \n",
    "        Schema schema=None, \n",
    "        preserve_index=None, \n",
    "        nthreads=None, \n",
    "        columns=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211be1f8-5117-4327-9e31-5ae280b4ea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.RecordBatch\n",
      "x: list<item: float>\n",
      "  child 0, item: float\n",
      "y: list<item: float>\n",
      "  child 0, item: float\n",
      "s: list<item: binary>\n",
      "  child 0, item: binary\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[b'world']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x      y           s\n",
       "0  [1.0]  [1.0]  [b'hello']\n",
       "1  [2.0]  [2.0]  [b'world']\n",
       "2  [3.0]  [2.0]  [b'hello']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = [\n",
    "    {'x': [1], 'y': [1], 's': ['hello']},\n",
    "    {'x': [2], 'y': [2], 's': ['world']},\n",
    "    {'x': [3], 'y': [2], 's': ['hello']},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "my_schema = pa.schema([\n",
    "    pa.field('x', pa.list_(pa.float32())),\n",
    "    pa.field('y', pa.list_(pa.float32())),\n",
    "    pa.field('s', pa.list_(pa.binary())),\n",
    "])\n",
    "\n",
    "test2 = pa.RecordBatch.from_pandas(df, schema=my_schema)\n",
    "print(test2)\n",
    "HR()\n",
    "\n",
    "test2.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d2da9-dad7-4ede-b452-741b1cc78c25",
   "metadata": {},
   "source": [
    "<a id='5.2.3'></a><a name='5.2.3'></a>\n",
    "### 5.2.3 RecordBatch with from_arrays()\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "[Construct a RecordBatch from multiple pyarrow.Arrays](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatch.html#pyarrow.RecordBatch.from_arrays)\n",
    "\n",
    "    static from_arrays(\n",
    "        list arrays, \n",
    "        names=None, \n",
    "        schema=None, \n",
    "        metadata=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0977748-5262-459a-a668-cfb5b5c1482a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.RecordBatch\n",
      "x: list<item: float>\n",
      "  child 0, item: float\n",
      "y: list<item: float>\n",
      "  child 0, item: float\n",
      "s: list<item: binary>\n",
      "  child 0, item: binary\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[b'world']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x      y           s\n",
       "0  [1.0]  [1.0]  [b'hello']\n",
       "1  [2.0]  [2.0]  [b'world']\n",
       "2  [3.0]  [2.0]  [b'hello']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pa.array([[1], [2], [3]])\n",
    "y = pa.array([[1], [2], [2]])\n",
    "s = pa.array([['hello'], ['world'], ['hello']])\n",
    "\n",
    "my_schema = pa.schema([\n",
    "    pa.field('x', pa.list_(pa.float32())),\n",
    "    pa.field('y', pa.list_(pa.float32())),\n",
    "    pa.field('s', pa.list_(pa.binary())),\n",
    "])\n",
    "\n",
    "test4 = pa.RecordBatch.from_arrays([x, y, s], schema=my_schema)\n",
    "\n",
    "print(test4)\n",
    "HR()\n",
    "\n",
    "test4.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f999fb-283c-4a8b-ae76-35a522537dfc",
   "metadata": {},
   "source": [
    "<a id='5.2.4'></a><a name='5.2.4'></a>\n",
    "### 5.2.4 RecordBatch with from_struct_array (struct of lists)\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "[Construct a RecordBatch from a StructArray](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatch.html#pyarrow.RecordBatch.from_struct_array)\n",
    "\n",
    "    static from_arrays(\n",
    "        list arrays, \n",
    "        names=None, \n",
    "        schema=None, \n",
    "        metadata=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8219ae4e-e797-47c8-a98a-a1103262e821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.RecordBatch\n",
      "x: list<item: float>\n",
      "  child 0, item: float\n",
      "y: list<item: float>\n",
      "  child 0, item: float\n",
      "s: list<item: binary>\n",
      "  child 0, item: binary\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[b'world']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.0]</td>\n",
       "      <td>[2.0]</td>\n",
       "      <td>[b'hello']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x      y           s\n",
       "0  [1.0]  [1.0]  [b'hello']\n",
       "1  [2.0]  [2.0]  [b'world']\n",
       "2  [3.0]  [2.0]  [b'hello']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [\n",
    "    ('x', pa.list_(pa.float32())),\n",
    "    ('y', pa.list_(pa.float32())),\n",
    "    ('s', pa.list_(pa.binary())),\n",
    "]\n",
    "\n",
    "struct = pa.array(\n",
    "    [\n",
    "        {'x': [1], 'y': [1], 's': ['hello']},\n",
    "        {'x': [2], 'y': [2], 's': ['world']},\n",
    "        {'x': [3], 'y': [2], 's': ['hello']},\n",
    "    ],\n",
    "    type=pa.struct(fields)\n",
    ")\n",
    "\n",
    "test5 = pa.RecordBatch.from_struct_array(struct)\n",
    "print(test5)\n",
    "HR()\n",
    "\n",
    "test5.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd75fa1-0b2a-4f65-9b4e-7aecc316cf41",
   "metadata": {},
   "source": [
    "<a id='5.2.5'></a><a name='5.2.5'></a>\n",
    "### 5.2.5 RecordBatch with from_struct_array (struct of scalars)\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "[Construct a RecordBatch from a StructArray](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatch.html#pyarrow.RecordBatch.from_struct_array)\n",
    "\n",
    "    static from_arrays(\n",
    "        list arrays, \n",
    "        names=None, \n",
    "        schema=None, \n",
    "        metadata=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7af16c07-c8c1-47dc-aebd-b9df5b2725fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyarrow.lib.StructArray'>\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'hello'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'world'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'hello'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x    y         s\n",
       "0  1.0  1.0  b'hello'\n",
       "1  2.0  2.0  b'world'\n",
       "2  3.0  2.0  b'hello'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [\n",
    "    ('x', pa.float32()),\n",
    "    ('y', pa.float32()),\n",
    "    ('s', pa.binary()),\n",
    "]\n",
    "\n",
    "struct = pa.array(\n",
    "    [\n",
    "        {'x': 1, 'y': 1, 's': 'hello'},\n",
    "        {'x': 2, 'y': 2, 's': 'world'},\n",
    "        {'x': 3, 'y': 2, 's': 'hello'},\n",
    "    ],\n",
    "    type=pa.struct(fields)\n",
    ")\n",
    "\n",
    "print(type(struct))\n",
    "HR()\n",
    "\n",
    "test5 = pa.RecordBatch.from_struct_array(struct).to_pandas()\n",
    "test5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671cc39-6ff2-4747-92b8-d29c052759e7",
   "metadata": {},
   "source": [
    "<a id='5.2.6'></a><a name='5.2.6'></a>\n",
    "### 5.2.6 tfxio.TensorRepresentative type alias\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "---\n",
    "\n",
    "`tfxio.TensorRepresentative` is a type alias for a `Dict[str, tensorflow_metadata.proto.v0.schema_pb2.TensorRepresentation]`, which establishes the relationship between a Tensor that a `preprocessing_fn` accepts and columns in the `RecordBatch`. See the code listing below.\n",
    "\n",
    "\n",
    "From this example, we can see this means `inputs['x']` in `preprocessing_fn` should be a dense `tf.Tensor`, whose values come from a column of name `col1` in the input `RecordBatch`es, and its (batched) shape should be `[batch_size, 2]`.\n",
    "\n",
    "A `schema_pb2.TensorRepresentation` is a Protobuf defined in TensorFlow Metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b5b1bd-04df-4706-be9c-dca2e6562c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': dense_tensor {\n",
       "   column_name: \"col1\"\n",
       "   shape {\n",
       "     dim {\n",
       "       size: 2\n",
       "     }\n",
       "   }\n",
       " }}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorRepresentative example\n",
    "from google.protobuf import text_format\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "tensor_representation: schema_pb2.TensorRepresentation = {\n",
    "    # Parses a text representation of a protocol message into a message.\n",
    "    'x': text_format.Parse(\n",
    "        # This message is a necessary argument\n",
    "        \"\"\"dense_tensor {column_name: \"col1\" shape {dim {size: 2}}}\"\"\",\n",
    "        # GeneratedProtocolMessageType\n",
    "        schema_pb2.TensorRepresentation()\n",
    "    )\n",
    "}\n",
    "\n",
    "tensor_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fb301-d485-4ba7-a11b-35e08b549bad",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='6.0'></a><a name='6.0'></a>\n",
    "# 6. Compatibility with TensorFlow\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "`tf.Transform` provides support for exporting the `transform_fn` as a SavedModel. The default behavior before the 0.30 release exported a TF 1.x SavedModel. Starting with the 0.30 release, the default behavior is to export a TF 2.x SavedModel, unless TF 2.x behaviors are explicitly disabled.\n",
    "\n",
    "When exporting the `transform_fn` as a TF 2.x SavedModel, the `preprocessing_fn` is expected to be traceable using `tf.function`. Additionally, if running your pipeline remotely (for example with `DataflowRunner`), ensure that the `preprocessing_fn` and any dependencies are packaged properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45ab9f-5762-4747-b0bf-541238341de7",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a id='7.0'></a><a name='7.0'></a>\n",
    "# 7. Input and output with Apache Beam\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Up to now, we have seen input and output data in the form of python lists (of `RecordBatch` or *\"instance dictionaries\"*). **This is a simplification that relies on Apache Beam's ability to work with lists as well as its main representation of data, the `PCollection`.**\n",
    "\n",
    "A `PCollection` is a data representation that forms a part of a Beam pipeline:\n",
    "\n",
    "* A Beam pipeline is formed by applying various `PTransform`s, including `AnalyzeDataset` and `TransformDataset`, and running the pipeline. \n",
    "\n",
    "* A `PCollection` is not created in the memory of the main binary, but instead is distributed among the workers (although this section uses the in-memory execution mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138698d2-e596-4e88-8a80-5753bb9f456c",
   "metadata": {},
   "source": [
    "<a id='7.1'></a><a name='7.1'></a>\n",
    "## 7.1 Pre-canned PCollection Sources (TFXIO)\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "**The `RecordBatch` format that our implementation accepts is a common format that other TFX libraries also accept.** \n",
    "\n",
    "Therefore, TFX offers convenient \"sources\" (eg `TFXIO` classes) that reads files of various formats on disk and produce `RecordBatch`es and can also return `tfxio.TensorAdapterConfig`, including inferred `tfxio.TensorRepresentations`.\n",
    "\n",
    "These `TFXIO`s can be found in the package `tfx_bsl` ([`tfx_bsl.public.tfxio`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio))\n",
    "\n",
    "\n",
    "**TFXIO Classes:**\n",
    "\n",
    "* BeamRecordCsvTFXIO: TFXIO implementation for CSV records in pcoll[bytes].\n",
    "\n",
    "* CsvTFXIO: TFXIO implementation for CSV.\n",
    "\n",
    "* RecordBatchToExamplesEncoder: Encodes pa.RecordBatch as a list of serialized tf.Examples.\n",
    "\n",
    "* RecordBatchesOptions: Options for TFXIO's RecordBatches.\n",
    "\n",
    "* TFExampleBeamRecord: TFXIO implementation for serialized tf.Examples in pcoll[bytes].\n",
    "\n",
    "* TFExampleRecord: TFXIO implementation for tf.Example on TFRecord.\n",
    "\n",
    "* TFGraphRecordDecoder: Base * for decoders that turns a list of bytes to (composite) tensors.\n",
    "\n",
    "* TFSequenceExampleBeamRecord: TFXIO implementation for serialized tf.SequenceExamples in pcoll[bytes].\n",
    "\n",
    "* TFSequenceExampleRecord: TFXIO implementation for tf.SequenceExample on TFRecord.\n",
    "\n",
    "* TFXIO: Abstract basic * of all TFXIO API implementations.\n",
    "\n",
    "* TensorAdapter: A TensorAdapter converts a RecordBatch to a collection of TF Tensors.\n",
    "\n",
    "* TensorAdapterConfig: Config to a TensorAdapter.\n",
    "\n",
    "* TensorFlowDatasetOptions: Options for TFXIO's TensorFlowDataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff713251-1c13-485e-8183-96c0f482f61d",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='8.0'></a><a name='8.0'></a>\n",
    "# 8. Example: \"Census Income dataset\"\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "The following example requires both reading and writing data on disk, as well as representing data as a `PCollection`.\n",
    "\n",
    "The \"Census Income\" dataset contains both categorical and numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ff7d3ec-f067-498e-97ed-7a196568226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\n",
      "3981312/3974305 [==============================] - 0s 0us/step\n",
      "3989504/3974305 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/gb/Desktop/izumi-handson/1-misc-study/chp13_tfx_04/adult.data'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('./chp13_tfx_04').absolute()\n",
    "\n",
    "train_data_file = tf.keras.utils.get_file(\n",
    "    'adult.data',\n",
    "    'https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data',\n",
    "    cache_subdir=data_dir\n",
    ")\n",
    "train_data_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d570d397-963a-47cd-9933-ed787992a71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_metadata.proto.v0.schema_pb2.Schema"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORDERED_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'age',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'education-num',\n",
    "]\n",
    "\n",
    "LABEL_KEY = 'label'\n",
    "\n",
    "RAW_DATA_FEATURE_SPEC = dict(\n",
    "    [(name, tf.io.FixedLenFeature([], tf.string))\n",
    "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
    "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
    "     for name in NUMERIC_FEATURE_KEYS] +\n",
    "    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n",
    ")\n",
    "\n",
    "SCHEMA = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
    "    tft.tf_metadata.schema_utils.schema_from_feature_spec(\n",
    "        RAW_DATA_FEATURE_SPEC\n",
    "    )\n",
    ").schema\n",
    "\n",
    "type(SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "904d3318-29de-43e0-acb7-a424ac203c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country   label  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\n",
    "    train_data_file,\n",
    "    names = ORDERED_CSV_COLUMNS\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebcbc6-abe2-4970-be30-3bce337009c2",
   "metadata": {},
   "source": [
    "The columns of the dataset are either categorical or numeric. This dataset describes a classification problem: predicting the last column where the individual earns more or less 50K per year. However, from the perspective of `tf.Transform`, this label is just another categorical problem.\n",
    "\n",
    "We use pre-built `tfxio.BeamRecordCsvTFXIO` to translate the CSV lines into `RecordBatches`. `TFXIO` requires two important pieces of information:\n",
    "\n",
    "1. TensorFlow Metadata Schema ([`tfmd.proto.schema_pb2`](https://www.tensorflow.org/tfx/tf_metadata/api_docs/python/tfmd/proto/schema_pb2)), that contains type and shape information about each CSV column. [`schema_pb2.TensorRepresentation`](https://www.tensorflow.org/tfx/tf_metadata/api_docs/python/tfmd/proto/schema_pb2/TensorRepresentation) is an optional part of the Schema. If not provided (which is the case in this example), they will be inferred from the type and shape information. We can get the Schema either by using helper functions we provide to translate from TF parsing specs (as shown in this example), or by running [`TensorFlow Data Validation`](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic).\n",
    "\n",
    "2. List of column names, in the order they appear in the CSV file. These names must match the feature names in the Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9868d08-e519-4cc9-812c-6093bc0436b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx_bsl.public import tfxio\n",
    "from tfx_bsl.coders.example_coder import RecordBatchToExamples\n",
    "\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61a863e3-692d-490e-9edf-17dae7b6a0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCollection[[24]: DecodeTrainData/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches.None]\n"
     ]
    }
   ],
   "source": [
    "# A pipeline object that manages a DAG of \n",
    "# :class:`~apache_beam.pvalue.PValue` s and their\n",
    "# :class:`~apache_beam.transforms.ptransform.PTransform` s.\n",
    "pipeline = beam.Pipeline()\n",
    "\n",
    "# TFXIO implementation for CSV records in pcoll[bytes].\n",
    "# Unlike tfxio.CsvTFXIO, this is a special TFXIO that does not actually do I/O \n",
    "# -- it relies on the caller to prepare a PCollection of bytes.\n",
    "# In other words, we only translate to RecordBatch here.\n",
    "# Reminder: A record batch is a collection of equal-length arrays \n",
    "# matching a particular Schema. It is a table-like data structure that is \n",
    "# semantically a sequence of fields, each a contiguous Arrow Array.\n",
    "csv_tfxio_translate = tfxio.BeamRecordCsvTFXIO(\n",
    "    physical_format = 'text',\n",
    "    column_names=ORDERED_CSV_COLUMNS,\n",
    "    schema = SCHEMA\n",
    ")\n",
    "\n",
    "# A multiple values (potentially huge) container.\n",
    "# tfx_bsl.public.tfxio.CsvTFXIO.BeamSource: Returns a beam PTransform \n",
    "# that produces PCollection[pa.RecordBatch]\n",
    "raw_data = (\n",
    "    pipeline\n",
    "    | 'ReadTrainData' >> beam.io.ReadFromText(\n",
    "        train_data_file, \n",
    "        coder=beam.coders.BytesCoder())\n",
    "    | 'FixCommasTrainData' >> beam.Map(\n",
    "        lambda line: line.replace(b', ', b','))\n",
    "    | 'DecodeTrainData' >> csv_tfxio_translate.BeamSource())\n",
    "\n",
    "\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28804e-7c0f-4419-a22e-b20c59db08db",
   "metadata": {},
   "source": [
    "---\n",
    "We had to do some additional steps after the CSV lines are read. Otherwise, we could use `tfxio.CsvTFXIO` to **handle both reading the file and translating** to `RecordBatch`, as in this following example:\n",
    "\n",
    "---\n",
    "\n",
    "Note: `tfxio.CsvTFXIO` automatically implements this IO functionality:\n",
    "\n",
    "https://github.com/tensorflow/tfx-bsl/blob/master/tfx_bsl/tfxio/csv_tfxio.py#L238\n",
    "\n",
    "<font size=2>\n",
    "    \n",
    "    def _CSVSource(self) -> beam.PTransform:\n",
    "        \"\"\"Returns a PTtransform that producese PCollection[bytes].\"\"\"\n",
    "        return beam.io.ReadFromText(\n",
    "            self._file_pattern,\n",
    "            coder=beam.coders.BytesCoder(),\n",
    "            validate=self._validate,\n",
    "            skip_header_lines=self._skip_header_lines\n",
    "    )\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53326e8a-cea7-4ded-82f6-d3594ad20a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCollection[[25]: TFXIORead/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches.None]\n"
     ]
    }
   ],
   "source": [
    "# TFXIO implementation for CSV.\n",
    "# https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/CsvTFXIO\n",
    "csv_tfxio_read_translate = tfxio.CsvTFXIO(\n",
    "    train_data_file,  # pattern to read csv files from. \n",
    "    telemetry_descriptors=[],  # identify the component that is instantiating this TFXIO.\n",
    "    column_names=ORDERED_CSV_COLUMNS,  # Order must match the order in the CSV file\n",
    "    schema=SCHEMA  # if provided, determines the data type of csv columns\n",
    ")\n",
    "\n",
    "# tfx_bsl.public.tfxio.CsvTFXIO.BeamSource: Returns a beam `PTransform` \n",
    "# that produces `PCollection[pa.RecordBatch]`.\n",
    "raw_data_2 = (\n",
    "    pipeline\n",
    "    | 'TFXIORead' >> csv_tfxio_read_translate.BeamSource()\n",
    ")\n",
    "\n",
    "print(raw_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af4540-cec6-463b-b3d2-d91820232041",
   "metadata": {},
   "source": [
    "---\n",
    "Preprocessing for this dataset is similar to the previous example, except the preprocessing function is programmatically generated instead of manually specifying each column. \n",
    "\n",
    "In the **preprocessing function** below, `NUMERICAL_COLUMNS` and `CATEGORICAL_COLUMNS` are lists that contain the names of the numeric and categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "094c8342-ded7-4ff9-bddd-116548c1c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_preprocessing_fn = \"\"\"\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    x = inputs['x']\n",
    "    y = inputs['y']\n",
    "    s = inputs['s']\n",
    "    \n",
    "    x_centered = x - tft.mean(x)\n",
    "    y_normalized = tft.scale_to_0_1(y)\n",
    "    s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "    x_centered_times_y_normalized = x_centered * y_normalized\n",
    "    \n",
    "    return {\n",
    "        'x_centered': x_centered,\n",
    "        'y_normalized': y_normalized,\n",
    "        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "        's_integerized': s_integerized\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7301d372-66a0-48d9-80e2-8bf48613d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OOV_BUCKETS = 1\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transposed columns.\"\"\"\n",
    "    # Since we are modifying some features and leaving others unchanged, \n",
    "    # we start by setting 'outputs' to a copy of 'inputs'\n",
    "    outputs = inputs.copy()\n",
    "    \n",
    "    # Scale numeric columns to have range [0, 1]\n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "        outputs[key] = tft.scale_to_0_1(outputs[key])\n",
    "        \n",
    "    # For all categorical columns except the label column, we generate a\n",
    "    # vocabulary but do not modify the feature. This vocabulary is instead\n",
    "    # used in a trainer, by means of a feature column, to convert the feature\n",
    "    # from a string to an integer id.\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[key] = tft.compute_and_apply_vocabulary(\n",
    "            tf.strings.strip(inputs[key]),\n",
    "            num_oov_buckets=NUM_OOV_BUCKETS,\n",
    "            vocab_filename=key\n",
    "        )\n",
    "        \n",
    "    # For the label column, we provide the mapping from string to index\n",
    "    with tf.init_scope():\n",
    "        # `init_scope` - Only initialize the table once\n",
    "        initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "            keys=['>50K', '<=50K'],\n",
    "            values=tf.cast(tf.range(2), tf.int64),\n",
    "            key_dtype=tf.string,\n",
    "            value_dtype=tf.int64\n",
    "        )\n",
    "        table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
    "        \n",
    "    outputs[LABEL_KEY] = table.lookup(outputs[LABEL_KEY])\n",
    "        \n",
    "    return outputs    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e0916-e2c3-4e24-b77d-f49b8b6aa075",
   "metadata": {},
   "source": [
    "One difference from the previous example is the label column manually specifies the mapping from the string to an index. So, `>50` is mapped to `0`, and `<=50K` is mapped to `1` because it's useful to know which index in the trained model corresponds to which label.\n",
    "\n",
    "The `record_batches` variable represents a `PCollection` of `pyarrow.RecordBatch`. The `tensor_adapter_config` is given by `csv_tfxio`, which is inferred from `SCHEMA` (and ultimately in this example, from the TF parsing specs).\n",
    "\n",
    "The final stage is to write the transformed data to disk. This has a similar procedure to reading the raw data. The schema used to do this is part of the output of `tft_beam.AnalyzeAndTransformDataset` which infers a schema for the output data. The code to write to disk is shown below. The schema is a part of the metadata but uses the two interchangeably in the `tf.Transform` API (eg pass the metadata to the `tft.coders.ExampleProtoCoder`). Be aware that this writes to a different format. Instead of `textio.WriteToText`, use Beam's built-in support for the `TFRecord` format, and use a coder to encode the data as `Example` protos. This is a better format to use for training, as shown in the next section.\n",
    "\n",
    "`transformed_eval_data_base` provides the base filename for the individual shards that are written.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "736c1e8b-514f-4246-8fdd-6a4bf05c96c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PCollection[[24]: DecodeTrainData/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches.None] at 0x153862670>,\n",
       " <tfx_bsl.tfxio.tensor_adapter.TensorAdapterConfig at 0x15300f970>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = (raw_data, csv_tfxio_read_translate.TensorAdapterConfig())\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "820c0510-523d-4c1e-993b-6f6345f4bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = tempfile.mkdtemp()\n",
    "\n",
    "with tft_beam.Context(temp_dir=working_dir):\n",
    "    transformed_dataset, transform_fn = (\n",
    "        raw_dataset \n",
    "        # Combination of AnalyzeDataset and TransformDataset\n",
    "        # Infers a schema for the output data\n",
    "        | tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn, output_record_batches=True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efde13e0-e1e4-489e-9645-935fd092db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'apache_beam.pvalue.PCollection'>\n",
      "<class 'tensorflow_transform.beam.tft_beam_io.beam_metadata_io.BeamDatasetMetadata'>\n",
      "----------------------------------------\n",
      "<class 'apache_beam.pvalue.PCollection'>\n",
      "<class 'tensorflow_transform.beam.tft_beam_io.beam_metadata_io.BeamDatasetMetadata'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transformed_dataset[0]))\n",
    "print(type(transformed_dataset[1]))\n",
    "HR()\n",
    "print(type(transform_fn[0]))\n",
    "print(type(transform_fn[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f550f5a-6bf4-4837-9c67-0deddaf66086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0B\tchp13_tfx_04\n",
      "Reset chp13_tfx_04\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'chp13_tfx_04'\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "!rm -fr {output_dir}\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!du -h {output_dir}\n",
    "# Make sure to clean this up\n",
    "# !rm -fr {output_dir}\n",
    "print(f\"Reset {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81e036d5-f494-4c44-be98-9bc38c42017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data, _ = transformed_dataset\n",
    "\n",
    "# Run this just as a side-effect\n",
    "_ = (\n",
    "    transformed_data\n",
    "    | 'EncodeTrainData' >>\n",
    "    beam.FlatMapTuple(lambda batch, _ : RecordBatchToExamples(batch))\n",
    "    | 'WriteTrainData' >>\n",
    "    beam.io.WriteToTFRecord(\n",
    "        os.path.join(output_dir, 'transformed.tfrecord')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a231b-5328-43d6-b3e3-f476580f9705",
   "metadata": {},
   "source": [
    "In addition to the training data, `transform_fn` is also written out with the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d198e9f-ade6-49d9-82b9-2e6b8327d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = (\n",
    "    transform_fn\n",
    "    | 'WriteTransformFn' >> tft_beam.WriteTransformFn(output_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43720a97-cd4a-45c5-a4f4-8bc9ab0ed486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0B\tchp13_tfx_04\n"
     ]
    }
   ],
   "source": [
    "!du -h {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8bb30-798a-4ed2-8740-0dcf5c74fc1a",
   "metadata": {},
   "source": [
    "---\n",
    "Run the entire Beam pipeline with `pipeline.run().wait_until_finish()`. Up until this point, the Beam pipeline represents a deferred, distributed computation. It provides instructions for what will be done, but the instructions have not been executed. This final call executes the specified pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b34f7c65-4377-468f-be1a-19fc67f5ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    }
   ],
   "source": [
    "# Runs the pipeline. Returns whatever our runner returns after running.\n",
    "# Waits until the pipeline finishes and returns the final status.\n",
    "result = pipeline.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491ddbf-c42d-4f2f-86d3-6ae9f93f66b6",
   "metadata": {},
   "source": [
    "After running the pipeline, the output directory contains two artifacts:\n",
    "\n",
    "* The transformed data, and the metadata describing it.\n",
    "* The `tf.saved_model` containing the resulting `preprocessing_fn`\n",
    "\n",
    "To see how to use these artifacts, refer to the [Advanced preprocessing tutorial](https://www.tensorflow.org/tfx/tutorials/transform/census)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2f382fe-ba45-44a1-b6b5-5f7ad86796c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x  5 gb  staff   160B Jul 28 00:20 \u001b[34mtransform_fn\u001b[m\u001b[m\n",
      "-rw-r--r--  1 gb  staff     0B Jul 28 00:20 transformed.tfrecord-00000-of-00001\n",
      "drwxr-xr-x  4 gb  staff   128B Jul 28 00:20 \u001b[34mtransformed_metadata\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fd7f589-0064-42a7-9564-4570ef99ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0K\tchp13_tfx_04/transformed_metadata\n",
      "8.0K\tchp13_tfx_04/transform_fn/variables\n",
      " 32K\tchp13_tfx_04/transform_fn/assets\n",
      "196K\tchp13_tfx_04/transform_fn\n",
      "204K\tchp13_tfx_04\n"
     ]
    }
   ],
   "source": [
    "!du -h {output_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
