{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cf4736-2749-4110-a6c3-753350bf2f79",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "<a name=\"top\"></a><!--Need for Colab-->\n",
    "# Quick introduction to TensorFlow Serving\n",
    "\n",
    "Using Docker, the subprocess module, and HTTP requests-logging with TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9173c60-1fa4-4d9d-9ad6-eea616ec867e",
   "metadata": {},
   "source": [
    "1. [Setup](#setup)\n",
    "2. [Introduction](#2.0)\n",
    "3. [Using subprocess with TensorFlow Serving](#3.0)\n",
    "4. [Set up the prebuilt half_plus_two model](#4.0)\n",
    "5. [HTTP Request Logging for TensorFlow Serving](#5.0)\n",
    "6. [Running a minimal Docker image with TensorFlow Serving](#6.0)\n",
    "7. [Subprocess to write logs to file](#7.0)\n",
    "8. [Health check](#8.0)\n",
    "    * 8.1 [Verify response](#8.1)\n",
    "    * 8.2 [Verify logs](#8.2)\n",
    "9. [Predict requests via POST](#9.0)\n",
    "10. [Misc properties](#10.0)\n",
    "11. [End and clean up processes](#11.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060391a-78ad-4cdf-9618-d08a5f45a2cf",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"setup\"></a>\n",
    "# 1. Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60da2bc3-2fc7-46e4-9641-14a5c91a6f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "requests  : 2.27.1\n",
      "tensorflow: 2.9.1\n",
      "\n",
      "Finished loading packages..\n"
     ]
    }
   ],
   "source": [
    "# stdlib imports\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# third party imports\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "\n",
    "# For debugging, provides version & hardware info\n",
    "try:\n",
    "    %load_ext watermark\n",
    "except ImportError:\n",
    "    print(\"Installing watermark:\")\n",
    "    !pip install watermark -q\n",
    "    %load_ext watermark\n",
    "finally:\n",
    "    %watermark --python --packages requests,tensorflow\n",
    "    \n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "\n",
    "print(\"Finished loading packages..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affb1a3-927d-4ea0-aeba-3d2bccf15d68",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"2.0\"></a><a name=\"2.0\"></a>\n",
    "# 2. Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We explore these tasks here:\n",
    "\n",
    "* Using subprocess with TensorFlow Serving.\n",
    "* HTTP Request Logging for TensorFlow Serving.\n",
    "* Set up the prebuilt model, half_plus_two.\n",
    "* Running a minimal Docker image with TensorFlow Serving.\n",
    "* Subprocess to write logs to file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c9298-c5f8-4477-b227-62b626e61ff0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"3.0\"></a><a name=\"3.0\"></a>\n",
    "# 3. Using subprocess with TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "\n",
    "\n",
    "A convenient way to use TensorFlow Serving is via Docker. We can operate the server and logging functions via Docker Commands on the CLI, but there are certain advantages (readability, maintenance, security, etc) to wrapping them in Python.\n",
    "\n",
    "The older method of doing this involved using either os.system or os.spawn*. Here, we will instead use the newer subprocess module. This allows us to spawn new processes, connect to their input/output/error pipes, and optionally obtain their return codes. This is a safer analog to os.system().\n",
    "\n",
    "The underlying process creation and management in `subprocess` is done by the [Popen Constructor](https://docs.python.org/3/library/subprocess.html#popen-constructor), `subprocess.Popen`. The underlying Popen interface can be used directly and offers the most flexibility. By default it results in a non-blocking call.\n",
    "\n",
    "Once you've created the Popen instance, some options are:\n",
    "* `wait()`:  to pause until the subprocess has exited.\n",
    "* `poll()`: check if it's exited without pausing.\n",
    "* `communicate()`: interact with process\n",
    "    - Send data to stdin. \n",
    "    - Read data from stdout and stderr, until end-of-file is reached. \n",
    "    - Wait for process to terminate and set the returncode attribute. \n",
    "\n",
    "\n",
    "**Popen Constructor:**\n",
    "\n",
    "<sup>\n",
    "\n",
    "```bash\n",
    "class subprocess.Popen(\n",
    "    args, \n",
    "    bufsize=- 1, \n",
    "    executable=None, \n",
    "    stdin=None, \n",
    "    stdout=None, \n",
    "    stderr=None, \n",
    "    preexec_fn=None, \n",
    "    close_fds=True, \n",
    "    shell=False, \n",
    "    cwd=None, \n",
    "    env=None, \n",
    "    universal_newlines=None, \n",
    "    startupinfo=None, \n",
    "    creationflags=0, \n",
    "    restore_signals=True, \n",
    "    start_new_session=False, \n",
    "    pass_fds=(), \n",
    "    *, \n",
    "    group=None, \n",
    "    extra_groups=None, \n",
    "    user=None, \n",
    "    umask=- 1, \n",
    "    encoding=None, \n",
    "    errors=None, \n",
    "    text=None, \n",
    "    pipesize=- 1\n",
    ")\n",
    "\n",
    "```\n",
    "<br>\n",
    "</sup>\n",
    "    \n",
    "A convenience function built upon the underlying Popen interface is `subprocess.run`. This is a blocking call, as it waits for the command(s) to complete, then return a CompletedProcess instance.\n",
    "\n",
    "There are older high-level APIs, existing prior to Python 3.5. The functionality provided by them has been superceded by `subprocess.Popen` and `subprocess.run`:\n",
    "\n",
    "* `subprocess.call`\n",
    "* `subprocess.check_call`\n",
    "* `subprocess.check_output`\n",
    "\n",
    "\n",
    "**Useful resources on subprocess:**\n",
    "\n",
    "- https://peps.python.org/pep-0324/\n",
    "- https://docs.python.org/3/whatsnew/2.4.html#pep-324-new-subprocess-module\n",
    "- https://docs.python.org/3/library/subprocess.html\n",
    "- https://www.bogotobogo.com/python/python_subprocess_module.php\n",
    "- https://qiita.com/HidKamiya/items/e192a55371a2961ca8a4 (JP)\n",
    "- https://www.programcreek.com/python/example/50/subprocess.Popen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b1648-bc50-4321-82c1-95343ae187f2",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"4.0\"></a><a name=\"4.0\"></a>\n",
    "# 4. Set up the prebuilt half_plus_two model\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "For this example, we will use Docker to deploy Tensorflow Serving and host the toy model **half_plus_two** that computes f(x) = (x / 2) + 2.  This model is found in the Tensorflow Serving Github repository.\n",
    "\n",
    "* https://www.tensorflow.org/tfx/serving/tensorboard\n",
    "* https://www.tensorflow.org/tfx/serving/docker\n",
    "* https://github.com/tensorflow/serving/tree/master/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu/00000123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ad7e03-4db7-4c44-bcbd-07c0f955600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path('serving').is_dir():\n",
    "    !git clone https://github.com/tensorflow/serving\n",
    "else:\n",
    "    print(\"Saved TensorFlow models in 'serving/tensorflow_serving/servables/tensorflow/testdata'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78141d26-977a-4010-a01a-77eb204c080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute pathway of demo models\n",
    "TESTDATA=Path(\"serving/tensorflow_serving/servables/tensorflow/testdata\").resolve()\n",
    "!du -hs {TESTDATA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d71035a-89d5-4f36-9273-1bfe3acd9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model_name and model_dir, used to start the Docker container\n",
    "model_saved = 'saved_model_half_plus_two_cpu'\n",
    "model_dir = (Path() / TESTDATA / model_saved).resolve()\n",
    "model_name = 'half_plus_two'\n",
    "\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96831a-b115-436f-9acb-bafe2afb42f9",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"5.0\"></a><a name=\"5.0\"></a>\n",
    "# 5. HTTP Request Logging for TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* The easiest way to enable logging for TensorFlow-Model-Serving is via environment variables. This is not specific to TensorFlow Serving, but general to TensorFlow.\n",
    "\n",
    "* `TF_CPP_MIN_VLOG_LEVEL` enables logging of the main C++ backend. However, even the lowest setting of `TF_CPP_MIN_VLOG_LEVEL=1` results in too much noise.\n",
    "\n",
    "* `TF_CPP_VMODULE` provides a way to constrain logging to specific modules or source files. The general format is `TF_CPP_VMODULE=<module_name>=1`, where the module name can be either the C++ or Python file name (without the extension).\n",
    "\n",
    "* Here, we can activate logging individually for http_server.cc via \n",
    "`TF_CPP_VMODULE=http_server=1`. This will enable simple HTTP request logging and errors.\n",
    "\n",
    "* To use this with Docker, we pass it as an environmental variable:  `--env TF_CPP_VMODULE=http_server=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089282c-ff60-40ff-a90b-4f31f7085ddd",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.0\"></a>\n",
    "# 6. Running a minimal Docker image with TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* This is the original Docker command-line format.\n",
    "\n",
    "* `docker run` also pulls a docker image (or repository) from the docker registry, if it doesn't already exist locally.\n",
    "\n",
    "\n",
    "This docker image features\n",
    "\n",
    "* Port 8500 exposed for gRPC\n",
    "* Port 8501 exposed for the REST API\n",
    "* Optional environment variable MODEL_NAME (defaults to model)\n",
    "* Optional environment variable MODEL_BASE_PATH (defaults to /models)\n",
    "\n",
    "---\n",
    "\n",
    "It can be easier to first to experiment and set up the Docker cli-commands first, then later wrap with subprocess.Popen in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbeb4a9-b1e1-475a-897c-b7d3a83d0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_cli = f\"\"\"\n",
    "docker run \\\\\n",
    "--rm --tty -p 8500:8500 -p 8501:8501 \\\\\n",
    "--name {model_name} \\\\\n",
    "--mount type=bind,source={model_dir},target=/models/{model_name} \\\\\n",
    "--env MODEL_NAME={model_name} \\\\\n",
    "--env TF_CPP_VMODULE='http_server=1' \\\\\n",
    "--detach \\\\\n",
    "--log-driver=json-file \\\\\n",
    "--log-opt=mode=non-blocking \\\\\n",
    "tensorflow/serving:latest\n",
    "\"\"\"\n",
    "\n",
    "print(cmd_cli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9427b25-0a8c-4064-a4d5-facc4242c010",
   "metadata": {},
   "source": [
    "---\n",
    "For more feedback, run these commands in different terminals before instantiating the Docker container:\n",
    "\n",
    "```bash\n",
    "$ watch docker ps\n",
    "$ docker logs -f half_plus_two\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a385010-45b1-4665-b44f-244cbd538cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_docker():  \n",
    "        \n",
    "    cmd=f\"\"\"\n",
    "docker run\n",
    "--rm --tty -p 8500:8500 -p 8501:8501\n",
    "--name {model_name}\n",
    "--mount type=bind,source={model_dir},target=/models/{model_name}\n",
    "--env MODEL_NAME={model_name}\n",
    "--env TF_CPP_VMODULE='http_server=1'\n",
    "--detach\n",
    "--log-driver=json-file\n",
    "--log-opt=mode=non-blocking\n",
    "tensorflow/serving:latest\n",
    "\"\"\"\n",
    "           \n",
    "    try:\n",
    "        proc = subprocess.Popen(\n",
    "            shlex.split(cmd),\n",
    "            stdout = subprocess.PIPE,\n",
    "            stderr = subprocess.PIPE\n",
    "        )\n",
    "        \n",
    "        # The communicate() method returns a tuple (stdoutdata, stderrdata)\n",
    "        # It only reads data from stdout and stderr.\n",
    "        out, err = proc.communicate()\n",
    "        out = out.decode()\n",
    "        err = err.decode()\n",
    "        print(f\"out: {(out.strip())}\")\n",
    "        \n",
    "        if err:\n",
    "            print(f\"err: {err}\")\n",
    "                \n",
    "        HR()\n",
    "        \n",
    "        sleep_time = 0.5 # Small time delay for docker instance to start up\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Subprocess error: {e.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    return proc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e9fff1-8e7d-44e6-97ff-654933524083",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = server_docker()\n",
    "#print(f\"docker process id: {proc.pid}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a27e02-1b25-4e57-b559-89abe79672ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If need to immediatley kill and remove unused containers/networks/images\n",
    "# !docker kill {model_name} && docker system prune --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b6e0c-c033-4d7e-8bf0-352cdb5ce426",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a name=\"7.0\"></a>\n",
    "# 7. Subprocess to write logs to file\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We can always access logs via `docker logs <container name>` on the CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86fd673b-6cf8-46f1-8a31-45eb0deb06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs --tail 5 {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83881179-2452-4a25-8f8c-ed0fdd7710cf",
   "metadata": {},
   "source": [
    "---\n",
    "We can also create a subprocess that redirects the logs to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d07d828a-39a5-441f-b723-771f9279546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_docker_process():  \n",
    "    cmd = f\"docker logs --follow {model_name}\"   \n",
    "    try:\n",
    "        proc = subprocess.Popen(\n",
    "            shlex.split(cmd),\n",
    "            stdout=open('logger_tfs.log', 'w'),\n",
    "            stderr=subprocess.STDOUT, # redirect to stdout\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff894074-69a5-405e-b659-2090bbc310f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_docker_process()\n",
    "\n",
    "!du -hs logger_tfs.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34158591-58d2-4773-a110-43fd22b0bcbc",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"8.0\"></a>\n",
    "# 8. Health check\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Creates a simple http client and send requests.\n",
    "\n",
    "* https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/tensorflow_model_server_test.py#L520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6f8574-39aa-4583-b29b-64c20bbd0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_status_rest():\n",
    "    try:\n",
    "        resp_data = requests.get(f'http://localhost:8501/v1/models/{model_name}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    else:\n",
    "        return resp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16128b-a24f-44f5-9b26-7981e898f7b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='8.1'></a><a name='8.1'></a>\n",
    "## 8.1 Verify response\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1b5aa6-b6ff-4419-ae64-a48fef6518f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_result = request_status_rest()\n",
    "pp.pprint(request_result.json())\n",
    "\n",
    "assert request_result.json() == {\n",
    "        'model_version_status': [{\n",
    "            'version': '123',\n",
    "            'state': 'AVAILABLE',\n",
    "            'status': {\n",
    "                'error_code': 'OK',\n",
    "                'error_message': ''\n",
    "            }\n",
    "        }]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca708a6-2bd5-4426-a52c-79a6b20218ef",
   "metadata": {},
   "source": [
    "<a id='8.2'></a><a name='8.2'></a>\n",
    "## 8.2 Verify logs\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722adb7c-5425-497a-afd3-372fa0e1f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(5):\n",
    "    resp = request_status_rest()\n",
    "    print(f\"===> TFS Status : {resp.headers['Date']} {resp}\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "HR()\n",
    "    \n",
    "!du -h logger_tfs.log\n",
    "HR()\n",
    "!tail -5 logger_tfs.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d2a15-0722-402d-aec6-6fefd7924aa3",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"9.0\"></a>\n",
    "# 9. Predict requests via POST\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "This is the calculation returned by the TensorFlow Model:\n",
    "\n",
    "f(x) = (x / 2) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02cb590-9087-4d7b-913e-25419abad4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -d '{\"instances\": [1.0]}' \\\n",
    "    -X POST http://localhost:8501/v1/models/half_plus_two:predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99003976-c824-465c-b47a-0cff4c12085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload\n",
    "data = json.dumps({\n",
    "    \"instances\": [1.0, 2.0, 5.0, -9]\n",
    "})\n",
    "print(f\"Payload:\\t{data}\")\n",
    "HR()\n",
    "\n",
    "headers = {\n",
    "    \"content-type\": \"application_json\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f'http://localhost:8501/v1/models/{model_name}:predict',\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "print(f\"Predictions:\\t{json.loads(response.text)['predictions']}\")\n",
    "HR()\n",
    "\n",
    "print(\"Properties of response:\\n\")\n",
    "pp.pprint(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc75b1-c520-4ece-85ac-5836a01155c8",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"10.0\"></a>\n",
    "# 10. Misc properties\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9211b5ea-7ae9-4180-8742-325e17c83dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environmental variables of this container\n",
    "!docker exec {model_name} env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc70e1e-d4a8-4f83-8619-cc98ef52a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using jq:\n",
    "!docker inspect {model_name} | jq '.[] | .Config.Env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "796e8de2-21f6-44b6-a28a-5016610350d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a toy example, we log into the container and check this environmental variable:\n",
    "!docker exec -it {model_name} /bin/bash | echo TF_CPP_VMODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c53de55d-3939-4e4e-868f-feb547e88e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tfs_dict(proc):\n",
    "    print(\"Properties of returned container process:\\n\")\n",
    "    for key in proc.__dict__:\n",
    "        if key == 'args':\n",
    "            HR()\n",
    "            print(key, '->', proc.__dict__[key])\n",
    "            HR()\n",
    "        else:\n",
    "            print(key, '->', proc.__dict__[key])\n",
    " \n",
    "show_tfs_dict(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca736e07-ec30-4199-8db6-08dacb4502f8",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"11.0\"></a>\n",
    "# 11. End and clean up processes\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e960478f-93f8-4cff-aa4d-0571124394e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill and remove unused containers, networks, image\n",
    "!docker kill {model_name} && docker system prune --force"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
