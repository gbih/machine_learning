{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cf4736-2749-4110-a6c3-753350bf2f79",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "<a name=\"top\"></a><!--Need for Colab-->\n",
    "# Quick introduction to TensorFlow Serving\n",
    "\n",
    "## Half Plus Two model\n",
    "\n",
    "Using Docker, subprocess module, prebuilt Half Plus Two model, and HTTP requests-logging with TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9173c60-1fa4-4d9d-9ad6-eea616ec867e",
   "metadata": {},
   "source": [
    "1. [Setup](#setup)\n",
    "2. [Introduction](#2.0)\n",
    "3. [Using subprocess with TensorFlow Serving](#3.0)\n",
    "4. [Set up the prebuilt half_plus_two model](#4.0)\n",
    "    * 4.1 [Inspect the SavedModel Signature](#4.1)\n",
    "5. [HTTP Request Logging for TensorFlow Serving](#5.0)\n",
    "6. [Running a minimal Docker image with TensorFlow Serving](#6.0)\n",
    "7. [Subprocess to write logs to file](#7.0)\n",
    "8. [Health check](#8.0)\n",
    "    * 8.1 [Verify response](#8.1)\n",
    "    * 8.2 [Verify logs](#8.2)\n",
    "9. [Predict requests via POST](#9.0)\n",
    "10. [Misc properties](#10.0)\n",
    "11. [End and clean up processes](#11.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060391a-78ad-4cdf-9618-d08a5f45a2cf",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"setup\"></a>\n",
    "# 1. Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60da2bc3-2fc7-46e4-9641-14a5c91a6f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "requests  : 2.27.1\n",
      "tensorflow: 2.9.1\n",
      "\n",
      "Finished loading packages..\n"
     ]
    }
   ],
   "source": [
    "# stdlib imports\n",
    "import asyncio\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# third party imports\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "\n",
    "# For debugging, provides version & hardware info\n",
    "try:\n",
    "    %load_ext watermark\n",
    "except ImportError:\n",
    "    print(\"Installing watermark:\")\n",
    "    !pip install watermark -q\n",
    "    %load_ext watermark\n",
    "finally:\n",
    "    %watermark --python --packages requests,tensorflow\n",
    "    \n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "\n",
    "print(\"Finished loading packages..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affb1a3-927d-4ea0-aeba-3d2bccf15d68",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"2.0\"></a><a name=\"2.0\"></a>\n",
    "# 2. Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We explore these tasks here:\n",
    "\n",
    "* Using subprocess with TensorFlow Serving.\n",
    "* HTTP Request Logging for TensorFlow Serving.\n",
    "* Set up the prebuilt model, half_plus_two.\n",
    "* Running a minimal Docker image with TensorFlow Serving.\n",
    "* Subprocess to write logs to file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c9298-c5f8-4477-b227-62b626e61ff0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"3.0\"></a><a name=\"3.0\"></a>\n",
    "# 3. Using subprocess with TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "\n",
    "\n",
    "A convenient way to use TensorFlow Serving is via Docker. We can operate the server and logging functions via Docker Commands on the CLI, but there are certain advantages (readability, maintenance, security, etc) to wrapping them in Python.\n",
    "\n",
    "The older method of doing this involved using either os.system or os.spawn*. Here, we will instead use the newer subprocess module. This allows us to spawn new processes, connect to their input/output/error pipes, and optionally obtain their return codes. This is a safer analog to os.system().\n",
    "\n",
    "The underlying process creation and management in `subprocess` is done by the [Popen Constructor](https://docs.python.org/3/library/subprocess.html#popen-constructor), `subprocess.Popen`. The underlying Popen interface can be used directly and offers the most flexibility. By default it results in a non-blocking call.\n",
    "\n",
    "Once you've created the Popen instance, some options are:\n",
    "* `wait()`:  to pause until the subprocess has exited.\n",
    "* `poll()`: check if it's exited without pausing.\n",
    "* `communicate()`: interact with process\n",
    "    - Send data to stdin. \n",
    "    - Read data from stdout and stderr, until end-of-file is reached. \n",
    "    - Wait for process to terminate and set the returncode attribute. \n",
    "\n",
    "\n",
    "**Popen Constructor:**\n",
    "\n",
    "<sup>\n",
    "\n",
    "```bash\n",
    "class subprocess.Popen(\n",
    "    args, \n",
    "    bufsize=- 1, \n",
    "    executable=None, \n",
    "    stdin=None, \n",
    "    stdout=None, \n",
    "    stderr=None, \n",
    "    preexec_fn=None, \n",
    "    close_fds=True, \n",
    "    shell=False, \n",
    "    cwd=None, \n",
    "    env=None, \n",
    "    universal_newlines=None, \n",
    "    startupinfo=None, \n",
    "    creationflags=0, \n",
    "    restore_signals=True, \n",
    "    start_new_session=False, \n",
    "    pass_fds=(), \n",
    "    *, \n",
    "    group=None, \n",
    "    extra_groups=None, \n",
    "    user=None, \n",
    "    umask=- 1, \n",
    "    encoding=None, \n",
    "    errors=None, \n",
    "    text=None, \n",
    "    pipesize=- 1\n",
    ")\n",
    "\n",
    "```\n",
    "<br>\n",
    "</sup>\n",
    "    \n",
    "A convenience function built upon the underlying Popen interface is `subprocess.run`. This is a blocking call, as it waits for the command(s) to complete, then return a CompletedProcess instance.\n",
    "\n",
    "There are older high-level APIs, existing prior to Python 3.5. The functionality provided by them has been superceded by `subprocess.Popen` and `subprocess.run`:\n",
    "\n",
    "* `subprocess.call`\n",
    "* `subprocess.check_call`\n",
    "* `subprocess.check_output`\n",
    "\n",
    "\n",
    "**Useful resources on subprocess:**\n",
    "\n",
    "- https://peps.python.org/pep-0324/\n",
    "- https://docs.python.org/3/whatsnew/2.4.html#pep-324-new-subprocess-module\n",
    "- https://docs.python.org/3/library/subprocess.html\n",
    "- https://www.bogotobogo.com/python/python_subprocess_module.php\n",
    "- https://qiita.com/HidKamiya/items/e192a55371a2961ca8a4 (JP)\n",
    "- https://www.programcreek.com/python/example/50/subprocess.Popen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b1648-bc50-4321-82c1-95343ae187f2",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"4.0\"></a><a name=\"4.0\"></a>\n",
    "# 4. Set up the prebuilt half_plus_two model\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "For this example, we will use Docker to deploy Tensorflow Serving and host the toy model **half_plus_two** that computes f(x) = (x / 2) + 2.  This model is found in the Tensorflow Serving Github repository.\n",
    "\n",
    "* https://www.tensorflow.org/tfx/serving/tensorboard\n",
    "* https://www.tensorflow.org/tfx/serving/docker\n",
    "* https://github.com/tensorflow/serving/tree/master/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu/00000123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77ad7e03-4db7-4c44-bcbd-07c0f955600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TensorFlow models in 'serving/tensorflow_serving/servables/tensorflow/testdata'\n"
     ]
    }
   ],
   "source": [
    "if not Path('serving').is_dir():\n",
    "    !git clone https://github.com/tensorflow/serving\n",
    "else:\n",
    "    print(\"Saved TensorFlow models in 'serving/tensorflow_serving/servables/tensorflow/testdata'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78141d26-977a-4010-a01a-77eb204c080c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2M\t/Users/gb/Desktop/tf_server_01/serving/tensorflow_serving/servables/tensorflow/testdata\n"
     ]
    }
   ],
   "source": [
    "# Get absolute pathway of demo models\n",
    "TESTDATA=Path(\"serving/tensorflow_serving/servables/tensorflow/testdata\").resolve()\n",
    "!du -hs {TESTDATA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d71035a-89d5-4f36-9273-1bfe3acd9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model_name and model_dir, used to start the Docker container\n",
    "model_saved = 'saved_model_half_plus_two_cpu'\n",
    "model_dir = (Path() / TESTDATA / model_saved).resolve()\n",
    "model_name = 'half_plus_two'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a2628-4eee-4d74-ac76-0debbc18f739",
   "metadata": {},
   "source": [
    "<a id='4.1'></a><a name='4.1'></a>\n",
    "## 4.1 Inspect the SavedModel Signature\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We can use the tool `saved_model_cli` to inspect the SignatureDefs (the callable methods) of a model. This allows us to confirm the input Tensor dtype and shape matches the model.\n",
    "\n",
    "Resources:\n",
    "* https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel\n",
    "* https://www.tensorflow.org/guide/saved_model\n",
    "* https://blog.tensorflow.org/2021/03/a-tour-of-savedmodel-signatures.html\n",
    "* https://www.tensorflow.org/tfx/tutorials/serving/rest_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a659829b-fae3-4b16-b471-29dd2f9b9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tag-sets in the latest SavedModel:\n",
      "----------------------------------------\n",
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "newest_model = max(glob.glob(f\"{model_dir}/*\"))\n",
    "\n",
    "print(\"All tag-sets in the latest SavedModel:\")\n",
    "HR()\n",
    "!saved_model_cli show \\\n",
    "--dir {newest_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be57aa21-63d9-4f6c-921d-52d770577e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All available SignatureDef keys in the MetaGraphDef specified by tag-set 'serve':\n",
      "----------------------------------------\n",
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"classify_x_to_y\"\n",
      "SignatureDef key: \"regress_x2_to_y3\"\n",
      "SignatureDef key: \"regress_x_to_y\"\n",
      "SignatureDef key: \"regress_x_to_y2\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "print(\"All available SignatureDef keys in the MetaGraphDef specified by tag-set 'serve':\")\n",
    "HR()\n",
    "!saved_model_cli show \\\n",
    "--dir {newest_model} --tag_set serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada6c3ee-d040-4e0f-96e3-3f3eb2a6e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All inputs and outputs TensorInfo for the specific SignatureDef 'serving_default' in the MetaGraph:\n",
      "Note that shape: (-1) implies 1-D shape for inference data, eg {'examples': [{...}]}\n",
      "----------------------------------------\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['x'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: x:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['y'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: y:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "print(\"All inputs and outputs TensorInfo for the specific SignatureDef 'serving_default' in the MetaGraph:\")\n",
    "print(\"Note that shape: (-1) implies 1-D shape for inference data, eg {'examples': [{...}]}\")\n",
    "HR()\n",
    "\n",
    "!saved_model_cli show \\\n",
    "--dir {newest_model} \\\n",
    "--tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96831a-b115-436f-9acb-bafe2afb42f9",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"5.0\"></a><a name=\"5.0\"></a>\n",
    "# 5. HTTP Request Logging for TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* The easiest way to enable logging for TensorFlow-Model-Serving is via environment variables. This is not specific to TensorFlow Serving, but general to TensorFlow.\n",
    "\n",
    "* `TF_CPP_MIN_VLOG_LEVEL` enables logging of the main C++ backend. However, even the lowest setting of `TF_CPP_MIN_VLOG_LEVEL=1` results in too much noise.\n",
    "\n",
    "* `TF_CPP_VMODULE` provides a way to constrain logging to specific modules or source files. The general format is `TF_CPP_VMODULE=<module_name>=1`, where the module name can be either the C++ or Python file name (without the extension).\n",
    "\n",
    "* Here, we can activate logging individually for http_server.cc via \n",
    "`TF_CPP_VMODULE=http_server=1`. This will enable simple HTTP request logging and errors.\n",
    "\n",
    "* To use this with Docker, we pass it as an environmental variable:  `--env TF_CPP_VMODULE=http_server=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089282c-ff60-40ff-a90b-4f31f7085ddd",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.0\"></a>\n",
    "# 6. Running a minimal Docker image with TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* This is the original Docker command-line format.\n",
    "\n",
    "* `docker run` also pulls a docker image (or repository) from the docker registry, if it doesn't already exist locally.\n",
    "\n",
    "\n",
    "This docker image features\n",
    "\n",
    "* Port 8500 exposed for gRPC\n",
    "* Port 8501 exposed for the REST API\n",
    "* Optional environment variable MODEL_NAME (defaults to model)\n",
    "* Optional environment variable MODEL_BASE_PATH (defaults to /models)\n",
    "\n",
    "---\n",
    "\n",
    "It can be easier to first to experiment and set up the Docker cli-commands first, then later wrap with subprocess.Popen in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abbeb4a9-b1e1-475a-897c-b7d3a83d0569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "docker run \\\n",
      "--rm --tty -p 8500:8500 -p 8501:8501 \\\n",
      "--name half_plus_two \\\n",
      "--mount type=bind,source=/Users/gb/Desktop/tf_server_01/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu,target=/models/half_plus_two \\\n",
      "--env MODEL_NAME=half_plus_two \\\n",
      "--env TF_CPP_VMODULE='http_server=1' \\\n",
      "--detach \\\n",
      "--log-driver=json-file \\\n",
      "--log-opt=mode=non-blocking \\\n",
      "tensorflow/serving:latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd_cli = f\"\"\"\n",
    "docker run \\\\\n",
    "--rm --tty -p 8500:8500 -p 8501:8501 \\\\\n",
    "--name {model_name} \\\\\n",
    "--mount type=bind,source={model_dir},target=/models/{model_name} \\\\\n",
    "--env MODEL_NAME={model_name} \\\\\n",
    "--env TF_CPP_VMODULE='http_server=1' \\\\\n",
    "--detach \\\\\n",
    "--log-driver=json-file \\\\\n",
    "--log-opt=mode=non-blocking \\\\\n",
    "tensorflow/serving:latest\n",
    "\"\"\"\n",
    "\n",
    "print(cmd_cli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9427b25-0a8c-4064-a4d5-facc4242c010",
   "metadata": {},
   "source": [
    "---\n",
    "For more feedback, run these commands in different terminals before instantiating the Docker container:\n",
    "\n",
    "```bash\n",
    "$ watch docker ps\n",
    "$ docker logs -f half_plus_two\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a385010-45b1-4665-b44f-244cbd538cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_docker():  \n",
    "        \n",
    "    cmd=f\"\"\"\n",
    "docker run\n",
    "--rm --tty -p 8500:8500 -p 8501:8501\n",
    "--name {model_name}\n",
    "--mount type=bind,source={model_dir},target=/models/{model_name}\n",
    "--env MODEL_NAME={model_name}\n",
    "--env TF_CPP_VMODULE='http_server=1'\n",
    "--detach\n",
    "--log-driver=json-file\n",
    "--log-opt=mode=non-blocking\n",
    "tensorflow/serving:latest\n",
    "\"\"\"\n",
    "           \n",
    "    try:\n",
    "        proc = subprocess.Popen(\n",
    "            shlex.split(cmd),\n",
    "            stdout = subprocess.PIPE,\n",
    "            stderr = subprocess.PIPE\n",
    "        )\n",
    "        \n",
    "        # The communicate() method returns a tuple (stdoutdata, stderrdata)\n",
    "        # It only reads data from stdout and stderr.\n",
    "        out, err = proc.communicate()\n",
    "        out = out.decode()\n",
    "        err = err.decode()\n",
    "        print(f\"out: {(out.strip())}\")\n",
    "        \n",
    "        if err:\n",
    "            print(f\"err: {err}\")\n",
    "                        \n",
    "        sleep_time = 0.5 # Small time delay for docker instance to start up\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Subprocess error: {e.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    return proc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25e9fff1-8e7d-44e6-97ff-654933524083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 3f51e0d8e34cfa4e44fbec46ab1f85a21895b0e14120e2391fda341f986e24f2\n"
     ]
    }
   ],
   "source": [
    "proc = server_docker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a27e02-1b25-4e57-b559-89abe79672ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If need to immediately kill and remove unused containers/networks/images\n",
    "# !docker kill {model_name} && docker system prune --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b6e0c-c033-4d7e-8bf0-352cdb5ce426",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a name=\"7.0\"></a>\n",
    "# 7. Subprocess to write logs to file\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We can always access logs via `docker logs <container name>` on the CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86fd673b-6cf8-46f1-8a31-45eb0deb06d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-06 13:48:01.563539: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2022-08-06 13:48:01.566350: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n",
      "2022-08-06 13:48:01.567415: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n"
     ]
    }
   ],
   "source": [
    "!docker logs --tail 5 {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83881179-2452-4a25-8f8c-ed0fdd7710cf",
   "metadata": {},
   "source": [
    "---\n",
    "We can also create a subprocess that redirects the logs to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07d828a-39a5-441f-b723-771f9279546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_docker_process(log_file):  \n",
    "    cmd = f\"docker logs --follow {model_name}\"   \n",
    "    try:\n",
    "        proc = subprocess.Popen(\n",
    "            shlex.split(cmd),\n",
    "            stdout=open(log_file, 'w'),\n",
    "            stderr=subprocess.STDOUT, # redirect to stdout\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff894074-69a5-405e-b659-2090bbc310f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_file: half_plus_two_tfs.log\n",
      "  0B\thalf_plus_two_tfs.log\n"
     ]
    }
   ],
   "source": [
    "log_file = f\"{model_name}_tfs.log\"\n",
    "print(f\"log_file: {log_file}\")\n",
    "\n",
    "log_docker_process(log_file)\n",
    "\n",
    "!du -hs {log_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34158591-58d2-4773-a110-43fd22b0bcbc",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"8.0\"></a>\n",
    "# 8. Health check\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Creates a simple http client and send requests.\n",
    "\n",
    "* https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/tensorflow_model_server_test.py#L520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e6f8574-39aa-4583-b29b-64c20bbd0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_status_rest():\n",
    "    try:\n",
    "        resp_data = requests.get(f'http://localhost:8501/v1/models/{model_name}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    else:\n",
    "        return resp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16128b-a24f-44f5-9b26-7981e898f7b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='8.1'></a><a name='8.1'></a>\n",
    "## 8.1 Verify response\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1b5aa6-b6ff-4419-ae64-a48fef6518f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'model_version_status': [ { 'state': 'AVAILABLE',\n",
      "                              'status': { 'error_code': 'OK',\n",
      "                                          'error_message': ''},\n",
      "                              'version': '123'}]}\n"
     ]
    }
   ],
   "source": [
    "request_result = request_status_rest()\n",
    "pp.pprint(request_result.json())\n",
    "\n",
    "assert request_result.json() == {\n",
    "        'model_version_status': [{\n",
    "            'version': '123',\n",
    "            'state': 'AVAILABLE',\n",
    "            'status': {\n",
    "                'error_code': 'OK',\n",
    "                'error_message': ''\n",
    "            }\n",
    "        }]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca708a6-2bd5-4426-a52c-79a6b20218ef",
   "metadata": {},
   "source": [
    "<a id='8.2'></a><a name='8.2'></a>\n",
    "## 8.2 Verify logs\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "722adb7c-5425-497a-afd3-372fa0e1f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> TFS Status : Sat, 06 Aug 2022 13:48:02 GMT <Response [200]>\n",
      "===> TFS Status : Sat, 06 Aug 2022 13:48:02 GMT <Response [200]>\n",
      "===> TFS Status : Sat, 06 Aug 2022 13:48:03 GMT <Response [200]>\n",
      "===> TFS Status : Sat, 06 Aug 2022 13:48:03 GMT <Response [200]>\n",
      "===> TFS Status : Sat, 06 Aug 2022 13:48:04 GMT <Response [200]>\n",
      "----------------------------------------\n",
      "8.0K\thalf_plus_two_tfs.log\n",
      "----------------------------------------\n",
      "2022-08-06 13:48:02.351596: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-06 13:48:02.870246: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-06 13:48:03.384137: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-06 13:48:03.901308: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-06 13:48:04.418427: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n"
     ]
    }
   ],
   "source": [
    "for x in range(5):\n",
    "    resp = request_status_rest()\n",
    "    print(f\"===> TFS Status : {resp.headers['Date']} {resp}\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "HR()\n",
    "    \n",
    "!du -h {log_file}\n",
    "HR()\n",
    "!tail -5 {log_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d2a15-0722-402d-aec6-6fefd7924aa3",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"9.0\"></a>\n",
    "# 9. Predict requests via POST\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "This is the calculation returned by the TensorFlow Model:\n",
    "\n",
    "f(x) = (x / 2) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a02cb590-9087-4d7b-913e-25419abad4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [2.5\n",
      "    ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl -d '{\"instances\": [1.0]}' \\\n",
    "    -X POST http://localhost:8501/v1/models/half_plus_two:predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99003976-c824-465c-b47a-0cff4c12085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload:\t{\"instances\": [1.0, 2.0, 5.0, -9]}\n",
      "----------------------------------------\n",
      "Predictions:\t[2.5, 3.0, 4.5, -2.5]\n",
      "----------------------------------------\n",
      "Properties of response:\n",
      "\n",
      "{ '_content': b'{\\n    \"predictions\": [2.5, 3.0, 4.5, -2.5\\n    ]\\n}',\n",
      "  '_content_consumed': True,\n",
      "  '_next': None,\n",
      "  'connection': <requests.adapters.HTTPAdapter object at 0x12956c430>,\n",
      "  'cookies': <RequestsCookieJar[]>,\n",
      "  'elapsed': datetime.timedelta(microseconds=9159),\n",
      "  'encoding': 'utf-8',\n",
      "  'headers': {'Content-Type': 'application/json', 'Date': 'Sat, 06 Aug 2022 13:48:05 GMT', 'Content-Length': '49'},\n",
      "  'history': [],\n",
      "  'raw': <urllib3.response.HTTPResponse object at 0x12956c9d0>,\n",
      "  'reason': 'OK',\n",
      "  'request': <PreparedRequest [POST]>,\n",
      "  'status_code': 200,\n",
      "  'url': 'http://localhost:8501/v1/models/half_plus_two:predict'}\n"
     ]
    }
   ],
   "source": [
    "# Payload\n",
    "data = json.dumps({\n",
    "    \"instances\": [1.0, 2.0, 5.0, -9]\n",
    "})\n",
    "print(f\"Payload:\\t{data}\")\n",
    "HR()\n",
    "\n",
    "headers = {\n",
    "    \"content-type\": \"application_json\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f'http://localhost:8501/v1/models/{model_name}:predict',\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "print(f\"Predictions:\\t{json.loads(response.text)['predictions']}\")\n",
    "HR()\n",
    "\n",
    "print(\"Properties of response:\\n\")\n",
    "pp.pprint(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc75b1-c520-4ece-85ac-5836a01155c8",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"10.0\"></a>\n",
    "# 10. Misc properties\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9211b5ea-7ae9-4180-8742-325e17c83dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "HOSTNAME=0c4399566773\n",
      "MODEL_NAME=half_plus_two\n",
      "TF_CPP_VMODULE=http_server=1\n",
      "MODEL_BASE_PATH=/models\n",
      "HOME=/root\n"
     ]
    }
   ],
   "source": [
    "# Check environmental variables of this container\n",
    "!docker exec {model_name} env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc70e1e-d4a8-4f83-8619-cc98ef52a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[0;32m\"MODEL_NAME=half_plus_two\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"TF_CPP_VMODULE=http_server=1\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"MODEL_BASE_PATH=/models\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Using jq:\n",
    "!docker inspect {model_name} | jq '.[] | .Config.Env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "796e8de2-21f6-44b6-a28a-5016610350d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_CPP_VMODULE\n"
     ]
    }
   ],
   "source": [
    "# As a toy example, we log into the container and check this environmental variable:\n",
    "!docker exec -it {model_name} /bin/bash | echo TF_CPP_VMODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c53de55d-3939-4e4e-868f-feb547e88e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties of returned container process:\n",
      "\n",
      "_waitpid_lock -> <unlocked _thread.lock object at 0x10c329db0>\n",
      "_input -> None\n",
      "_communication_started -> True\n",
      "----------------------------------------\n",
      "args -> ['docker', 'run', '--rm', '--tty', '-p', '8500:8500', '-p', '8501:8501', '--name', 'half_plus_two', '--mount', 'type=bind,source=/Users/gb/Desktop/tf_server_01/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu,target=/models/half_plus_two', '--env', 'MODEL_NAME=half_plus_two', '--env', 'TF_CPP_VMODULE=http_server=1', '--detach', '--log-driver=json-file', '--log-opt=mode=non-blocking', 'tensorflow/serving:latest']\n",
      "----------------------------------------\n",
      "stdin -> None\n",
      "stdout -> <_io.BufferedReader name=77>\n",
      "stderr -> <_io.BufferedReader name=79>\n",
      "pid -> 53364\n",
      "returncode -> 0\n",
      "encoding -> None\n",
      "errors -> None\n",
      "text_mode -> None\n",
      "_sigint_wait_secs -> 0.25\n",
      "_closed_child_pipe_fds -> True\n",
      "_child_created -> True\n",
      "_fileobj2output -> {<_io.BufferedReader name=77>: [b'0c4399566773c846711a0f49b01dc3cd29790fe769835f96fa7831b6ea83a89e\\n', b''], <_io.BufferedReader name=79>: [b'']}\n"
     ]
    }
   ],
   "source": [
    "def show_tfs_dict(proc):\n",
    "    print(\"Properties of returned container process:\\n\")\n",
    "    for key in proc.__dict__:\n",
    "        if key == 'args':\n",
    "            HR()\n",
    "            print(key, '->', proc.__dict__[key])\n",
    "            HR()\n",
    "        else:\n",
    "            print(key, '->', proc.__dict__[key])\n",
    " \n",
    "show_tfs_dict(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca736e07-ec30-4199-8db6-08dacb4502f8",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"11.0\"></a>\n",
    "# 11. End and clean up processes\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e960478f-93f8-4cff-aa4d-0571124394e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "half_plus_two\n",
      "Total reclaimed space: 0B\n"
     ]
    }
   ],
   "source": [
    "# Kill and remove unused containers, networks, image\n",
    "!docker kill {model_name} && docker system prune --force"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
