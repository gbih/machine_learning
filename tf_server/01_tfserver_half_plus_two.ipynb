{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cf4736-2749-4110-a6c3-753350bf2f79",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "<a name=\"top\"></a><!--Need for Colab-->\n",
    "# Quick introduction to TensorFlow Serving\n",
    "\n",
    "## Half Plus Two model\n",
    "\n",
    "Using Docker, subprocess module, prebuilt Half Plus Two model, and HTTP requests-logging with TensorFlow Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9173c60-1fa4-4d9d-9ad6-eea616ec867e",
   "metadata": {},
   "source": [
    "1. [Setup](#setup)\n",
    "2. [Introduction](#2.0)\n",
    "3. [Using subprocess with TensorFlow Serving](#3.0)\n",
    "4. [Set up the prebuilt half_plus_two model](#4.0)\n",
    "    * 4.1 [Inspect the SavedModel Signature](#4.1)\n",
    "5. [HTTP Request Logging for TensorFlow Serving](#5.0)\n",
    "6. [Running a minimal Docker image with TensorFlow Serving](#6.0)\n",
    "7. [Subprocess to write logs to file](#7.0)\n",
    "    * 7.1 [Verify logs](#7.1)\n",
    "8. [RESTful APIs](#8.0)\n",
    "    * 8.1 [Model status API](#8.1)\n",
    "    * 8.2 [Model Metadata API](#8.2)\n",
    "    * 8.3 [Predict API](#8.3)\n",
    "    * 8.4 [Classify and Regress API](#8.4)\n",
    "9. [Miscellaneous](#9.0)\n",
    "10. [End and clean up processes](#10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060391a-78ad-4cdf-9618-d08a5f45a2cf",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"setup\"></a>\n",
    "# 1. Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60da2bc3-2fc7-46e4-9641-14a5c91a6f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 7.34.0\n",
      "\n",
      "requests  : 2.27.1\n",
      "tensorflow: 2.9.1\n",
      "\n",
      "Finished loading packages..\n"
     ]
    }
   ],
   "source": [
    "# stdlib imports\n",
    "import asyncio\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# third party imports\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "\n",
    "# For debugging, provides version & hardware info\n",
    "try:\n",
    "    %load_ext watermark\n",
    "except ImportError:\n",
    "    print(\"Installing watermark:\")\n",
    "    !pip install watermark -q\n",
    "    %load_ext watermark\n",
    "finally:\n",
    "    %watermark --python --packages requests,tensorflow\n",
    "    \n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "\n",
    "print(\"Finished loading packages..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affb1a3-927d-4ea0-aeba-3d2bccf15d68",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"2.0\"></a><a name=\"2.0\"></a>\n",
    "# 2. Introduction\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We explore these tasks here:\n",
    "\n",
    "* Using subprocess with TensorFlow Serving.\n",
    "* HTTP Request Logging for TensorFlow Serving.\n",
    "* Set up the prebuilt model, half_plus_two.\n",
    "* Running a minimal Docker image with TensorFlow Serving.\n",
    "* Subprocess to write logs to file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c9298-c5f8-4477-b227-62b626e61ff0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"3.0\"></a><a name=\"3.0\"></a>\n",
    "# 3. Using subprocess with TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "\n",
    "\n",
    "A convenient way to use TensorFlow Serving is via Docker. We can operate the server and logging functions via Docker Commands on the CLI, but there are certain advantages (readability, maintenance, security, etc) to wrapping them in Python.\n",
    "\n",
    "The older method of doing this involved using either os.system or os.spawn*. Here, we will instead use the newer subprocess module. This allows us to spawn new processes, connect to their input/output/error pipes, and optionally obtain their return codes. This is a safer analog to os.system().\n",
    "\n",
    "The underlying process creation and management in `subprocess` is done by the [Popen Constructor](https://docs.python.org/3/library/subprocess.html#popen-constructor), `subprocess.Popen`. The underlying Popen interface can be used directly and offers the most flexibility. By default it results in a non-blocking call.\n",
    "\n",
    "Once you've created the Popen instance, some options are:\n",
    "* `wait()`:  to pause until the subprocess has exited.\n",
    "* `poll()`: check if it's exited without pausing.\n",
    "* `communicate()`: interact with process\n",
    "    - Send data to stdin. \n",
    "    - Read data from stdout and stderr, until end-of-file is reached. \n",
    "    - Wait for process to terminate and set the returncode attribute. \n",
    "\n",
    "\n",
    "**Popen Constructor:**\n",
    "\n",
    "<sup>\n",
    "\n",
    "```bash\n",
    "class subprocess.Popen(\n",
    "    args, \n",
    "    bufsize=- 1, \n",
    "    executable=None, \n",
    "    stdin=None, \n",
    "    stdout=None, \n",
    "    stderr=None, \n",
    "    preexec_fn=None, \n",
    "    close_fds=True, \n",
    "    shell=False, \n",
    "    cwd=None, \n",
    "    env=None, \n",
    "    universal_newlines=None, \n",
    "    startupinfo=None, \n",
    "    creationflags=0, \n",
    "    restore_signals=True, \n",
    "    start_new_session=False, \n",
    "    pass_fds=(), \n",
    "    *, \n",
    "    group=None, \n",
    "    extra_groups=None, \n",
    "    user=None, \n",
    "    umask=- 1, \n",
    "    encoding=None, \n",
    "    errors=None, \n",
    "    text=None, \n",
    "    pipesize=- 1\n",
    ")\n",
    "\n",
    "```\n",
    "<br>\n",
    "</sup>\n",
    "    \n",
    "A convenience function built upon the underlying Popen interface is `subprocess.run`. This is a blocking call, as it waits for the command(s) to complete, then return a CompletedProcess instance.\n",
    "\n",
    "There are older high-level APIs, existing prior to Python 3.5. The functionality provided by them has been superceded by `subprocess.Popen` and `subprocess.run`:\n",
    "\n",
    "* `subprocess.call`\n",
    "* `subprocess.check_call`\n",
    "* `subprocess.check_output`\n",
    "\n",
    "\n",
    "**Useful resources on subprocess:**\n",
    "\n",
    "- https://peps.python.org/pep-0324/\n",
    "- https://docs.python.org/3/whatsnew/2.4.html#pep-324-new-subprocess-module\n",
    "- https://docs.python.org/3/library/subprocess.html\n",
    "- https://www.bogotobogo.com/python/python_subprocess_module.php\n",
    "- https://qiita.com/HidKamiya/items/e192a55371a2961ca8a4 (JP)\n",
    "- https://www.programcreek.com/python/example/50/subprocess.Popen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b1648-bc50-4321-82c1-95343ae187f2",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"4.0\"></a><a name=\"4.0\"></a>\n",
    "# 4. Set up the prebuilt half_plus_two model\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "For this example, we will use Docker to deploy Tensorflow Serving and host the toy model **half_plus_two** that computes f(x) = (x / 2) + 2.  This model is found in the Tensorflow Serving Github repository.\n",
    "\n",
    "* https://www.tensorflow.org/tfx/serving/tensorboard\n",
    "* https://www.tensorflow.org/tfx/serving/docker\n",
    "* https://github.com/tensorflow/serving/tree/master/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu/00000123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ad7e03-4db7-4c44-bcbd-07c0f955600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TensorFlow models in 'serving/tensorflow_serving/servables/tensorflow/testdata'\n"
     ]
    }
   ],
   "source": [
    "if not Path('serving').is_dir():\n",
    "    !git clone https://github.com/tensorflow/serving\n",
    "else:\n",
    "    print(\"Saved TensorFlow models in 'serving/tensorflow_serving/servables/tensorflow/testdata'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78141d26-977a-4010-a01a-77eb204c080c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2M\t/Users/gb/Desktop/tf_server_01/serving/tensorflow_serving/servables/tensorflow/testdata\n"
     ]
    }
   ],
   "source": [
    "# Get absolute pathway of demo models\n",
    "TESTDATA=Path(\"serving/tensorflow_serving/servables/tensorflow/testdata\").resolve()\n",
    "!du -hs {TESTDATA}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d71035a-89d5-4f36-9273-1bfe3acd9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model_name and model_dir, used to start the Docker container\n",
    "model_saved = 'saved_model_half_plus_two_cpu'\n",
    "model_dir = (Path() / TESTDATA / model_saved).resolve()\n",
    "model_name = 'half_plus_two'\n",
    "model_newest = max(glob.glob(f\"{model_dir}/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a2628-4eee-4d74-ac76-0debbc18f739",
   "metadata": {},
   "source": [
    "<a id='4.1'></a><a name='4.1'></a>\n",
    "## 4.1 Inspect the SavedModel Signature\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We can use the tool `saved_model_cli` to inspect the SignatureDefs (the callable methods) of a model. This allows us to confirm the input Tensor dtype and shape matches the model.\n",
    "\n",
    "Resources:\n",
    "* https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel\n",
    "* https://www.tensorflow.org/guide/saved_model\n",
    "* https://blog.tensorflow.org/2021/03/a-tour-of-savedmodel-signatures.html\n",
    "* https://www.tensorflow.org/tfx/tutorials/serving/rest_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a659829b-fae3-4b16-b471-29dd2f9b9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tag-sets in the latest SavedModel:\n",
      "----------------------------------------\n",
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "print(\"All tag-sets in the latest SavedModel:\")\n",
    "HR()\n",
    "!saved_model_cli show --dir {model_newest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be57aa21-63d9-4f6c-921d-52d770577e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All available SignatureDef keys in the MetaGraphDef specified by tag-set 'serve':\n",
      "----------------------------------------\n",
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"classify_x_to_y\"\n",
      "SignatureDef key: \"regress_x2_to_y3\"\n",
      "SignatureDef key: \"regress_x_to_y\"\n",
      "SignatureDef key: \"regress_x_to_y2\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "print(\"All available SignatureDef keys in the MetaGraphDef specified by tag-set 'serve':\")\n",
    "HR()\n",
    "!saved_model_cli show --dir {model_newest} --tag_set serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ada6c3ee-d040-4e0f-96e3-3f3eb2a6e564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All inputs and outputs TensorInfo for the specific SignatureDef 'serving_default' in the MetaGraph:\n",
      "Note that shape: (-1) implies 1-D shape for inference data, eg {'examples': [{...}]}\n",
      "----------------------------------------\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['x'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: x:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['y'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: y:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "print(\"All inputs and outputs TensorInfo for the specific SignatureDef 'serving_default' in the MetaGraph:\")\n",
    "print(\"Note that shape: (-1) implies 1-D shape for inference data, eg {'examples': [{...}]}\")\n",
    "HR()\n",
    "\n",
    "!saved_model_cli show --dir {model_newest} \\\n",
    "    --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "087391ab-fd5a-4356-9b4c-821cf99a1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['classify_x_to_y']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['inputs'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: unknown_rank\n",
      "        name: tf_example:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['scores'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: y:0\n",
      "  Method name is: tensorflow/serving/classify\n",
      "\n",
      "signature_def['regress_x2_to_y3']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['inputs'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: x2:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['outputs'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: y3:0\n",
      "  Method name is: tensorflow/serving/regress\n",
      "\n",
      "signature_def['regress_x_to_y']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['inputs'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: unknown_rank\n",
      "        name: tf_example:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['outputs'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: y:0\n",
      "  Method name is: tensorflow/serving/regress\n",
      "\n",
      "signature_def['regress_x_to_y2']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['inputs'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: unknown_rank\n",
      "        name: tf_example:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['outputs'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: y2:0\n",
      "  Method name is: tensorflow/serving/regress\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['x'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: x:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['y'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: y:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "# Show all available information in the SavedModel\n",
    "\n",
    "!saved_model_cli show --dir {model_newest} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96831a-b115-436f-9acb-bafe2afb42f9",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"5.0\"></a><a name=\"5.0\"></a>\n",
    "# 5. HTTP Request Logging for TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* The easiest way to enable logging for TensorFlow-Model-Serving is via environment variables. This is not specific to TensorFlow Serving, but general to TensorFlow.\n",
    "\n",
    "* `TF_CPP_MIN_VLOG_LEVEL` enables logging of the main C++ backend. However, even the lowest setting of `TF_CPP_MIN_VLOG_LEVEL=1` results in too much noise.\n",
    "\n",
    "* `TF_CPP_VMODULE` provides a way to constrain logging to specific modules or source files. The general format is `TF_CPP_VMODULE=<module_name>=1`, where the module name can be either the C++ or Python file name (without the extension).\n",
    "\n",
    "* Here, we can activate logging individually for http_server.cc via \n",
    "`TF_CPP_VMODULE=http_server=1`. This will enable simple HTTP request logging and errors.\n",
    "\n",
    "* To use this with Docker, we pass it as an environmental variable:  `--env TF_CPP_VMODULE=http_server=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089282c-ff60-40ff-a90b-4f31f7085ddd",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.0\"></a>\n",
    "# 6. Running a minimal Docker image with TensorFlow Serving\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* This is the original Docker command-line format.\n",
    "\n",
    "* `docker run` also pulls a docker image (or repository) from the docker registry, if it doesn't already exist locally.\n",
    "\n",
    "\n",
    "This docker image features\n",
    "\n",
    "* Port 8500 exposed for gRPC\n",
    "* Port 8501 exposed for the REST API\n",
    "* Optional environment variable MODEL_NAME (defaults to model)\n",
    "* Optional environment variable MODEL_BASE_PATH (defaults to /models)\n",
    "\n",
    "---\n",
    "\n",
    "It can be easier to first to experiment and set up the Docker cli-commands first, then later wrap with subprocess.Popen in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abbeb4a9-b1e1-475a-897c-b7d3a83d0569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "docker run \\\n",
      "--rm --tty -p 8500:8500 -p 8501:8501 \\\n",
      "--name half_plus_two \\\n",
      "--mount type=bind,source=/Users/gb/Desktop/tf_server_01/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu,target=/models/half_plus_two \\\n",
      "--env MODEL_NAME=half_plus_two \\\n",
      "--env TF_CPP_VMODULE='http_server=1' \\\n",
      "--detach \\\n",
      "--log-driver=json-file \\\n",
      "--log-opt=mode=non-blocking \\\n",
      "tensorflow/serving:latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd_cli = f\"\"\"\n",
    "docker run \\\\\n",
    "--rm --tty -p 8500:8500 -p 8501:8501 \\\\\n",
    "--name {model_name} \\\\\n",
    "--mount type=bind,source={model_dir},target=/models/{model_name} \\\\\n",
    "--env MODEL_NAME={model_name} \\\\\n",
    "--env TF_CPP_VMODULE='http_server=1' \\\\\n",
    "--detach \\\\\n",
    "--log-driver=json-file \\\\\n",
    "--log-opt=mode=non-blocking \\\\\n",
    "tensorflow/serving:latest\n",
    "\"\"\"\n",
    "\n",
    "print(cmd_cli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9427b25-0a8c-4064-a4d5-facc4242c010",
   "metadata": {},
   "source": [
    "---\n",
    "For more feedback, run these commands in different terminals before instantiating the Docker container:\n",
    "\n",
    "```bash\n",
    "$ watch docker ps\n",
    "$ docker logs -f half_plus_two\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a385010-45b1-4665-b44f-244cbd538cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_docker():  \n",
    "        \n",
    "    cmd=f\"\"\"\n",
    "docker run\n",
    "--rm --tty -p 8500:8500 -p 8501:8501\n",
    "--name {model_name}\n",
    "--mount type=bind,source={model_dir},target=/models/{model_name}\n",
    "--env MODEL_NAME={model_name}\n",
    "--env TF_CPP_VMODULE='http_server=1'\n",
    "--detach\n",
    "--log-driver=json-file\n",
    "--log-opt=mode=non-blocking\n",
    "tensorflow/serving:latest\n",
    "\"\"\"\n",
    "           \n",
    "    try:\n",
    "        proc = subprocess.Popen(\n",
    "            shlex.split(cmd),\n",
    "            stdout = subprocess.PIPE,\n",
    "            stderr = subprocess.PIPE\n",
    "        )\n",
    "        \n",
    "        # The communicate() method returns a tuple (stdoutdata, stderrdata)\n",
    "        # It only reads data from stdout and stderr.\n",
    "        out, err = proc.communicate()\n",
    "        out = out.decode()\n",
    "        err = err.decode()\n",
    "        print(f\"out: {(out.strip())}\")\n",
    "        \n",
    "        if err:\n",
    "            print(f\"err: {err}\")\n",
    "                        \n",
    "        sleep_time = 0.5 # Small time delay for docker instance to start up\n",
    "        time.sleep(sleep_time)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Subprocess error: {e.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    return proc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e9fff1-8e7d-44e6-97ff-654933524083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: caba9295feb2aa2c568cd7f9d163dc0dc93ee91bbbc9110488fb3735da08d75d\n"
     ]
    }
   ],
   "source": [
    "# Instantiate TFS with our model \n",
    "proc = server_docker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a27e02-1b25-4e57-b559-89abe79672ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If need to immediately kill and remove unused containers/networks/images\n",
    "# !docker kill {model_name} && docker system prune --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b6e0c-c033-4d7e-8bf0-352cdb5ce426",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a name=\"7.0\"></a>\n",
    "# 7. Subprocess to write logs to file\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "We can always access logs via `docker logs <container name>` on the CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86fd673b-6cf8-46f1-8a31-45eb0deb06d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-07 23:49:17.507844: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled\n",
      "2022-08-07 23:49:17.512011: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2022-08-07 23:49:17.514273: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n"
     ]
    }
   ],
   "source": [
    "!docker logs --tail 5 {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83881179-2452-4a25-8f8c-ed0fdd7710cf",
   "metadata": {},
   "source": [
    "---\n",
    "We can also create a subprocess that redirects the logs to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d07d828a-39a5-441f-b723-771f9279546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_docker_process(log_file):  \n",
    "    cmd = f\"docker logs --follow {model_name}\"   \n",
    "    try:\n",
    "        proc = subprocess.Popen(\n",
    "            shlex.split(cmd),\n",
    "            stdout=open(log_file, 'w'),\n",
    "            stderr=subprocess.STDOUT, # redirect to stdout\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff894074-69a5-405e-b659-2090bbc310f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_file: half_plus_two_tfs.log\n",
      "  0B\thalf_plus_two_tfs.log\n"
     ]
    }
   ],
   "source": [
    "log_file = f\"{model_name}_tfs.log\"\n",
    "print(f\"log_file: {log_file}\")\n",
    "\n",
    "log_docker_process(log_file)\n",
    "\n",
    "!du -hs {log_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca708a6-2bd5-4426-a52c-79a6b20218ef",
   "metadata": {},
   "source": [
    "<a id='7.1'></a><a name='7.1'></a>\n",
    "## 7.1 Verify logs\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Check that requests are being logged to the log file via the subprocess `log_docker_process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "722adb7c-5425-497a-afd3-372fa0e1f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> TFS Status : Mon, 08 Aug 2022 01:26:09 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:09 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:09 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:10 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:10 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:10 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:10 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:10 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:11 GMT <Response [200]>\n",
      "===> TFS Status : Mon, 08 Aug 2022 01:26:11 GMT <Response [200]>\n",
      "----------------------------------------\n",
      " 44K\thalf_plus_two_tfs.log\n",
      "----------------------------------------\n",
      "2022-08-08 01:26:09.427568: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:09.646680: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:09.865409: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:10.087928: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:10.304002: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:10.523174: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:10.740902: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:10.957131: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:11.170536: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n",
      "2022-08-08 01:26:11.389176: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    resp = request_status_rest()\n",
    "    print(f\"===> TFS Status : {resp.headers['Date']} {resp}\")\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "HR()\n",
    "    \n",
    "!du -h {log_file}\n",
    "HR()\n",
    "!tail -10 {log_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34158591-58d2-4773-a110-43fd22b0bcbc",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"8.0\"></a>\n",
    "# 8. RESTful APIs\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Send requests with TensorFlow ModelServer RESTful APIs\n",
    "\n",
    "Reference:\n",
    "\n",
    "* https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/api_rest.md\n",
    "* https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/tensorflow_model_server_test.py#L520\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9220203f-7e84-4f19-84bd-7d3e3fb41537",
   "metadata": {},
   "source": [
    "<a name=\"8.1\"></a>\n",
    "## 8.1 Model status API\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* Returns the status of a model in the ModelServer.\n",
    "* If successful, returns a JSON representation of `GetModelStatusResponse` protobuf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3e6f8574-39aa-4583-b29b-64c20bbd0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'model_version_status': [ { 'state': 'AVAILABLE',\n",
      "                              'status': { 'error_code': 'OK',\n",
      "                                          'error_message': ''},\n",
      "                              'version': '123'}]}\n",
      "----------------------------------------\n",
      "2022-08-08 01:20:56.911064: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two body: 0 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp_data = requests.get(f'http://localhost:8501/v1/models/{model_name}')\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "pp.pprint(resp_data.json())\n",
    "\n",
    "HR()\n",
    "!tail -1 {log_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb61adc-d6c9-4dfb-997e-628dfa8c9a7c",
   "metadata": {},
   "source": [
    "<a name=\"8.2\"></a>\n",
    "## 8.2 Model Metadata API\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* Returns the metadata of a model in the ModelServer. For example, we can inspect the details of the input and output tensors.\n",
    "* Returns a JSON representation of `GetModelMetadataResponse` protobuf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8ce22817-0262-47a6-af70-36a473c50d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"{'model_spec': {'name': 'half_plus_two', 'signature_name': '', 'version': \"\n",
      " \"'123'}, 'metadata': {'signature_def': {'signature_def': {'regress_x_to_y': \"\n",
      " \"{'inputs': {'inputs': {'dtype': 'DT_STRING', 'tensor_shape': {'dim': [], \"\n",
      " \"'unknown_rank': True}, 'name': 'tf_example:0'}}, 'outputs': {'outputs': \"\n",
      " \"{'dtype': 'DT_FLOAT', 'tensor_shape': {'dim': [{'size': '-1', 'name': ''}, \"\n",
      " \"{'size': '1', 'name': ''}], 'unknown_rank': False}, 'name': 'y:0'}}, \"\n",
      " \"'method_name': 'tensorflow/serving/regress'}, 'regress_x_to_y2': {'inputs': \"\n",
      " \"{'inputs': {'dtype': 'DT_STRING', 'tensor_shape': {'dim': [], \"\n",
      " \"'unknown_rank': True}, 'name': 'tf_example:0'}}, 'outputs': {'outputs': \"\n",
      " \"{'dtype': 'DT_FLOAT', 'tensor_shape': {'dim': [{'size': '-1', 'name': ''}, \"\n",
      " \"{'size': '1', 'name': ''}], 'unknown_rank': False}, 'name': 'y2:0'}}, \"\n",
      " \"'method_name': 'tensorflow/serving/regress'}, 'classify_x_to_y': {'inputs': \"\n",
      " \"{'inputs': {'dtype': 'DT_STRING', 'tensor_shape': {'dim': [], \"\n",
      " \"'unknown_rank': True}, 'name': 'tf_example:0'}}, 'outputs': {'scores': \"\n",
      " \"{'dtype': 'DT_FLOAT', 'tensor_shape': {'dim': [{'size': '-1', 'name': ''}, \"\n",
      " \"{'size': '1', 'name': ''}], 'unknown_rank': False}, 'name': 'y:0'}}, \"\n",
      " \"'method_name': 'tensorflow/serving/classify'}, 'regress_x2_to_y3': \"\n",
      " \"{'inputs': {'inputs': {'dtype': 'DT_FLOAT', 'tensor_shape': {'dim': \"\n",
      " \"[{'size': '-1', 'name': ''}, {'size': '1', 'name': ''}], 'unknown_rank': \"\n",
      " \"False}, 'name': 'x2:0'}}, 'outputs': {'outputs': {'dtype': 'DT_FLOAT', \"\n",
      " \"'tensor_shape': {'dim': [{'size': '-1', 'name': ''}, {'size': '1', 'name': \"\n",
      " \"''}], 'unknown_rank': False}, 'name': 'y3:0'}}, 'method_name': \"\n",
      " \"'tensorflow/serving/regress'}, 'serving_default': {'inputs': {'x': {'dtype': \"\n",
      " \"'DT_FLOAT', 'tensor_shape': {'dim': [{'size': '-1', 'name': ''}, {'size': \"\n",
      " \"'1', 'name': ''}], 'unknown_rank': False}, 'name': 'x:0'}}, 'outputs': {'y': \"\n",
      " \"{'dtype': 'DT_FLOAT', 'tensor_shape': {'dim': [{'size': '-1', 'name': ''}, \"\n",
      " \"{'size': '1', 'name': ''}], 'unknown_rank': False}, 'name': 'y:0'}}, \"\n",
      " \"'method_name': 'tensorflow/serving/predict'}}}}}\")\n",
      "----------------------------------------\n",
      "HTTP request log output:\n",
      "\n",
      "2022-08-08 01:27:59.068706: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: GET /v1/models/half_plus_two/metadata body: 0 bytes.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resp_data = requests.get(f'http://localhost:8501/v1/models/{model_name}/metadata')\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "json_dump = (resp_data.json())\n",
    "pp.pprint(repr(json_dump))\n",
    "\n",
    "HR()\n",
    "\n",
    "print(\"HTTP request log output:\\n\")\n",
    "!tail -1 {log_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ea0b7-d7c8-4369-9345-ce6a835f1e1a",
   "metadata": {},
   "source": [
    "<a name=\"8.3\"></a>\n",
    "## 8.3 Predict API\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "* The request body for predict API must be JSON object formatted as follows (for clarity, it may be easier to always include the \"signature_name\" field):\n",
    "\n",
    "```bash\n",
    "{\n",
    "  // (Optional) Serving signature to use.\n",
    "  // If unspecifed default serving signature is used.\n",
    "  \"signature_name\": <string>,\n",
    "\n",
    "  // Input Tensors in row (\"instances\") or columnar (\"inputs\") format.\n",
    "  // A request can have either of them but NOT both.\n",
    "  \"instances\": <value>|<(nested)list>|<list-of-objects>\n",
    "  \"inputs\": <value>|<(nested)list>|<object>\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* The predict request returns a JSON object in response body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "922891fe-6c5f-4cbe-ac83-58f0552e8cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [2.5, 3.0, 4.5\n",
      "    ]\n",
      "}\n",
      "----------------------------------------\n",
      "HTTP request log output:\n",
      "\n",
      "2022-08-08 01:28:07.891367: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: POST /v1/models/half_plus_two:predict body: 50 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Command line\n",
    "!curl -d '{\"signature_name\": \"\", \"instances\": [1.0,2.0,5.0]}' \\\n",
    "    -H 'cache-control: no-cache' \\\n",
    "    -X POST http://localhost:8501/v1/models/half_plus_two:predict\n",
    "\n",
    "print()\n",
    "HR()\n",
    "\n",
    "print(\"HTTP request log output:\\n\")\n",
    "!tail -1 {log_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "40368920-c281-458e-bb9f-e9b60aa69458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload:\t{\"instances\": [1.0, 2.0, 5.0, -9]}\n",
      "----------------------------------------\n",
      "Predictions:\t[2.5, 3.0, 4.5, -2.5]\n",
      "----------------------------------------\n",
      "Properties of response:\n",
      "\n",
      "{ '_content': b'{\\n    \"predictions\": [2.5, 3.0, 4.5, -2.5\\n    ]\\n}',\n",
      "  '_content_consumed': True,\n",
      "  '_next': None,\n",
      "  'connection': <requests.adapters.HTTPAdapter object at 0x1261ed2e0>,\n",
      "  'cookies': <RequestsCookieJar[]>,\n",
      "  'elapsed': datetime.timedelta(microseconds=12732),\n",
      "  'encoding': 'utf-8',\n",
      "  'headers': {'Content-Type': 'application/json', 'Date': 'Mon, 08 Aug 2022 01:28:14 GMT', 'Content-Length': '49'},\n",
      "  'history': [],\n",
      "  'raw': <urllib3.response.HTTPResponse object at 0x1261edc10>,\n",
      "  'reason': 'OK',\n",
      "  'request': <PreparedRequest [POST]>,\n",
      "  'status_code': 200,\n",
      "  'url': 'http://localhost:8501/v1/models/half_plus_two:predict'}\n",
      "----------------------------------------\n",
      "HTTP request log output:\n",
      "\n",
      "2022-08-08 01:28:14.351032: I tensorflow_serving/model_servers/http_server.cc:162] Processing HTTP request: POST /v1/models/half_plus_two:predict body: 34 bytes.\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "\n",
    "# Payload\n",
    "data = json.dumps({\n",
    "    \"instances\": [1.0, 2.0, 5.0, -9]\n",
    "})\n",
    "print(f\"Payload:\\t{data}\")\n",
    "HR()\n",
    "\n",
    "headers = {\n",
    "    \"content-type\": \"application_json\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f'http://localhost:8501/v1/models/{model_name}:predict',\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "print(f\"Predictions:\\t{json.loads(response.text)['predictions']}\")\n",
    "HR()\n",
    "\n",
    "print(\"Properties of response:\\n\")\n",
    "pp.pprint(response.__dict__)\n",
    "\n",
    "HR()\n",
    "\n",
    "print(\"HTTP request log output:\\n\")\n",
    "!tail -1 {log_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83f280-58a8-4d8c-8aba-9e39a65dcf84",
   "metadata": {},
   "source": [
    "<a name=\"8.4\"></a>\n",
    "## 8.4 Classify and Regress API\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "To-do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc75b1-c520-4ece-85ac-5836a01155c8",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"9.0\"></a>\n",
    "# 9. Miscellaneous\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9211b5ea-7ae9-4180-8742-325e17c83dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "HOSTNAME=caba9295feb2\n",
      "MODEL_NAME=half_plus_two\n",
      "TF_CPP_VMODULE=http_server=1\n",
      "MODEL_BASE_PATH=/models\n",
      "HOME=/root\n"
     ]
    }
   ],
   "source": [
    "# Check environmental variables of this container\n",
    "!docker exec {model_name} env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fdc70e1e-d4a8-4f83-8619-cc98ef52a7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m[\n",
      "  \u001b[0;32m\"MODEL_NAME=half_plus_two\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"TF_CPP_VMODULE=http_server=1\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0;32m\"MODEL_BASE_PATH=/models\"\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Using jq:\n",
    "!docker inspect {model_name} | jq '.[] | .Config.Env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "796e8de2-21f6-44b6-a28a-5016610350d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_CPP_VMODULE\n"
     ]
    }
   ],
   "source": [
    "# As a toy example, we log into the container and check this environmental variable:\n",
    "!docker exec -it {model_name} /bin/bash | echo TF_CPP_VMODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c53de55d-3939-4e4e-868f-feb547e88e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties of returned container process:\n",
      "\n",
      "_waitpid_lock -> <unlocked _thread.lock object at 0x126053690>\n",
      "_input -> None\n",
      "_communication_started -> True\n",
      "----------------------------------------\n",
      "args -> ['docker', 'run', '--rm', '--tty', '-p', '8500:8500', '-p', '8501:8501', '--name', 'half_plus_two', '--mount', 'type=bind,source=/Users/gb/Desktop/tf_server_01/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu,target=/models/half_plus_two', '--env', 'MODEL_NAME=half_plus_two', '--env', 'TF_CPP_VMODULE=http_server=1', '--detach', '--log-driver=json-file', '--log-opt=mode=non-blocking', 'tensorflow/serving:latest']\n",
      "----------------------------------------\n",
      "stdin -> None\n",
      "stdout -> <_io.BufferedReader name=70>\n",
      "stderr -> <_io.BufferedReader name=78>\n",
      "pid -> 6242\n",
      "returncode -> 0\n",
      "encoding -> None\n",
      "errors -> None\n",
      "text_mode -> None\n",
      "_sigint_wait_secs -> 0.25\n",
      "_closed_child_pipe_fds -> True\n",
      "_child_created -> True\n",
      "_fileobj2output -> {<_io.BufferedReader name=70>: [b'caba9295feb2aa2c568cd7f9d163dc0dc93ee91bbbc9110488fb3735da08d75d\\n', b''], <_io.BufferedReader name=78>: [b'']}\n"
     ]
    }
   ],
   "source": [
    "def show_tfs_dict(proc):\n",
    "    print(\"Properties of returned container process:\\n\")\n",
    "    for key in proc.__dict__:\n",
    "        if key == 'args':\n",
    "            HR()\n",
    "            print(key, '->', proc.__dict__[key])\n",
    "            HR()\n",
    "        else:\n",
    "            print(key, '->', proc.__dict__[key])\n",
    " \n",
    "show_tfs_dict(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca736e07-ec30-4199-8db6-08dacb4502f8",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"10.0\"></a>\n",
    "# 10. End and clean up processes\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e960478f-93f8-4cff-aa4d-0571124394e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "half_plus_two\n",
      "Total reclaimed space: 0B\n"
     ]
    }
   ],
   "source": [
    "# Kill and remove unused containers, networks, image\n",
    "!docker kill {model_name} && docker system prune --force"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
