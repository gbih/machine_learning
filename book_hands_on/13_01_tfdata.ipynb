{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x132t4TjetWR"
   },
   "source": [
    "<a id='top'></a><a name='top'></a>\n",
    "# Chapter 13 – Loading and Preprocessing Data with TensorFlow\n",
    "\n",
    "[Version 3](https://github.com/ageron/handson-ml3/blob/main/13_loading_and_preprocessing_data.ipynb)\n",
    "\n",
    "The original notebook is split into two separate notebooks, due to length:\n",
    "\n",
    "1. tf.data\n",
    "2. TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-or2dtZetWU"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/13_loading_and_preprocessing_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h48uHI4getWW"
   },
   "source": [
    "1. [Setup](#setup)<a name=\"setup_top\"></a>\n",
    "2. [The Data API](#2.0)<a name=\"1.0_top\"></a>\n",
    "    * [2.1 Chaining transformations](#2.1)\n",
    "        - [2.1.1 Dataset methods and immutable datasets](#2.1.1)\n",
    "    * [2.2 Simple transformations](#2.2)\n",
    "    * [2.3 More complicated chaining transformations](#2.3)\n",
    "    * [2.4 Shuffling the Data](#2.4)\n",
    "3.  [tf.data API: End-to-end example](#3.0)\n",
    "    * [3.1 Load and split dataset to multiple CSV files](#3.1)\n",
    "    * [3.2 Building an Input Pipeline](#3.2)\n",
    "    * [3.3 Interleaving lines from multiple files](#3.3)\n",
    "    * [3.4 Preprocessing the Data](#3.4)\n",
    "    * [3.5 Putting everything together (w/o Prefetching)](#3.5)\n",
    "    * [3.6 Putting everything together (with Prefetching)](#3.6)\n",
    "    * [3.7 Using the Dataset with tf.keras](#3.7)\n",
    "    * [3.8 Custom training loop](#38)\n",
    "    * [3.9 Creating a TF Function to perform training loop](#3.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtlmP_s1etWZ"
   },
   "source": [
    "---\n",
    "<a id='setup'></a><a name='setup'></a>\n",
    "# 1. Setup\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QeQ2qdjietWa",
    "outputId": "3ed1481d-f682-404e-cfbe-fd4971570807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded libraries..\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "# global seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Installing tensorflow_transform takes a long time on COLAB,\n",
    "# should wait until we actually need it, which is later.\n",
    "# if IS_COLAB or IS_KAGGLE:\n",
    "#     !pip install tensorflow_transform -q\n",
    "\n",
    "def HR():\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "print(\"Loaded libraries..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7bqYQ7gWetWb"
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data_chp13'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uarlCpyZetWd"
   },
   "source": [
    "---\n",
    "<a id='2.0'></a><a name='2.0'></a>\n",
    "# 2. tf.data API: Introduction\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQH3Ao_xetWe"
   },
   "source": [
    "<a id='2.1'></a><a name='2.1'></a>\n",
    "## 2.1 Creating tf.data.Dataset\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "The easiest way to create a tf.data.Dataset from in-memory data is via `tf.data.Dataset.from_tensor_slices()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tv5pzYx8etWf",
    "outputId": "b1ca39b1-c08b-4848-9a57-e90ccceb6622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "----------------------------------------\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(9, shape=(), dtype=int64)\n",
      "----------------------------------------\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 00:36:25.919472: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def listing_2_1():\n",
    "    \n",
    "    # Creates a sequence of numbers, equivalent to np.arange\n",
    "    X = tf.range(10)\n",
    "\n",
    "    dataset1 = tf.data.Dataset.from_tensor_slices(X)\n",
    "    for item in dataset1:\n",
    "        print(item)\n",
    "    \n",
    "    HR()\n",
    "    \n",
    "    # Alternative using tf.data.Dataset.range\n",
    "    dataset2 = tf.data.Dataset.range(10)\n",
    "    for item in dataset2:\n",
    "        print(item)\n",
    "    \n",
    "    HR()\n",
    "    \n",
    "    X_nested = {\"a\": ([1,2,3], [4,5,6]), \"b\": [7,8,9]}\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    \n",
    "listing_2_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y3HQERqetWj",
    "tags": []
   },
   "source": [
    "<a id='2.1.1'></a><a name='2.1.1'></a>\n",
    "### 2.1.1 Dataset methods and immutable datasets\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Key points:\n",
    "\n",
    "* All tensors are immutable (similar to Python numbers and strings): \n",
    "you can never update the contents of a tensor, only create a new one,\n",
    "or in this case, essentially rebind to the same variable name.\n",
    "The value at the memory address is not changed.\n",
    "* In the case of dataset methods, they do not modify datasets, they create new ones, so we have to make sure to keep a reference to these new datasets (e.g., with = ... ), or else there is no binding going on.\n",
    "* For convenience with these immutable datasets, we can *reuse* variable names (eg `dataset`): \n",
    "    - In programming languages like Elixir, this is referred to as rebinding, used just as a convenience. \n",
    "    - In F#, we similarly rebind a variable identifier, not change the value at that memory location. This shadows the original name, where F# will create a new name internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a5M2WMOetWj",
    "outputId": "5c08e2ea-ff3b-473d-d317-b581a8fca1fd",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4308535824: memory address of variable a\n",
      "4308535856: memory address of variable b\n",
      "var a and b point to different memory addresses.\n",
      "\n",
      "4308535856: memory address of variable a\n",
      "var a and b now point to the same address.\n",
      "----------------------------------------\n",
      "4944869504: memory address of first variable dataset\n",
      "4944869360: memory address of second variable dataset\n",
      "----------------------------------------\n",
      "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def listing2_1_1():\n",
    "\n",
    "    # Testing passing by reference in Python\n",
    "    a = 1\n",
    "    print(f\"{(id(a))}: memory address of variable a\")\n",
    "    \n",
    "    b = 2\n",
    "    print(f\"{(id(b))}: memory address of variable b\")\n",
    "    \n",
    "    print(\"var a and b point to different memory addresses.\\n\")\n",
    "        \n",
    "    # Passing-by-reference in Python\n",
    "    # Python variables work with references to objects representing the values.\n",
    "    # Here, we have made both a and b references to the same object.\n",
    "    a = b\n",
    "    print(f\"{(id(a))}: memory address of variable a\")\n",
    "    \n",
    "    try:\n",
    "        assert id(a) == id(b)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {repr(e)}\")\n",
    "    else:\n",
    "        print(\"var a and b now point to the same address.\")\n",
    "    \n",
    "    HR()\n",
    "    \n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "    dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "    print(f\"{(id(dataset))}: memory address of first variable dataset\")\n",
    "\n",
    "    dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
    "    print(f\"{(id(dataset))}: memory address of second variable dataset\")\n",
    "    \n",
    "    HR()\n",
    "    \n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "        \n",
    "listing2_1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJZKhcXSetWg"
   },
   "source": [
    "<a id='2.2'></a><a name='2.2'></a>\n",
    "## 2.2 Simple transformations\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Once we have a dataset, we can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQeRY1naetWg",
    "outputId": "41f97600-af1c-4005-8fa3-5a83d7193a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original dataset, range [1..10]:\n",
      "\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "----------------------------------------\n",
      "2. map and lambda (x * 2):\n",
      "\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "----------------------------------------\n",
      "3. filter and lambda (x > 3):\n",
      "\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "----------------------------------------\n",
      "4. filter, lambda, reduce (tf.reduce_sum(x) > 3):\n",
      "\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def listing2_2():\n",
    "\n",
    "    # Create initial dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "    print(\"1. Original dataset, range [1..10]:\\n\")\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    HR()\n",
    "    \n",
    "    dataset2 = dataset.map(lambda x: x * 2)\n",
    "    print(\"2. map and lambda (x * 2):\\n\")\n",
    "    for item in dataset2:\n",
    "        print(item)\n",
    "    HR()\n",
    "    \n",
    "    dataset3 = dataset.filter(lambda x: x > 3)\n",
    "    print(\"3. filter and lambda (x > 3):\\n\")\n",
    "    for item in dataset3:\n",
    "        print(item)\n",
    "    HR()\n",
    "            \n",
    "    dataset4 = dataset.filter(lambda x: tf.reduce_sum(x) > 3)\n",
    "    print(\"4. filter, lambda, reduce (tf.reduce_sum(x) > 3):\\n\")\n",
    "    for item in dataset4:\n",
    "        print(item)\n",
    "        \n",
    "listing2_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI9-7NPUetWi"
   },
   "source": [
    "<a id='2.3'></a><a name='2.3'></a>\n",
    "## 2.3 More complicated chaining transformations\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Here we use the same `dataset` variable. We end up essentially doing a single very long chaining-transformation on it.\n",
    "\n",
    "**Note**: \n",
    "\n",
    "We are not really mutating it, but continually binding the new value to a different object (even though it has the same name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cq4m3wUsetWi",
    "outputId": "2fc33055-fecb-46ba-f6ff-bd3395fca974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original dataset, range [1..10], repeat(3), batch(7):\n",
      "\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
      "----------------------------------------\n",
      "2. map and lambda (x * 2):\n",
      "\n",
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n",
      "----------------------------------------\n",
      "3. filter, lambda, reduce (tf.reduce_sum(x) > 50):\n",
      "\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "----------------------------------------\n",
      "dataset.take(2):\n",
      "\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def listing2_3():\n",
    "\n",
    "    # Create initial dataset\n",
    "    dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "        .repeat(3)\n",
    "        .batch(7)\n",
    "    )\n",
    "    print(\"1. Original dataset, range [1..10], repeat(3), batch(7):\\n\")\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    HR()\n",
    "        \n",
    "        \n",
    "    dataset = dataset.map(lambda x: x * 2)\n",
    "    print(\"2. map and lambda (x * 2):\\n\")\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    HR()\n",
    "        \n",
    "    dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "    print(\"3. filter, lambda, reduce (tf.reduce_sum(x) > 50):\\n\")\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    HR()\n",
    "    \n",
    "    print(\"dataset.take(2):\\n\")\n",
    "    for item in dataset.take(2):\n",
    "        print(item)\n",
    "        \n",
    "listing2_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def listing2_3b():\n",
    "\n",
    "    dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "        .repeat(3)\n",
    "        .batch(7)\n",
    "        .map(lambda x: x * 2)\n",
    "        .filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "    )\n",
    "\n",
    "    for item in dataset.take(2):\n",
    "        print(item)\n",
    "    \n",
    "listing2_3b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpWHNa6SetWl"
   },
   "source": [
    "<a id='2.4'></a><a name='2.4'></a>\n",
    "## 2.4 Shuffling the Data\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Gradient Descent works best when the instances in the training set are i.i.d. We can use Python method chaining to shuffle the instances as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8evo2n6tetWl",
    "outputId": "21326186-015a-4ba4-a9d6-af75a3356f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No shuffling:\n",
      "\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9], shape=(6,), dtype=int64)\n",
      "----------------------------------------\n",
      "With shuffling:\n",
      "\n",
      "tf.Tensor([3 0 1 6 2 5 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 4 1 9 4 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 5 0 8 9 6], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def listing2_4():\n",
    "\n",
    "    dataset = (tf.data.Dataset\n",
    "               .range(10)\n",
    "               .repeat(2)\n",
    "               .batch(7)\n",
    "              )\n",
    "    print(\"No shuffling:\\n\")\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    HR()\n",
    "    \n",
    "    dataset = (tf.data.Dataset\n",
    "               .range(10)\n",
    "               .repeat(2)\n",
    "               .shuffle(buffer_size=4, seed=42)\n",
    "               .batch(7)\n",
    "              )\n",
    "    print(\"With shuffling:\\n\")\n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    \n",
    "listing2_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a id='3.0'></a><a name='3.0'></a>\n",
    "# 3. tf.data API: End-to-end example\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "The tasks to explore here are:\n",
    "\n",
    "* Interleaving lines from multiple lines\n",
    "* Build an input pipeline\n",
    "* Preprocess the data\n",
    "* Prefetching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX03m1TJetWm"
   },
   "source": [
    "<a id='3.1'></a><a name='3.1'></a>\n",
    "## 3.1 Load and split dataset to multiple CSV files\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Fetch, split and normalize the [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html). \n",
    "\n",
    "---\n",
    "\n",
    "**Note**:\n",
    "\n",
    "[Example for reshape API:](https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape)\n",
    "\n",
    "<sup>\n",
    "    \n",
    "```python\n",
    "import numpy as np\n",
    "x = np.array([[2,3,4], [5,6,7]]) \n",
    "\n",
    "# Convert any shape to 1D shape\n",
    "x = np.reshape(x, (-1)) # Making it 1 row -> (6,)\n",
    "\n",
    "# When you don't care about rows and just want to fix number of columns\n",
    "x = np.reshape(x, (-1, 1)) # Making it 1 column -> (6, 1)\n",
    "x = np.reshape(x, (-1, 2)) # Making it 2 column -> (3, 2)\n",
    "x = np.reshape(x, (-1, 3)) # Making it 3 column -> (2, 3)\n",
    "\n",
    "# When you don't care about columns and just want to fix number of rows\n",
    "x = np.reshape(x, (1, -1)) # Making it 1 row -> (1, 6)\n",
    "x = np.reshape(x, (2, -1)) # Making it 2 row -> (2, 3)\n",
    "x = np.reshape(x, (3, -1)) # Making it 3 row -> (3, 2)\n",
    "```\n",
    "    \n",
    "</sup>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qtVyPuYcetWm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target.reshape(-1, 1),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(type(X_train))\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_mean = scaler.mean_\n",
    "# X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNj6CjueetWm"
   },
   "source": [
    "For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, start by splitting the housing dataset and save it to 20 CSV files.\n",
    "\n",
    "---\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "`pathlib.Path()` provides more functionality than `os.path.join`.\n",
    "\n",
    "`pathlib.Path()` returns a string, while `os.path.join` returns a pathlib.PosixPath. Also, pathlib.Path() also offers many built-in methods for file and directory handling, such as `mkdir`:\n",
    "\n",
    "\n",
    "<sup> \n",
    "    \n",
    "```\n",
    "absolute, anchor, as_posix, as_uri, chmod, cwd, drive, exists, expanduser, glob, group, home, is_absolute, is_block_device, is_char_device, is_dir, is_fifo, is_file, is_mount, is_reserved, is_socket, is_symlink, iterdir, joinpath, lchmod, link_to, lstat, match, mkdir, name, open, owner, parent, parents, parts, read_bytes, read_text, relative_to, rename, replace, resolve, rglob, rmdir, root, samefile, stat, stem, suffix, suffixes, symlink_to, touch, unlink, with_name, with_suffix, write_bytes, write_text\n",
    "```\n",
    "    \n",
    "</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_parts: 20\n",
      "m: 11,610\n",
      "np.arange(m): [    0     1     2 ... 11607 11608 11609]\n",
      "len chunks: 20\n",
      "len chunks[0]: 581\n",
      "len chunks[1]: 581\n",
      "----------------------------------------\n",
      "len 581: data_chp13/datasets/housing/my_train_00.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_01.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_02.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_03.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_04.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_05.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_06.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_07.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_08.csv\n",
      "len 581: data_chp13/datasets/housing/my_train_09.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_10.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_11.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_12.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_13.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_14.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_15.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_16.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_17.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_18.csv\n",
      "len 580: data_chp13/datasets/housing/my_train_19.csv\n",
      "n_parts: 10\n",
      "m: 3,870\n",
      "np.arange(m): [   0    1    2 ... 3867 3868 3869]\n",
      "len chunks: 10\n",
      "len chunks[0]: 387\n",
      "len chunks[1]: 387\n",
      "----------------------------------------\n",
      "len 387: data_chp13/datasets/housing/my_valid_00.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_01.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_02.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_03.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_04.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_05.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_06.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_07.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_08.csv\n",
      "len 387: data_chp13/datasets/housing/my_valid_09.csv\n",
      "n_parts: 10\n",
      "m: 5,160\n",
      "np.arange(m): [   0    1    2 ... 5157 5158 5159]\n",
      "len chunks: 10\n",
      "len chunks[0]: 516\n",
      "len chunks[1]: 516\n",
      "----------------------------------------\n",
      "len 516: data_chp13/datasets/housing/my_test_00.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_01.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_02.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_03.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_04.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_05.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_06.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_07.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_08.csv\n",
      "len 516: data_chp13/datasets/housing/my_test_09.csv\n"
     ]
    }
   ],
   "source": [
    "def save_to_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    print(f\"n_parts: {n_parts}\")\n",
    "    \n",
    "    # create pathlib.PosixPath\n",
    "    housing_dir = Path() / DATA_ROOT / \"datasets\" / \"housing\"\n",
    "    housing_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename_format = \"my_{}_{:02d}.csv\"\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    chunks = np.array_split(np.arange(m), n_parts)\n",
    "    print(f\"m: {m:,}\")\n",
    "    print(f\"np.arange(m): {np.arange(m)}\")\n",
    "    print(f\"len chunks: {len(chunks)}\")\n",
    "    print(f\"len chunks[0]: {len(chunks[0])}\")\n",
    "    print(f\"len chunks[1]: {len(chunks[1])}\")\n",
    "    HR()\n",
    "    \n",
    "    for file_idx, row_indices in enumerate(chunks):\n",
    "        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(str(part_csv))\n",
    "        print(f\"len {len(row_indices)}: {part_csv}\")\n",
    "        \n",
    "        with open(part_csv, \"w\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "\n",
    "# GB: Just a convenient way to create a dataset from two ndarrays??\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "# Create list of file paths for training, validation, testing datasets\n",
    "train_filepaths = save_to_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "----------------------------------------\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "----------------------------------------\n",
      "[[1 2 3 0 0 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# This should be an extended note!\n",
    "# numpy.c_\n",
    "# Indexing routine\n",
    "# Translates slice objects to concatenation along the second axis.\n",
    "# This is short-hand for np.r_['-1,2,0', index expression], which is useful because of its common occurrence. In particular, arrays will be stacked along their last axis after being upgraded to at least 2-D with 1’s post-pended to the shape (column vectors made out of 1-D arrays).\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.c_.html\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.r_.html\n",
    "\n",
    "test1 = np.c_[np.array([1,2,3])]\n",
    "print(test1)\n",
    "HR()\n",
    "\n",
    "test2 = np.c_[np.array([1,2,3]), np.array([4,5,6])]\n",
    "print(test2)\n",
    "HR()\n",
    "\n",
    "test3 = np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]\n",
    "print(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "Rq4x-1uAetWo",
    "outputId": "f8bf0c66-9ec6-4ef2-80d0-dbe17862f3d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MedInc</th>\n",
       "      <td>3.521400</td>\n",
       "      <td>5.327500</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HouseAge</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveRooms</th>\n",
       "      <td>3.049945</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>7.542373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveBedrms</th>\n",
       "      <td>1.106548</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>1.591525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>1447.000000</td>\n",
       "      <td>3464.000000</td>\n",
       "      <td>1328.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveOccup</th>\n",
       "      <td>1.605993</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>2.250847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <td>37.630000</td>\n",
       "      <td>33.690000</td>\n",
       "      <td>38.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <td>-122.430000</td>\n",
       "      <td>-117.390000</td>\n",
       "      <td>-122.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MedianHouseValue</th>\n",
       "      <td>1.442000</td>\n",
       "      <td>1.687000</td>\n",
       "      <td>1.621000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0            1            2\n",
       "MedInc               3.521400     5.327500     3.100000\n",
       "HouseAge            15.000000     5.000000    29.000000\n",
       "AveRooms             3.049945     6.490060     7.542373\n",
       "AveBedrms            1.106548     0.991054     1.591525\n",
       "Population        1447.000000  3464.000000  1328.000000\n",
       "AveOccup             1.605993     3.443340     2.250847\n",
       "Latitude            37.630000    33.690000    38.440000\n",
       "Longitude         -122.430000  -117.390000  -122.980000\n",
       "MedianHouseValue     1.442000     1.687000     1.621000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first few lines of these CSV files\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2Le5BZMetWo",
    "outputId": "a24ac40e-729b-456e-8b0b-2432cb4820e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "# In text mode\n",
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative\n",
    "print(\"\".join(open(train_filepaths[0]).readlines()[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwKcs2mWetWo",
    "outputId": "c8f223b4-4cb7-4814-9b47-ceb8ddb7d9e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_chp13/datasets/housing/my_train_00.csv',\n",
       " 'data_chp13/datasets/housing/my_train_01.csv',\n",
       " 'data_chp13/datasets/housing/my_train_02.csv',\n",
       " 'data_chp13/datasets/housing/my_train_03.csv',\n",
       " 'data_chp13/datasets/housing/my_train_04.csv',\n",
       " 'data_chp13/datasets/housing/my_train_05.csv',\n",
       " 'data_chp13/datasets/housing/my_train_06.csv',\n",
       " 'data_chp13/datasets/housing/my_train_07.csv',\n",
       " 'data_chp13/datasets/housing/my_train_08.csv',\n",
       " 'data_chp13/datasets/housing/my_train_09.csv',\n",
       " 'data_chp13/datasets/housing/my_train_10.csv',\n",
       " 'data_chp13/datasets/housing/my_train_11.csv',\n",
       " 'data_chp13/datasets/housing/my_train_12.csv',\n",
       " 'data_chp13/datasets/housing/my_train_13.csv',\n",
       " 'data_chp13/datasets/housing/my_train_14.csv',\n",
       " 'data_chp13/datasets/housing/my_train_15.csv',\n",
       " 'data_chp13/datasets/housing/my_train_16.csv',\n",
       " 'data_chp13/datasets/housing/my_train_17.csv',\n",
       " 'data_chp13/datasets/housing/my_train_18.csv',\n",
       " 'data_chp13/datasets/housing/my_train_19.csv']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9k7TVLDetWp"
   },
   "source": [
    "<a id='3.2'></a><a name='2.1'></a>\n",
    "## 3.2 Building an Input Pipeline\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "**API Note:**\n",
    "\n",
    "[tf.data.Dataset.list_files](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files)\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "list_files(\n",
    "    file_pattern, shuffle=None, seed=None, name=None\n",
    ")\n",
    "```\n",
    "\n",
    "A dataset of all files matching one or more glob patterns.\n",
    "\n",
    "The file_pattern argument should be a small number of glob patterns. If your filenames have already been globbed, use Dataset.from_tensor_slices(filenames) instead, as re-globbing every filename with list_files may result in poor performance with remote storage systems.\n",
    "Note: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVy6y1LdetWp",
    "outputId": "b0c43f87-78b2-4555-d318-6ded50bd0be3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "----------------------------------------\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'data_chp13/datasets/housing/my_train_13.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset containing these file paths\n",
    "# .list_files() returns a dataset that shuffles the file paths \n",
    "# Create a dataset of all files matching a pattern via tf.data.Dataset.list_file\n",
    "\n",
    "filepath_dataset = tf.data.Dataset.list_files(\n",
    "    train_filepaths, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(filepath_dataset)\n",
    "\n",
    "HR()\n",
    "\n",
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YppIX_bGetWq"
   },
   "source": [
    "<a id='3.3'></a><a name='3.3'></a>\n",
    "## 3.3 Interleaving lines from multiple files\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Use `.interleave()` to read from five files at a time, \n",
    "and interleave their lines (skipping first line of each file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSMSi0YOetWq",
    "outputId": "84727d49-bbfc-4d02-8549-425ebb2679e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ParallelInterleaveDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "----------------------------------------\n",
      "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n",
      "b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5\n",
    "\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers,\n",
    "    # Determine num of threads dynamically based on available CPU\n",
    "    num_parallel_calls=tf.data.AUTOTUNE \n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "HR()\n",
    "\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMaBPUwaetWq",
    "outputId": "dcb4952f-84d3-4f38-d12f-3abb0beca3f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that field 4 is interpreted as a string\n",
    "record_defaults = [\n",
    "    0, \n",
    "    np.nan, \n",
    "    tf.constant(np.nan, dtype=tf.float64),\n",
    "    \"Hello\",\n",
    "    tf.constant([])\n",
    "]\n",
    "\n",
    "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEXR27EVetWq",
    "outputId": "01396d80-1970-46d5-c8f5-941719a3f5b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All missing fields are replaced with their default value, when provided\n",
    "parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n",
    "parsed_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8L1DQK4PetWr",
    "outputId": "25b61b1f-99d2-4b33-a220-79ac1a82f272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# The 5th field is compulsory (since we provided tf.constant([]),\n",
    "# a the default_value, so we get an exception is we do not provide it.\n",
    "try:\n",
    "    parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5U7UCDjTetWs",
    "outputId": "73d6465b-6802-43aa-b71e-8c9dfe9a8ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# The number of fields should match exactly the number of fields in the record_defaults\n",
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXh1dWpHetWs"
   },
   "source": [
    "<a id='3.4'></a><a name='3.4'></a>\n",
    "## 3.4 Preprocessing the Data\n",
    "<a href=\"#top\">[back to top]</a>\n",
    "\n",
    "Compute the mean and standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
      "array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
      "       -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "X_mean, X_std = scaler.mean_, scaler.scale_\n",
    "n_inputs = 8 # X_train.shape[-1]\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y\n",
    "\n",
    "result = preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzinffGpetWs",
    "outputId": "9e02bfb6-c4fd-47ee-da52-32bd0e8ed6ab"
   },
   "outputs": [],
   "source": [
    "# n_inputs = 8 # X_train.shape[-1]\n",
    "\n",
    "# @tf.function\n",
    "# def preprocess(line):\n",
    "#     defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "#     fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "#     x = tf.stack(fields[:-1])\n",
    "#     y = tf.stack(fields[-1:])\n",
    "#     return (x - X_mean) / X_std, y\n",
    "\n",
    "# result = preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT0FtcVNetWt"
   },
   "source": [
    "<a id='3.5'></a><a name='3.5'></a>\n",
    "## 3.5 Putting everything together (w/o Prefetching)\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Efe4EbefetWt"
   },
   "outputs": [],
   "source": [
    "def csv_reader_dataset_no_prefetch(\n",
    "    filepaths,\n",
    "    n_readers=5,\n",
    "    n_read_threads=None,\n",
    "    n_parse_threads=5,\n",
    "    shuffle_buffer_size=10_000,\n",
    "    seed=42,\n",
    "    batch_size=32\n",
    "    ):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, \n",
    "        num_parallel_calls=n_read_threads\n",
    "    )\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "    \n",
    "    # dataset = (\n",
    "    #     dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    #     .shuffle(shuffle_buffer_size, seed=seed)\n",
    "    #     .batch(batch_size)\n",
    "    # )\n",
    "    # return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWotWeKuetWt",
    "outputId": "6c4c0f5e-d462-47a9-a4e6-71c0fcbf84d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[-1.2345318   0.1879177  -0.18384208  0.19340092 -0.4273575   0.49201018\n",
      "   1.0838584  -1.3871703 ]\n",
      " [-1.3836461  -0.7613805  -0.3076956  -0.07978077 -0.05045014  0.32237166\n",
      "   0.50294524 -0.1027696 ]\n",
      " [-0.41767654 -0.91959685 -0.5876468  -0.01253252  2.441884   -0.30059808\n",
      "  -0.68699217  0.521939  ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.804]\n",
      " [0.53 ]\n",
      " [1.745]], shape=(3, 1), dtype=float32)\n",
      "----------------------------------------\n",
      "X = tf.Tensor(\n",
      "[[-0.58831733  0.02970133 -0.70486885  0.16348003  0.8174406  -0.29916376\n",
      "  -0.70573175  0.6568782 ]\n",
      " [-1.3526396  -1.868895   -0.84703934 -0.0277291   0.58563805 -0.10333684\n",
      "  -1.3756571   1.2116159 ]\n",
      " [-0.16590534  1.8491895  -0.24013318 -0.0694841  -0.141711   -0.41202638\n",
      "   0.994848   -1.4321475 ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[2.045  ]\n",
      " [3.25   ]\n",
      " [5.00001]], shape=(3, 1), dtype=float32)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_set = csv_reader_dataset_no_prefetch(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in example_set.take(2):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    HR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTHFqHJ2etWu"
   },
   "source": [
    "<a id='3.6'></a><a name='3.6'></a>\n",
    "## 3.6 Putting everything together (with Prefetching)\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "fkRkQvx1etWu"
   },
   "outputs": [],
   "source": [
    "def csv_reader_dataset(\n",
    "    filepaths,\n",
    "    n_readers=5,\n",
    "    n_read_threads=None,\n",
    "    n_parse_threads=5,\n",
    "    shuffle_buffer_size=10_000,\n",
    "    seed=42,\n",
    "    batch_size=32\n",
    "    ):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, \n",
    "        num_parallel_calls=n_read_threads\n",
    "    )\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9kEckihetWu",
    "outputId": "2a5b045d-4cc1-441e-af64-adc8206e9c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[-1.2345318   0.1879177  -0.18384208  0.19340092 -0.4273575   0.49201018\n",
      "   1.0838584  -1.3871703 ]\n",
      " [-1.3836461  -0.7613805  -0.3076956  -0.07978077 -0.05045014  0.32237166\n",
      "   0.50294524 -0.1027696 ]\n",
      " [-0.41767654 -0.91959685 -0.5876468  -0.01253252  2.441884   -0.30059808\n",
      "  -0.68699217  0.521939  ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.804]\n",
      " [0.53 ]\n",
      " [1.745]], shape=(3, 1), dtype=float32)\n",
      "----------------------------------------\n",
      "X = tf.Tensor(\n",
      "[[-0.58831733  0.02970133 -0.70486885  0.16348003  0.8174406  -0.29916376\n",
      "  -0.70573175  0.6568782 ]\n",
      " [-1.3526396  -1.868895   -0.84703934 -0.0277291   0.58563805 -0.10333684\n",
      "  -1.3756571   1.2116159 ]\n",
      " [-0.16590534  1.8491895  -0.24013318 -0.0694841  -0.141711   -0.41202638\n",
      "   0.994848   -1.4321475 ]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[2.045  ]\n",
      " [3.25   ]\n",
      " [5.00001]], shape=(3, 1), dtype=float32)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show the first couple of batches produced by the dataset\n",
    "example_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in example_set.take(2):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    HR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtd9JaxOetWy",
    "outputId": "612e77ef-cc41-42cb-f861-bb5a392cc821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetV2 methods:\n",
      "----------------------------------------\n",
      "● apply()               \tApplies a transformation function to this dataset.\n",
      "● as_numpy_iterator()   \tReturns an iterator which converts all elements of the dataset to numpy.\n",
      "● batch()               \tCombines consecutive elements of this dataset into batches.\n",
      "● bucket_by_sequence_length()\tA transformation that buckets elements in a `Dataset` by length.\n",
      "● cache()               \tCaches the elements in this dataset.\n",
      "● cardinality()         \tReturns the cardinality of the dataset, if known.\n",
      "● choose_from_datasets()\tCreates a dataset that deterministically chooses elements from `datasets`.\n",
      "● concatenate()         \tCreates a `Dataset` by concatenating the given dataset with this dataset.\n",
      "● element_spec()        \tThe type specification of an element of this dataset.\n",
      "● enumerate()           \tEnumerates the elements of this dataset.\n",
      "● filter()              \tFilters this dataset according to `predicate`.\n",
      "● flat_map()            \tMaps `map_func` across this dataset and flattens the result.\n",
      "● from_generator()      \tCreates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
      "● from_tensor_slices()  \tCreates a `Dataset` whose elements are slices of the given tensors.\n",
      "● from_tensors()        \tCreates a `Dataset` with a single element, comprising the given tensors.\n",
      "● get_single_element()  \tReturns the single element of the `dataset`.\n",
      "● group_by_window()     \tGroups windows of elements by key and reduces them.\n",
      "● interleave()          \tMaps `map_func` across this dataset, and interleaves the results.\n",
      "● list_files()          \tA dataset of all files matching one or more glob patterns.\n",
      "● map()                 \tMaps `map_func` across the elements of this dataset.\n",
      "● options()             \tReturns the options for this dataset and its inputs.\n",
      "● padded_batch()        \tCombines consecutive elements of this dataset into padded batches.\n",
      "● prefetch()            \tCreates a `Dataset` that prefetches elements from this dataset.\n",
      "● random()              \tCreates a `Dataset` of pseudorandom values.\n",
      "● range()               \tCreates a `Dataset` of a step-separated range of values.\n",
      "● reduce()              \tReduces the input dataset to a single element.\n",
      "● rejection_resample()  \tA transformation that resamples a dataset to a target distribution.\n",
      "● repeat()              \tRepeats this dataset so each original value is seen `count` times.\n",
      "● sample_from_datasets()\tSamples elements at random from the datasets in `datasets`.\n",
      "● scan()                \tA transformation that scans a function across an input dataset.\n",
      "● shard()               \tCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      "● shuffle()             \tRandomly shuffles the elements of this dataset.\n",
      "● skip()                \tCreates a `Dataset` that skips `count` elements from this dataset.\n",
      "● snapshot()            \tAPI to persist the output of the input dataset.\n",
      "● take()                \tCreates a `Dataset` with at most `count` elements from this dataset.\n",
      "● take_while()          \tA transformation that stops dataset iteration based on a `predicate`.\n",
      "● unbatch()             \tSplits elements of a dataset into multiple elements.\n",
      "● unique()              \tA transformation that discards duplicate elements of a `Dataset`.\n",
      "● window()              \tReturns a dataset of \"windows\".\n",
      "● with_options()        \tReturns a new `tf.data.Dataset` with the given options set.\n",
      "● zip()                 \tCreates a `Dataset` by zipping together the given datasets.\n"
     ]
    }
   ],
   "source": [
    "# Short description of each method\n",
    "input = tf.data.Dataset\n",
    "print(str(f\"{input.__name__} methods:\"))\n",
    "HR()\n",
    "# m: function/attributes\n",
    "for m in dir(input):\n",
    "    if not (m.startswith(\"_\") or m.endswith(\"_\")):\n",
    "        func = getattr(tf.data.Dataset, m)\n",
    "        if hasattr(func, \"__doc__\"):\n",
    "            print(\"● {:22s}\\t{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1z7558jetWv"
   },
   "source": [
    "<a id='3.7'></a><a name='3.7'></a>\n",
    "## 3.7 Using the Dataset with tf.keras\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xDUEbOXetWv",
    "outputId": "4c943899-58e2-4977-d335-6e407764045d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_input_dataset': <BatchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>, '_buffer_size': <tf.Tensor: shape=(), dtype=int64, numpy=1>, '_metadata': , '_variant_tensor_attr': <tf.Tensor: shape=(), dtype=variant, value=<PrefetchDatasetOp::Dataset>>, '_graph_attr': <tensorflow.python.framework.ops.Graph object at 0x122b2a6a0>, '_options_attr': <tensorflow.python.data.ops.options.Options object at 0x1272bdf10>}\n",
      "----------------------------------------\n",
      "{'_input_dataset': <BatchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>, '_buffer_size': <tf.Tensor: shape=(), dtype=int64, numpy=1>, '_metadata': , '_variant_tensor_attr': <tf.Tensor: shape=(), dtype=variant, value=<PrefetchDatasetOp::Dataset>>, '_graph_attr': <tensorflow.python.framework.ops.Graph object at 0x122b2a6a0>, '_options_attr': <tensorflow.python.data.ops.options.Options object at 0x1272ef790>}\n",
      "----------------------------------------\n",
      "{'_input_dataset': <BatchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>, '_buffer_size': <tf.Tensor: shape=(), dtype=int64, numpy=1>, '_metadata': , '_variant_tensor_attr': <tf.Tensor: shape=(), dtype=variant, value=<PrefetchDatasetOp::Dataset>>, '_graph_attr': <tensorflow.python.framework.ops.Graph object at 0x122b2a6a0>, '_options_attr': <tensorflow.python.data.ops.options.Options object at 0x127344d30>}\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)\n",
    "\n",
    "print(train_set.__dict__)\n",
    "HR()\n",
    "print(valid_set.__dict__)\n",
    "HR()\n",
    "print(test_set.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8vlHQXUetWv",
    "outputId": "155fd08e-16ab-48bc-e69d-9da184ef1313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 0.8518 - val_loss: 54.4303\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5156 - val_loss: 14.4364\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8050 - val_loss: 6.3684\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4961 - val_loss: 8.7145\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4494 - val_loss: 0.7267\n"
     ]
    }
   ],
   "source": [
    "# Clear Keras global state for Functional model-building API\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "\n",
    "# batch_size = 32\n",
    "\n",
    "hist = model.fit(\n",
    "    train_set,\n",
    "    #steps_per_epoch=len(X_train) // batch_size,\n",
    "    validation_data=valid_set,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 947us/step - loss: 0.4192\n",
      "Evaluate result: {test_mse}\n"
     ]
    }
   ],
   "source": [
    "test_mse = model.evaluate(test_set)\n",
    "new_set = test_set.take(3) # pretend we have 3 new samples\n",
    "y_pred = model.predict(new_set) # can just pass a Numpy array\n",
    "\n",
    "print(\"Evaluate result: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.8'></a><a name='3.8'></a>\n",
    "## 3.8 Custom training loop\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and loss function for training\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        # Do one Gradient step\n",
    "        print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5aRhKLHetWx"
   },
   "source": [
    "<a id='3.9'></a><a name='3.9'></a>\n",
    "## 3.9 Creating a TF Function to perform training loop\n",
    "<a href=\"#top\">[back to top]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VL7h9l60etWx",
    "outputId": "bedf718e-bf6a-4f16-9cae-0468f793f2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_epochs: 5\n",
      "----------------------------------------\n",
      "Epoch 5/5\n",
      "Done training..\n"
     ]
    }
   ],
   "source": [
    "# Clear Keras global state for Functional model-building API\n",
    "keras.backend.clear_session()\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "\n",
    "# Creating a TF Function the performs the whole training loop\n",
    "@tf.function\n",
    "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "print(f\"n_epochs: {n_epochs}\")\n",
    "HR()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # '\\r' is the carriage return\n",
    "    # Use this to overwrite text, and stay on the same line.\n",
    "    print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
    "    train_one_epoch(model, optimizer, loss_fn, train_set)\n",
    "\n",
    "print()\n",
    "print(\"Done training..\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "13_loading_and_preprocessing_data.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
