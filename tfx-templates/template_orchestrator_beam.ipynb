{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqZC7Cv942w0"
      },
      "source": [
        "<a id='top'></a>\n",
        "<a name=\"top\"></a><!--Need for Colab-->\n",
        "# Create a TFX pipeline using tfx template\n",
        "\n",
        "## Beam Orchestrator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4AligwyFGY"
      },
      "source": [
        "1. Introduction\n",
        "2. Setup\n",
        "    * 2.1 Setup project structure\n",
        "3. Check previous pipelines\n",
        "4. Delete pipeline in the given orchestrator\n",
        "5. Check available model templates\n",
        "6. Copy predefined template to project directory\n",
        "7. Browse the copied source files.\n",
        "8. Workaround for missing beam_runner.py file\n",
        "9. Configuration of project\n",
        "    * 9.1 project_root/pipeline/pipeline.py/\n",
        "    * 9.2 project_root/pipeline/configs.py\n",
        "    * 9.3 Configure project_root/{orchestrator}__runner.py\n",
        "    * 9.4 Examine project_root/{ORCHESTRATOR_FILE}\n",
        "10. Create a new pipeline in the given orchestrator.\n",
        "11. Create a new run for a pipeline (tfx run create)\n",
        "12. Add components and update the pipeline, creating new artifacts.\n",
        "    * 12.1 Add components for data validation\n",
        "    * 12.2 Run new pipeline instance (tfx run create)\n",
        "    * 12.3 Add components for training\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRBoc5la42w0",
        "tags": []
      },
      "source": [
        "---\n",
        "<a id=\"1.0\"></a><a name=\"1.0\"></a>\n",
        "# 1. Introduction\n",
        "<a href=\"#top\">[back to top]</a>\n",
        "\n",
        "1. This is a heavily-annotated version of the original Google tutorial, \"Create a TFX pipeline using templates with Beam orchestrator\". This project builds a pipeline using the Taxi Trips dataset released by the City of Chicago.\n",
        "\n",
        "1. The main command-group options to remember are:\n",
        "* `tfx pipeline` - Create and manage TFX pipelines.\n",
        "* `tfx run` - Create and manage runs of TFX pipelines on various orchestration platforms.\n",
        "* `tfx template` - Experimental commands for listing and copying TFX pipeline templates.\n",
        "\n",
        "\n",
        "3. We define a runner to actually run the pipeline. This serves as the entrypoint to this project.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "* TFX CLI uses the KFP (Kubeflow Pipelines) SDK underneath. \n",
        "\n",
        "**Resources**\n",
        "\n",
        "* [Create a TFX pipeline using templates with Beam orchestrator](https://www.tensorflow.org/tfx/tutorials/tfx/template_beam)\n",
        "* [Using the TFX Command-line Interface](https://github.com/tensorflow/tfx/blob/master/docs/guide/cli.md)\n",
        "* [Create a TFX pipeline using templates with Local orchestrator](https://www.tensorflow.org/tfx/tutorials/tfx/template_local)\n",
        "* [Create a TFX pipeline using templates](https://www.tensorflow.org/tfx/tutorials/tfx/template)\n",
        "* [Taxi Trips dataset](\n",
        "https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)\n",
        "* [KFP(Kubeflow Pipelines) SDK](https://github.com/tensorflow/tfx/issues/5020)\n",
        "\n",
        "**Orchestrators**\n",
        "\n",
        "* [Beam](https://www.tensorflow.org/tfx/tutorials/tfx/template_beam)\n",
        "* [Kubeflow on Google Cloud](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines)\n",
        "* [Vertex AI example](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training)\n",
        "* [Google Cloud AI example](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfdpUCNWyFGc"
      },
      "source": [
        "---\n",
        "# 2. Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Need if running on Colab or Kaggle\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    #!{sys.executable} -m pip install --upgrade \"tfx<2\" &> /dev/null\n",
        "    !pip install --upgrade tfx &> /dev/null\n",
        "    !apt-get install tree \n",
        "    print()\n",
        "    print(\"Need to restart runtime on Colab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tywA0VHo0PY2",
        "outputId": "dfc694be-8981-43e5-cb47-be3e312ea4bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (297 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\n",
            "Need to restart runtime on Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IYGyT4ib42xG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a36720-e3f5-47c9-a760-8851bc349791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished with imports.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import pprint\n",
        "import subprocess \n",
        "import sys\n",
        "import time\n",
        "from time import process_time\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "DEBUG = True\n",
        "\n",
        "# Need to reinit after runtime restart\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "print(\"Finished with imports.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWcu4ytyFGf"
      },
      "source": [
        "<a name=\"2.1\"></a>\n",
        "## 2.1 Setup project structure\n",
        "<a href=\"#top\">[back to top]</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organize project as {engine}{template}{version}"
      ],
      "metadata": {
        "id": "UGcV9Uzby625"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euO_xiySyFGj",
        "outputId": "a67b7851-8982-4451-d6b1-5b1d221e8a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current dir:\t/content\n",
            "PIPELINE_NAME:\tpipeline_beam_penguin_01\n",
            "PROJECT_DIR:\t/content/pipeline_beam_penguin_01\n",
            "PIPELINE_FILE:\t/content/pipeline_beam_penguin_01/pipeline/pipeline.py\n",
            "ORCHESTRATOR_FILE:\t/content/pipeline_beam_penguin_01/beam_runner.py\n",
            "OUTPUT_ARTIFACTS_DIR:\ttfx_artifacts_beam_penguin_01\n"
          ]
        }
      ],
      "source": [
        "# Run these examples:\n",
        "# beam, taxi, 01  (DONE)\n",
        "# beam, penguin, 01 (DONE)\n",
        "# local, taxi, 01 (DONE)\n",
        "# local, penguin, 01\n",
        "\n",
        "######\n",
        "\n",
        "# 1. Define engine [beam, local]\n",
        "ENGINE = 'beam'\n",
        "\n",
        "# 2. Define template [taxi, penguin]\n",
        "TEMPLATE = \"penguin\"\n",
        "\n",
        "# If needed, added version, such as _01\n",
        "VERSION = '_01'\n",
        "\n",
        "#####\n",
        "\n",
        "# 3. Define up project structure\n",
        "PIPELINE_NAME=f\"pipeline_{ENGINE}_{TEMPLATE}{VERSION}\"\n",
        "\n",
        "# Create a project directory with absolute path\n",
        "PROJECT_DIR = Path(PIPELINE_NAME).resolve()\n",
        "\n",
        "# 4. Create shortcut to __runner.py [beam_runner.py, local_runner.py]\n",
        "if ENGINE == 'beam':\n",
        "    ORCHESTRATOR_FILE = f'{PROJECT_DIR}/beam_runner.py'\n",
        "elif ENGINE == 'local':\n",
        "    ORCHESTRATOR_FILE = f'{PROJECT_DIR}/local_runner.py'\n",
        "\n",
        "# Create shortcut to pipeline.py\n",
        "PIPELINE_FILE = f\"{PROJECT_DIR}/pipeline/pipeline.py\"\n",
        "\n",
        "# Create output dir for artifacts\n",
        "OUTPUT_ARTIFACTS_DIR = f\"tfx_artifacts_{ENGINE}_{TEMPLATE}{VERSION}\"\n",
        "\n",
        "def HR():\n",
        "    print(\"-\"*40)\n",
        "    \n",
        "current_dir = !pwd\n",
        "print(f\"Current dir:\\t{current_dir[0]}\")\n",
        "print(f\"PIPELINE_NAME:\\t{PIPELINE_NAME}\")\n",
        "print(f\"PROJECT_DIR:\\t{PROJECT_DIR}\")\n",
        "print(f\"PIPELINE_FILE:\\t{PIPELINE_FILE}\")\n",
        "print(f\"ORCHESTRATOR_FILE:\\t{ORCHESTRATOR_FILE}\")\n",
        "print(f\"OUTPUT_ARTIFACTS_DIR:\\t{OUTPUT_ARTIFACTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G2GYFvEByFGl"
      },
      "outputs": [],
      "source": [
        "# If need to clean up\n",
        "# !rm -fr {PROJECT_DIR}\n",
        "# !rm -fr {OUTPUT_ARTIFACTS_DIR}\n",
        "\n",
        "# print(\"Done cleaning up project_dir\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHPkVOUeyFGm"
      },
      "source": [
        "---\n",
        "## 3. Check previous pipelines\n",
        "\n",
        "To avoid naming conflicts, first check any previous pipeline projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m07D4JMeyFGn",
        "outputId": "ad56951f-a316-4a71-c159-3f7af31179c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing pipelines in the beam orchestrator:\n",
            "----------------------------------------\n",
            "CLI\n",
            "Listing all pipelines\n",
            "No pipelines to display.\n"
          ]
        }
      ],
      "source": [
        "# Lists all the pipelines in the given orchestrator.\n",
        "# https://github.com/tensorflow/tfx/blob/master/tfx/tools/cli/commands/pipeline.py#L278\n",
        "print(f\"Listing pipelines in the {ENGINE} orchestrator:\")\n",
        "HR()\n",
        "!tfx pipeline list --engine={ENGINE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8KynTk6yFGn"
      },
      "source": [
        "---\n",
        "## 4. Delete pipeline in the given orchestrator\n",
        "\n",
        "Even if you delete the project folder, the pipeline is still registered with the local orchestrator, so you have to also delete it there (if you want to rebuild it).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bwvOB_k5yFGo"
      },
      "outputs": [],
      "source": [
        "# https://github.com/tensorflow/tfx/blob/master/tfx/tools/cli/commands/pipeline.py#L245\n",
        "# print(f\"Deleting pipeline {PIPELINE_NAME} in the {ENGINE} orchestrator:\")\n",
        "# HR()\n",
        "# !tfx pipeline delete --pipeline-name={PIPELINE_NAME} --engine={ENGINE}\n",
        "# For example,\n",
        "# !tfx pipeline delete --pipeline-name=pipeline_beam_taxi_01 --engine=beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyFDS3-hyFGo"
      },
      "source": [
        "## 5. Check available model templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfNwKmunyFGp",
        "outputId": "19299ccd-21dc-42e9-d001-69e6cb305777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLI\n",
            "Available templates:\n",
            "- taxi\n",
            "- penguin\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/tensorflow/tfx/blob/master/tfx/tools/cli/commands/template.py#L32\n",
        "if DEBUG:\n",
        "    !tfx template list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7D37UoayFGp"
      },
      "source": [
        "## 6. Copy predefined template to project directory\n",
        "\n",
        "\n",
        "Copy a template to the destination directory. At this step, we still do not specify which orchestrator to use.\n",
        "\n",
        "Usage:\n",
        "    \n",
        "```bash\n",
        "tfx template copy\n",
        "    --model=model\n",
        "    --pipeline_name=pipeline-name\n",
        "    --destination_path=destination-path\n",
        "```\n",
        "\n",
        "[Reference](https://github.com/tensorflow/tfx/blob/master/docs/guide/cli.md#copy)\n",
        "\n",
        "---\n",
        "\n",
        "This step should create these files:\n",
        "\n",
        "```\n",
        "├── data\n",
        "│   └── data.csv\n",
        "├── kubeflow_runner.py\n",
        "├── local_runner.py\n",
        "├── models\n",
        "│   ├── constants.py\n",
        "│   ├── features.py\n",
        "│   ├── features_test.py\n",
        "│   ├── model.py\n",
        "│   ├── model_test.py\n",
        "│   ├── preprocessing.py\n",
        "│   └── preprocessing_test.py\n",
        "└── pipeline\n",
        "    ├── configs.py\n",
        "    └── pipeline.py\n",
        "\n",
        "3 directories, 12 files\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwhLam3gyFGq",
        "outputId": "2e8da573-e7c8-4d38-cb14-ee0eca83eafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying predefined 'penguin' template to project directory:\n",
            "Done.\n",
            "----------------------------------------\n",
            "/content/pipeline_beam_penguin_01\n",
            "├── data\n",
            "│   └── data.csv\n",
            "├── kubeflow_runner.py\n",
            "├── local_runner.py\n",
            "├── models\n",
            "│   ├── constants.py\n",
            "│   ├── features.py\n",
            "│   ├── features_test.py\n",
            "│   ├── model.py\n",
            "│   ├── model_test.py\n",
            "│   ├── preprocessing.py\n",
            "│   └── preprocessing_test.py\n",
            "└── pipeline\n",
            "    ├── configs.py\n",
            "    └── pipeline.py\n",
            "\n",
            "3 directories, 12 files\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Copying predefined '{TEMPLATE}' template to project directory:\")\n",
        "\n",
        "# https://github.com/tensorflow/tfx/blob/master/tfx/tools/cli/commands/template.py#L59\n",
        "!tfx template copy \\\n",
        "    --model={TEMPLATE} \\\n",
        "    --pipeline_name={PIPELINE_NAME} \\\n",
        "    --destination_path={PROJECT_DIR} &> /dev/null\n",
        "\n",
        "print(\"Done.\")\n",
        "\n",
        "HR()\n",
        "\n",
        "proc = subprocess.Popen(\n",
        "    [\"tree\", PROJECT_DIR, \"-I\", \"__pycache__|__init__.py\"], \n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "tree_01_template_copy = proc.communicate()[0]\n",
        "print(tree_01_template_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svmka0IWyFGr"
      },
      "source": [
        "---\n",
        "Note that we still have not yet registered this pipeline with the local orchestrator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlAqmidlyFGs",
        "outputId": "f4fc2ea7-1873-4ff6-874f-62bd23e58449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLI\n",
            "Listing all pipelines\n",
            "No pipelines to display.\n"
          ]
        }
      ],
      "source": [
        "#  Check if pipeline has been registered in orchestrator or not\n",
        "if DEBUG:\n",
        "    !tfx pipeline list --engine={ENGINE}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdiHik_w42xN"
      },
      "source": [
        "## 7. Browse the copied source files.\n",
        "\n",
        "-   `pipeline` - This directory contains the definition of the pipeline\n",
        "    -   `configs.py` — defines common constants for pipeline runners\n",
        "    -   `pipeline.py` — defines TFX components and a pipeline\n",
        "-   `models` - This directory contains ML model definitions.\n",
        "    -   `features.py`, `features_test.py` — defines features for the model\n",
        "    -   `preprocessing.py`, `preprocessing_test.py` — defines preprocessing\n",
        "        jobs using `tf.Transform`\n",
        "    -   `estimator` - This directory contains an Estimator based model.\n",
        "        -   `constants.py` — defines constants of the model\n",
        "        -   `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
        "    -   `keras` - This directory contains a Keras based model.\n",
        "        -   `constants.py` — defines constants of the model\n",
        "        -   `model.py`, `model_test.py` — defines DNN model using Keras\n",
        "-   `local_runner.py`, `kubeflow_runner.py` — define runners for each orchestration engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "QZFuBEPfyFGs"
      },
      "source": [
        "---\n",
        "## 8. Workaround for missing beam_runner.py file\n",
        "\n",
        "The file beam_runner.py is not generated by the template, so we implement it here.\n",
        "\n",
        "Reference:\n",
        "\n",
        "* https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_native_keras.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gYs45ldXyFGt"
      },
      "outputs": [],
      "source": [
        "# Transform and Model\n",
        "_beam_runner_file = f\"{PROJECT_DIR}/beam_runner.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrwdgFiKyFGt",
        "outputId": "d9c61f86-8407-4916-b41b-2599758042ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/pipeline_beam_penguin_01/beam_runner.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {_beam_runner_file}\n",
        "# This file is written from jupyter notebook\n",
        "# Copyright 2022 George Baptista\n",
        "# Copyright 2020 Google LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Define BeamDagRunner to run the pipeline.\"\"\"\n",
        "\n",
        "import os\n",
        "from absl import logging\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from pipeline import configs\n",
        "from pipeline import pipeline\n",
        "\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "# TFX pipeline produces many output files and metadata. All output data will be\n",
        "# stored under this OUTPUT_DIR.\n",
        "# NOTE: It is recommended to have a separated OUTPUT_DIR which is *outside* of\n",
        "#       the source code structure. Please change OUTPUT_DIR to other location\n",
        "#       where we can store outputs of the pipeline.\n",
        "OUTPUT_DIR = '.'\n",
        "\n",
        "# TFX produces two types of outputs, files and metadata.\n",
        "# - Files will be created under PIPELINE_ROOT directory.\n",
        "# - Metadata will be written to SQLite database in METADATA_PATH.\n",
        "PIPELINE_ROOT = os.path.join(OUTPUT_DIR, 'tfx_pipeline_output',\n",
        "                             configs.PIPELINE_NAME)\n",
        "METADATA_PATH = os.path.join(OUTPUT_DIR, 'tfx_metadata', configs.PIPELINE_NAME,\n",
        "                             'metadata.db')\n",
        "\n",
        "# The last component of the pipeline, \"Pusher\" will produce serving model under\n",
        "# SERVING_MODEL_DIR.\n",
        "SERVING_MODEL_DIR = os.path.join(PIPELINE_ROOT, 'serving_model')\n",
        "\n",
        "# Specifies data file directory. DATA_PATH should be a directory containing CSV\n",
        "# files for CsvExampleGen in this example. By default, data files are in the\n",
        "# `data` directory.\n",
        "DATA_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')\n",
        "\n",
        "\n",
        "def run():\n",
        "    \"\"\"Define a pipeline.\"\"\"\n",
        "\n",
        "    #tfx.orchestration.BeamDagRunner().run(\n",
        "    BeamDagRunner().run(\n",
        "        pipeline.create_pipeline(\n",
        "            pipeline_name=configs.PIPELINE_NAME,\n",
        "            pipeline_root=PIPELINE_ROOT,\n",
        "            data_path=DATA_PATH,\n",
        "            preprocessing_fn=configs.PREPROCESSING_FN,\n",
        "            run_fn=configs.RUN_FN,\n",
        "            train_args=tfx.proto.TrainArgs(num_steps=configs.TRAIN_NUM_STEPS),\n",
        "            eval_args=tfx.proto.EvalArgs(num_steps=configs.EVAL_NUM_STEPS),\n",
        "            eval_accuracy_threshold=configs.EVAL_ACCURACY_THRESHOLD,\n",
        "            serving_model_dir=SERVING_MODEL_DIR,\n",
        "            metadata_connection_config=tfx.orchestration.metadata\n",
        "            .sqlite_metadata_connection_config(METADATA_PATH)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    logging.set_verbosity(logging.INFO)\n",
        "    run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuM6B8N5yFGu"
      },
      "source": [
        "---\n",
        "## 9. Configuration of project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np4INOyIyFGu"
      },
      "source": [
        "### 9.1 project_root/pipeline/pipeline.py/\n",
        "\n",
        "**Cache setting**\n",
        "\n",
        "Every time we create a pipeline run, every component runs again and again even though the input and the parameters may not change.\n",
        "It is waste of time and resources, and you can skip those executions with pipeline caching. You can enable caching by specifying `enable_cache=True` for the `Pipeline` object in `pipeline.py`.\n",
        "\n",
        "Note:\n",
        "\n",
        "Before, relied on this setting with -e, which specifies what follows is the script that you want to execute with sed.\n",
        "\n",
        "```bash\n",
        "sed -i -e 's/\\# enable_cache=True/enable_cache=True/'\n",
        "```\n",
        "\n",
        "However, this results in the -e being consumed by the following cell `sed -i` option, resulting in the suffix py-e for the backup file. To fix this, we can stop using the -e option here, and instead use this option:\n",
        "\n",
        "```bash\n",
        "sed -i '' 's/\\# enable_cache=True/enable_cache=True/' {PIPELINE_FILE}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sys import platform\n",
        "\n",
        "if platform == \"darwin\":\n",
        "    print(\"OSX\")\n",
        "    OSX_syntax = \"''\"\n",
        "else:\n",
        "    OSX_syntax = ''\n",
        "\n",
        "print(OSX_syntax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBrVxcpd3Cnb",
        "outputId": "82a4c853-f069-43c0-fe7e-94ae45e9a3f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3tHdkGEyFGu",
        "outputId": "0e4d0a1a-2e6f-478c-a8cd-bd9204cea6c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pipeline.py\n",
            "153: enable_cache=True,\n"
          ]
        }
      ],
      "source": [
        "!sed -i {OSX_syntax} 's/\\# enable_cache=True/enable_cache=True/' {PIPELINE_FILE}\n",
        "\n",
        "print(Path(PIPELINE_FILE).name)\n",
        "# Check changes\n",
        "!grep -n 'enable_cache=True' {PIPELINE_FILE} | tr -s \" \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "CGGD3DcAyFGw"
      },
      "source": [
        "### 9.2 project_root/pipeline/configs.py\n",
        "\n",
        "1. Comment out the google.auth code in pipeline/configs.py (unless you are actually using it).\n",
        "\n",
        "2. Add a placeholder value for `GOOGLE_CLOUD_PROJECT`\n",
        "After commenting out that block, and add a placeholder value in its place:\n",
        "\n",
        "We can do this programatically via the sed tool.\n",
        "\n",
        "Resources:\n",
        "* https://github.com/sharkdp/bat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd1-4uvkyFGw",
        "outputId": "bbe0ec76-f093-4b24-bbf9-14b55fdc0123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Copyright 2020 Google LLC. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\"\"\"TFX penguin template configurations.\n",
            "\n",
            "This file defines environments for a TFX penguin pipeline.\n",
            "\"\"\"\n",
            "\n",
            "import os  # pylint: disable=unused-import\n",
            "\n",
            "# TODO(b/149347293): Move more TFX CLI flags into python configuration.\n",
            "\n",
            "# Pipeline name will be used to identify this pipeline.\n",
            "PIPELINE_NAME = 'pipeline_beam_penguin_01'\n",
            "\n",
            "# GCP related configs.\n",
            "\n",
            "# Following code will retrieve your GCP project. You can choose which project\n",
            "# to use by setting GOOGLE_CLOUD_PROJECT environment variable.\n",
            "##### try:\n",
            "#####   import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error\n",
            "#####   try:\n",
            "#####     _, GOOGLE_CLOUD_PROJECT = google.auth.default()\n",
            "#####   except google.auth.exceptions.DefaultCredentialsError:\n",
            "#####     GOOGLE_CLOUD_PROJECT = ''\n",
            "##### except ImportError:\n",
            "#####   GOOGLE_CLOUD_PROJECT = ''\n",
            "GOOGLE_CLOUD_PROJECT='placeholder'\n",
            "# Specify your GCS bucket name here. You have to use GCS to store output files\n",
            "# when running a pipeline with Kubeflow Pipeline on GCP or when running a job\n",
            "# using Dataflow. Default is '<gcp_project_name>-kubeflowpipelines-default'.\n",
            "# This bucket is created automatically when you deploy KFP from marketplace.\n",
            "GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + '-kubeflowpipelines-default'\n",
            "\n",
            "# Following image will be used to run pipeline components run if Kubeflow\n",
            "# Pipelines used.\n",
            "# This image will be automatically built by CLI if we use --build-image flag.\n",
            "PIPELINE_IMAGE = f'gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}'\n",
            "\n",
            "PREPROCESSING_FN = 'models.preprocessing.preprocessing_fn'\n",
            "RUN_FN = 'models.model.run_fn'\n",
            "\n",
            "TRAIN_NUM_STEPS = 100\n",
            "EVAL_NUM_STEPS = 15\n",
            "\n",
            "# Change this value according to your use cases.\n",
            "EVAL_ACCURACY_THRESHOLD = 0.6\n",
            "\n",
            "# Google Cloud BigQuery related configs.\n",
            "# Use following configs to use BigQueryExampleGen as a data source.\n",
            "#\n",
            "# Beam args to use BigQueryExampleGen with Beam DirectRunner.\n",
            "#\n",
            "# BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n",
            "#    '--project=' + GOOGLE_CLOUD_PROJECT,\n",
            "#    '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n",
            "#    ]\n",
            "\n",
            "# The query that extracts the examples from BigQuery.\n",
            "#\n",
            "# BIG_QUERY_QUERY = \"\"\"\n",
            "#         SELECT ...\n",
            "#         FROM\n",
            "#         WHERE\n",
            "# \"\"\"\n"
          ]
        }
      ],
      "source": [
        "# The relevant code is on lines 30-37\n",
        "# \n",
        "# try:\n",
        "#   import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error\n",
        "#   try:\n",
        "#     _, GOOGLE_CLOUD_PROJECT = google.auth.default()\n",
        "#   except google.auth.exceptions.DefaultCredentialsError:\n",
        "#     GOOGLE_CLOUD_PROJECT = ''\n",
        "# except ImportError:\n",
        "#   GOOGLE_CLOUD_PROJECT = ''\n",
        "\n",
        "# Possible error for duplicate .py-e files being created:\n",
        "# Also it is looking like the -e is being gobbled up by the -i option as the suffix for the backup file will be made as filename-e as is shown in your snippet\n",
        "# https://serverfault.com/questions/939762/update-all-python-files-via-linux-command\n",
        "\n",
        "config_file = f'{PROJECT_DIR}/pipeline/configs.py'\n",
        "\n",
        "# Use -i option to edit the original file in-place\n",
        "already_commented = !grep \"#####\" {config_file}\n",
        "if not already_commented:\n",
        "    # Since we know the exact location, simply use line numbers to comment out block\n",
        "    !sed -i {OSX_syntax} '30,37 s/^/##### /' {config_file}\n",
        "\n",
        "# Need dummy value for GOOGLE_CLOUD_PROJECT\n",
        "# sed on OS X requires the extension to be explicitly specified. \n",
        "# The workaround is to set an empty string.\n",
        "!sed -i {OSX_syntax} \"38 s/.*/GOOGLE_CLOUD_PROJECT='placeholder'/\" {config_file}\n",
        "\n",
        "#print(\"Affected lines in source file:\\n\")\n",
        "# print selected lines, with line numbering\n",
        "#!sed '30,40!d;=' {config_file} | sed 'N;s/\\n/ /'\n",
        "\n",
        "if IS_COLAB:\n",
        "    !cat {config_file}\n",
        "else:\n",
        "    !bat --theme=GitHub --color=always --wrap never {config_file}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sGhNIoTyFGx"
      },
      "source": [
        "### 9.3 Configure project_root/{orchestrator}__runner.py\n",
        "\n",
        "1. Configure OUTPUT_DIR to `./project_root/tfx_artifacts`. This will hold both `tfx_metadata` and `tfx_pipeline_output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lvZ73GZyFGx",
        "outputId": "0783e6f3-1524-422f-9328-98f7d8f27a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beam_runner.py\n",
            "33:OUTPUT_DIR = 'tfx_artifacts_beam_penguin_01'\n"
          ]
        }
      ],
      "source": [
        "# It's more clear to use interpolation when passing arguments containing lots of '' syntax\n",
        "source = \"OUTPUT_DIR = '.'\"\n",
        "target = f\"OUTPUT_DIR = '{OUTPUT_ARTIFACTS_DIR}'\"\n",
        "\n",
        "!sed -i {OSX_syntax} \"s/$source/$target/\" {ORCHESTRATOR_FILE}\n",
        "\n",
        "# Show line number, use tr -s option to replace instances of repeated chars with a single char. \n",
        "# https://pubs.opengroup.org/onlinepubs/9699919799/utilities/tr.html#tag_20_132_04\n",
        "\n",
        "print(Path(ORCHESTRATOR_FILE).name)\n",
        "!grep -n \"$target\" {ORCHESTRATOR_FILE} | tr -s \" \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws8v-0fkyFGx"
      },
      "source": [
        "---\n",
        "### 9.4 Examine project_root/{ORCHESTRATOR_FILE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXXai1zfyFGy",
        "outputId": "989341cf-9240-4dd3-ee7f-3c942717a70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Copyright 2020 Google LLC. All Rights Reserved.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\"\"\"TFX penguin template configurations.\n",
            "\n",
            "This file defines environments for a TFX penguin pipeline.\n",
            "\"\"\"\n",
            "\n",
            "import os  # pylint: disable=unused-import\n",
            "\n",
            "# TODO(b/149347293): Move more TFX CLI flags into python configuration.\n",
            "\n",
            "# Pipeline name will be used to identify this pipeline.\n",
            "PIPELINE_NAME = 'pipeline_beam_penguin_01'\n",
            "\n",
            "# GCP related configs.\n",
            "\n",
            "# Following code will retrieve your GCP project. You can choose which project\n",
            "# to use by setting GOOGLE_CLOUD_PROJECT environment variable.\n",
            "##### try:\n",
            "#####   import google.auth  # pylint: disable=g-import-not-at-top  # pytype: disable=import-error\n",
            "#####   try:\n",
            "#####     _, GOOGLE_CLOUD_PROJECT = google.auth.default()\n",
            "#####   except google.auth.exceptions.DefaultCredentialsError:\n",
            "#####     GOOGLE_CLOUD_PROJECT = ''\n",
            "##### except ImportError:\n",
            "#####   GOOGLE_CLOUD_PROJECT = ''\n",
            "GOOGLE_CLOUD_PROJECT='placeholder'\n",
            "# Specify your GCS bucket name here. You have to use GCS to store output files\n",
            "# when running a pipeline with Kubeflow Pipeline on GCP or when running a job\n",
            "# using Dataflow. Default is '<gcp_project_name>-kubeflowpipelines-default'.\n",
            "# This bucket is created automatically when you deploy KFP from marketplace.\n",
            "GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + '-kubeflowpipelines-default'\n",
            "\n",
            "# Following image will be used to run pipeline components run if Kubeflow\n",
            "# Pipelines used.\n",
            "# This image will be automatically built by CLI if we use --build-image flag.\n",
            "PIPELINE_IMAGE = f'gcr.io/{GOOGLE_CLOUD_PROJECT}/{PIPELINE_NAME}'\n",
            "\n",
            "PREPROCESSING_FN = 'models.preprocessing.preprocessing_fn'\n",
            "RUN_FN = 'models.model.run_fn'\n",
            "\n",
            "TRAIN_NUM_STEPS = 100\n",
            "EVAL_NUM_STEPS = 15\n",
            "\n",
            "# Change this value according to your use cases.\n",
            "EVAL_ACCURACY_THRESHOLD = 0.6\n",
            "\n",
            "# Google Cloud BigQuery related configs.\n",
            "# Use following configs to use BigQueryExampleGen as a data source.\n",
            "#\n",
            "# Beam args to use BigQueryExampleGen with Beam DirectRunner.\n",
            "#\n",
            "# BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n",
            "#    '--project=' + GOOGLE_CLOUD_PROJECT,\n",
            "#    '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n",
            "#    ]\n",
            "\n",
            "# The query that extracts the examples from BigQuery.\n",
            "#\n",
            "# BIG_QUERY_QUERY = \"\"\"\n",
            "#         SELECT ...\n",
            "#         FROM\n",
            "#         WHERE\n",
            "# \"\"\"\n"
          ]
        }
      ],
      "source": [
        "if IS_COLAB:\n",
        "    !cat {config_file}\n",
        "else:\n",
        "    !bat --theme=GitHub --color=always --wrap never {config_file}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6kcVnwayFGy"
      },
      "source": [
        "---\n",
        "## 10. Create a new pipeline in the given orchestrator.\n",
        "\n",
        "`tfx pipeline`\n",
        "\n",
        "Usage:\n",
        "\n",
        "```shell\n",
        "tfx pipeline create --pipeline_path=pipeline-path\n",
        "    [\n",
        "    --endpoint=endpoint\n",
        "    --engine=engine\n",
        "    --iap_client_id=iap-client-id\n",
        "    --namespace=namespace\n",
        "    --build_image\n",
        "    --build_base_image=build-base-image\n",
        "    ]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Next, create a new pipeline now with `pipeline create`.\n",
        "\n",
        "\n",
        "This registers your pipeline as defined in local_runner.py without actually running it, and creates the empty folder `tfx_metadata`. This creates a new run instance for a pipeline in the orchestrator. \n",
        "This also creates the actual artifacts.\n",
        "\n",
        "Components in the TFX pipeline generate outputs for each run as ML Metadata Artifacts, and they need to be stored somewhere. Here, they are stored in `tfx_metadata`.\n",
        "\n",
        "This step should create these files:\n",
        "\n",
        "```\n",
        "tfx_artifacts\n",
        "└── tfx_metadata\n",
        "    └── pipeline_penguin_01\n",
        "```\n",
        "\n",
        "Resources:\n",
        "\n",
        "* https://github.com/tensorflow/tfx/blob/master/tfx/tools/cli/commands/run.py#L79  \n",
        "* https://github.com/tensorflow/tfx/blob/master/tfx/tools/cli/commands/pipeline.py#L123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVn_6AxMyFG0",
        "outputId": "d203ba20-3d93-4e26-bc29-cb79f82d76d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tfx.orchestration.beam.beam_dag_runner.BeamDagRunner'>\n"
          ]
        }
      ],
      "source": [
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "print(BeamDagRunner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E1uRgY8yFG1",
        "outputId": "7984e0b7-cc3a-41f3-bd25-d8d9b61f2f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create a new pipeline in the given orchestrator.\n",
            "CLI\n",
            "Creating pipeline\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "Pipeline \"pipeline_beam_penguin_01\" created successfully.\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# Note that there is no name-parameter here. Instead, this \n",
        "# implicitly uses the value defined in /pipeline/configs.py:\n",
        "# PIPELINE_NAME = 'penguin_pipeline'\n",
        "# So, we have to make sure there is no naming-conflict with other pipelines.\n",
        "print(\"Create a new pipeline in the given orchestrator.\")\n",
        "!tfx pipeline create --engine={ENGINE} --pipeline_path={ORCHESTRATOR_FILE}\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOzefN-ByFG1",
        "outputId": "5425fce7-024f-4693-f9c1-4a83ae916c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfx_artifacts_beam_penguin_01\n",
            "└── tfx_metadata\n",
            "    └── pipeline_beam_penguin_01\n",
            "\n",
            "2 directories, 0 files\n",
            "\n"
          ]
        }
      ],
      "source": [
        "proc = subprocess.Popen(\n",
        "    [\"tree\", OUTPUT_ARTIFACTS_DIR, \"-I\", \"__pycache__|__init__.py\"], \n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "tree_00_tfx_artifacts = proc.communicate()[0]\n",
        "print(tree_00_tfx_artifacts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgS5hgptyFG2",
        "outputId": "91480b70-c3de-4e32-8aea-c8623d732c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLI\n",
            "Listing all pipelines\n",
            "------------------------------\n",
            "pipeline_beam_penguin_01\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Note that we have now registered this pipeline with the orchestrator:\n",
        "if DEBUG:\n",
        "    !tfx pipeline list --engine={ENGINE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8U6f-tdyFG3",
        "outputId": "d487c3b3-aa3f-4c63-8853-fdbf8db8cda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "files not the same\n"
          ]
        }
      ],
      "source": [
        "# Check that there are not files created after this step\n",
        "if DEBUG:\n",
        "    try:\n",
        "        assert tree_01_template_copy == tree_02_pipeline_create\n",
        "    except:\n",
        "        print(\"files not the same\")\n",
        "    \n",
        "# Hence, `tfx pipeline create --engine={ENGINE} --pipeline_path=local_runner.py` \n",
        "# does not create any artifacts, but instead registers with the appropriate orchestrator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReKM6iy1yFG3"
      },
      "source": [
        "**Note** Maybe we can just run `python xxxx_runner.py` on the CLI. Is this any faster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuCfqdreyFG4"
      },
      "source": [
        "## 11. Create a new run for a pipeline (tfx run create)\n",
        "\n",
        "Execute the created pipeline using `tfx run create` command.\n",
        "\n",
        "This creates a new run instance for a pipeline in the orchestrator. In other words, this  is when we actually run the TFX components defined in our project.\n",
        "\n",
        "* This creates \n",
        "\n",
        "```\n",
        "tfx_artifacts\n",
        "├── tfx_metadata\n",
        "│   └── pipeline_penguin_01\n",
        "│       └── metadata.db\n",
        "└── tfx_pipeline_output\n",
        "    └── pipeline_penguin_01\n",
        "        ├── CsvExampleGen\n",
        "        │   └── examples\n",
        "        │       └── 1\n",
        "        │           ├── Split-eval\n",
        "        │           │   └── data_tfrecord-00000-of-00001.gz\n",
        "        │           └── Split-train\n",
        "        │               └── data_tfrecord-00000-of-00001.gz\n",
        "        ├── SchemaGen\n",
        "        │   └── schema\n",
        "        │       └── 3\n",
        "        │           └── schema.pbtxt\n",
        "        └── StatisticsGen\n",
        "            └── statistics\n",
        "                └── 2\n",
        "                    ├── Split-eval\n",
        "                    │   └── FeatureStats.pb\n",
        "                    └── Split-train\n",
        "                        └── FeatureStats.pb\n",
        "\n",
        "17 directories, 6 files\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnTC_Rql42xZ",
        "outputId": "d319d3fa-4d09-4d1a-e9a4-73c6990c3ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create a new run instance for the pipeline in the beam orchestrator:\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "# Start an execution run with the newly created pipeline.\n",
        "# This contains three TFX components   # &> /dev/null\n",
        "print(f\"Create a new run instance for the pipeline in the {ENGINE} orchestrator:\")\n",
        "!tfx run create --engine={ENGINE} --pipeline_name={PIPELINE_NAME} &> /dev/null\n",
        "print(\"Finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZT32o2yFG5",
        "outputId": "9c73cf14-49aa-4db8-b454-faba567a6e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfx_artifacts_beam_penguin_01\n",
            "├── tfx_metadata\n",
            "│   └── pipeline_beam_penguin_01\n",
            "│       └── metadata.db\n",
            "└── tfx_pipeline_output\n",
            "    └── pipeline_beam_penguin_01\n",
            "        ├── CsvExampleGen\n",
            "        │   └── examples\n",
            "        │       └── 1\n",
            "        │           ├── Split-eval\n",
            "        │           │   └── data_tfrecord-00000-of-00001.gz\n",
            "        │           └── Split-train\n",
            "        │               └── data_tfrecord-00000-of-00001.gz\n",
            "        ├── SchemaGen\n",
            "        │   └── schema\n",
            "        │       └── 3\n",
            "        │           └── schema.pbtxt\n",
            "        └── StatisticsGen\n",
            "            └── statistics\n",
            "                └── 2\n",
            "                    ├── Split-eval\n",
            "                    │   └── FeatureStats.pb\n",
            "                    └── Split-train\n",
            "                        └── FeatureStats.pb\n",
            "\n",
            "17 directories, 6 files\n",
            "\n"
          ]
        }
      ],
      "source": [
        "proc = subprocess.Popen(\n",
        "    [\"tree\", OUTPUT_ARTIFACTS_DIR, \"-I\", \"__pycache__|__init__.py\"], \n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "tree_01_tfx_artifacts = proc.communicate()[0]\n",
        "print(tree_01_tfx_artifacts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvwrwxTwyFG5"
      },
      "source": [
        "---\n",
        "\n",
        "## 12. Add components and update the pipeline, creating new artifacts.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.1 Add components for data validation\n",
        "\n",
        "In this step, you will add components for data validation including StatisticsGen, SchemaGen, and ExampleValidator. If you are interested in data validation, please see Get started with Tensorflow Data Validation.\n",
        "\n",
        "**NOTE**\n",
        "\n",
        "Make these changes in  `{project_root}/pipeline/pipeline.py`\n",
        "\n",
        "```python\n",
        "components.append(example_gen) # already added\n",
        "\n",
        "components.append(statistics_gen)\n",
        "components.append(schema_gen)\n",
        "components.append(example_validator)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEh20mmNyFG6",
        "outputId": "7ff38767-d23f-44d7-e711-e17365995cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50: components.append(example_gen)\n",
            "55: components.append(statistics_gen)\n",
            "61: components.append(schema_gen)\n",
            "65: components.append(schema_gen)\n",
            "71: components.append(example_validator)\n"
          ]
        }
      ],
      "source": [
        "!sed -i {OSX_syntax} 's/\\# components.append(example_gen)/components.append(example_gen)/' {PIPELINE_FILE}\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(statistics_gen)/components.append(statistics_gen)/' {PIPELINE_FILE}\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(schema_gen)/components.append(schema_gen)/' {PIPELINE_FILE}\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(example_validator)/components.append(example_validator)/' {PIPELINE_FILE}\n",
        "\n",
        "# Show line number, use tr -s option to replace instances of repeated chars with a single char. \n",
        "# https://pubs.opengroup.org/onlinepubs/9699919799/utilities/tr.html#tag_20_132_04\n",
        "!grep -n 'components.append(example_gen)' {PIPELINE_FILE} | tr -s \" \"\n",
        "!grep -n 'components.append(statistics_gen)' {PIPELINE_FILE} | tr -s \" \"\n",
        "!grep -n 'components.append(schema_gen)' {PIPELINE_FILE} | tr -s \" \"\n",
        "!grep -n 'components.append(example_validator)' {PIPELINE_FILE} | tr -s \" \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu9mdwKGyFG6",
        "outputId": "ec10f669-f713-44a6-e7c6-a0421e6f09e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating pipeline:\n",
            "CLI\n",
            "Updating pipeline\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "Pipeline \"pipeline_beam_penguin_01\" updated successfully.\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# Update the pipeline with the modified pipeline definition.\n",
        "## How does this know which pipeline to run? By this variable, `--pipeline-path=local_runner.py`\n",
        "print(\"Updating pipeline:\")\n",
        "!tfx pipeline update --engine={ENGINE} --pipeline_path={ORCHESTRATOR_FILE}\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu6qZdJCyFG7"
      },
      "source": [
        "### 12.2 Run new pipeline instance (tfx run create)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaQqlUnyyFG7",
        "outputId": "8bf09c49-d0ca-4c9a-fdf7-0360aa64994d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running new pipeline instance:\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# Execute another run of the updated pipeline to create artifacts\n",
        "print(\"Running new pipeline instance:\")\n",
        "!tfx run create --engine={ENGINE} --pipeline_name {PIPELINE_NAME} &> /dev/null\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfSv1zOIyFG8",
        "outputId": "298c1272-50f5-4de7-d231-c1f4f958de89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfx_artifacts_beam_penguin_01\n",
            "├── tfx_metadata\n",
            "│   └── pipeline_beam_penguin_01\n",
            "│       └── metadata.db\n",
            "└── tfx_pipeline_output\n",
            "    └── pipeline_beam_penguin_01\n",
            "        ├── CsvExampleGen\n",
            "        │   └── examples\n",
            "        │       └── 1\n",
            "        │           ├── Split-eval\n",
            "        │           │   └── data_tfrecord-00000-of-00001.gz\n",
            "        │           └── Split-train\n",
            "        │               └── data_tfrecord-00000-of-00001.gz\n",
            "        ├── SchemaGen\n",
            "        │   └── schema\n",
            "        │       └── 3\n",
            "        │           └── schema.pbtxt\n",
            "        └── StatisticsGen\n",
            "            └── statistics\n",
            "                └── 2\n",
            "                    ├── Split-eval\n",
            "                    │   └── FeatureStats.pb\n",
            "                    └── Split-train\n",
            "                        └── FeatureStats.pb\n",
            "\n",
            "17 directories, 6 files\n",
            "\n"
          ]
        }
      ],
      "source": [
        "proc = subprocess.Popen(\n",
        "    [\"tree\", OUTPUT_ARTIFACTS_DIR, \"-I\", \"__pycache__|__init__.py\"], \n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "tree_02_tfx_artifacts = proc.communicate()[0]\n",
        "print(tree_02_tfx_artifacts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p7CLDvD42xe"
      },
      "source": [
        "---\n",
        "### 12.3 Add components for training\n",
        "\n",
        "In this step, you will add components for training and model validation including `Transform`, `Trainer`, `Resolver`, `Evaluator`, and `Pusher`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dapu9tdvyFG9",
        "outputId": "f8473e1c-ed82-486f-9cfd-42dfa9ab1dd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79: components.append(transform)\n",
            "92: components.append(trainer)\n",
            "102: components.append(model_resolver)\n",
            "135: components.append(evaluator)\n",
            "145: components.append(pusher)\n"
          ]
        }
      ],
      "source": [
        "# Uncomment these components\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(transform)/components.append(transform)/' {PIPELINE_FILE}\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(trainer)/components.append(trainer)/' {PIPELINE_FILE}\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(model_resolver)/components.append(model_resolver)/' {PIPELINE_FILE}\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(evaluator)/components.append(evaluator)/' {PIPELINE_FILE}\n",
        "!sed -i {OSX_syntax} 's/\\# components.append(pusher)/components.append(pusher)/' {PIPELINE_FILE}\n",
        "\n",
        "# Check the changes\n",
        "!grep -n 'components.append(transform)' {PIPELINE_FILE} | tr -s \" \"\n",
        "!grep -n 'components.append(trainer)' {PIPELINE_FILE} | tr -s \" \"\n",
        "!grep -n 'components.append(model_resolver)' {PIPELINE_FILE} | tr -s \" \"\n",
        "!grep -n 'components.append(evaluator)' {PIPELINE_FILE} | tr -s \" \"\n",
        "!grep -n 'components.append(pusher)' {PIPELINE_FILE} | tr -s \" \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik8JbnRq42xf",
        "outputId": "d7d0125c-a306-476f-dd81-21d6eb3b5554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating pipeline:\n",
            "CLI\n",
            "Updating pipeline\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
            "Pipeline \"pipeline_beam_penguin_01\" updated successfully.\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# Update the pipeline with the modified pipeline definition.\n",
        "print(\"Updating pipeline:\")\n",
        "#!tfx pipeline update --engine={ENGINE} --pipeline_path={ORCHESTRATOR_FILE} &> /dev/null\n",
        "!tfx pipeline update --engine={ENGINE} --pipeline_path={ORCHESTRATOR_FILE}\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcZ2VuihyFG-",
        "outputId": "6fcc3bb2-aadf-412a-8c1d-db8ce05b12d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time 40.6647 seconds\n",
            "----------------------------------------\n",
            "CPU process_time: 0.269693\n",
            "----------------------------------------\n",
            "CPU times: user 233 ms, sys: 37 ms, total: 270 ms\n",
            "Wall time: 40.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "tic = time.perf_counter()\n",
        "t1_start = process_time() \n",
        "\n",
        "# Execute another run of the updated pipeline to create artifacts\n",
        "!tfx run create --engine={ENGINE} --pipeline_name {PIPELINE_NAME} &> /dev/null\n",
        "\n",
        "toc = time.perf_counter()\n",
        "print(f\"Elapsed time {toc - tic:0.4f} seconds\")\n",
        "HR()\n",
        "\n",
        "t1_stop = process_time()\n",
        "print(f\"CPU process_time: {(t1_stop-t1_start):.6f}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRscQndgyFG_",
        "outputId": "bd2115e3-3bcc-4631-de48-e1256d37da21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "real\t0m21.819s\n",
            "user\t0m17.594s\n",
            "sys\t0m2.291s\n",
            "Elapsed time 21.9071 seconds\n",
            "----------------------------------------\n",
            "CPU process_time: 0.164955\n",
            "----------------------------------------\n",
            "CPU times: user 142 ms, sys: 22.9 ms, total: 165 ms\n",
            "Wall time: 21.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "tic = time.perf_counter()\n",
        "t1_start = process_time() \n",
        "# Execute another run of the updated pipeline to create artifacts\n",
        "#!tfx run create --engine={ENGINE} --pipeline_name {PIPELINE_NAME} &> /dev/null\n",
        "\n",
        "!time python {ORCHESTRATOR_FILE} >/dev/null 2>&1\n",
        "\n",
        "toc = time.perf_counter()\n",
        "print(f\"Elapsed time {toc - tic:0.4f} seconds\")\n",
        "HR()\n",
        "\n",
        "t1_stop = process_time()\n",
        "print(f\"CPU process_time: {(t1_stop-t1_start):.6f}\") \n",
        "HR()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YMA8uNQyFG_"
      },
      "source": [
        "This creates artifacts in serving_model, StatisticsGen, Trainer, Transform.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nks-NFO1yFHA",
        "outputId": "080b595a-afc2-4831-f0f2-68a3c678f6fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfx_artifacts_beam_penguin_01\n",
            "├── tfx_metadata\n",
            "│   └── pipeline_beam_penguin_01\n",
            "│       └── metadata.db\n",
            "└── tfx_pipeline_output\n",
            "    └── pipeline_beam_penguin_01\n",
            "        ├── CsvExampleGen\n",
            "        │   └── examples\n",
            "        │       └── 1\n",
            "        │           ├── Split-eval\n",
            "        │           │   └── data_tfrecord-00000-of-00001.gz\n",
            "        │           └── Split-train\n",
            "        │               └── data_tfrecord-00000-of-00001.gz\n",
            "        ├── Evaluator\n",
            "        │   ├── blessing\n",
            "        │   │   ├── 13\n",
            "        │   │   │   └── BLESSED\n",
            "        │   │   └── 21\n",
            "        │   │       └── BLESSED\n",
            "        │   └── evaluation\n",
            "        │       ├── 13\n",
            "        │       │   ├── attributions-00000-of-00001.tfrecord\n",
            "        │       │   ├── eval_config.json\n",
            "        │       │   ├── metrics-00000-of-00001.tfrecord\n",
            "        │       │   ├── plots-00000-of-00001.tfrecord\n",
            "        │       │   └── validations.tfrecord\n",
            "        │       └── 21\n",
            "        │           ├── attributions-00000-of-00001.tfrecord\n",
            "        │           ├── eval_config.json\n",
            "        │           ├── metrics-00000-of-00001.tfrecord\n",
            "        │           ├── plots-00000-of-00001.tfrecord\n",
            "        │           └── validations.tfrecord\n",
            "        ├── Pusher\n",
            "        │   └── pushed_model\n",
            "        │       ├── 14\n",
            "        │       │   ├── assets\n",
            "        │       │   ├── keras_metadata.pb\n",
            "        │       │   ├── saved_model.pb\n",
            "        │       │   └── variables\n",
            "        │       │       ├── variables.data-00000-of-00001\n",
            "        │       │       └── variables.index\n",
            "        │       └── 22\n",
            "        │           ├── assets\n",
            "        │           ├── keras_metadata.pb\n",
            "        │           ├── saved_model.pb\n",
            "        │           └── variables\n",
            "        │               ├── variables.data-00000-of-00001\n",
            "        │               └── variables.index\n",
            "        ├── SchemaGen\n",
            "        │   └── schema\n",
            "        │       └── 3\n",
            "        │           └── schema.pbtxt\n",
            "        ├── serving_model\n",
            "        │   ├── 1661262244\n",
            "        │   │   ├── assets\n",
            "        │   │   ├── keras_metadata.pb\n",
            "        │   │   ├── saved_model.pb\n",
            "        │   │   └── variables\n",
            "        │   │       ├── variables.data-00000-of-00001\n",
            "        │   │       └── variables.index\n",
            "        │   └── 1661262267\n",
            "        │       ├── assets\n",
            "        │       ├── keras_metadata.pb\n",
            "        │       ├── saved_model.pb\n",
            "        │       └── variables\n",
            "        │           ├── variables.data-00000-of-00001\n",
            "        │           └── variables.index\n",
            "        ├── StatisticsGen\n",
            "        │   └── statistics\n",
            "        │       └── 2\n",
            "        │           ├── Split-eval\n",
            "        │           │   └── FeatureStats.pb\n",
            "        │           └── Split-train\n",
            "        │               └── FeatureStats.pb\n",
            "        ├── Trainer\n",
            "        │   ├── model\n",
            "        │   │   └── 11\n",
            "        │   │       └── Format-Serving\n",
            "        │   │           ├── assets\n",
            "        │   │           ├── keras_metadata.pb\n",
            "        │   │           ├── saved_model.pb\n",
            "        │   │           └── variables\n",
            "        │   │               ├── variables.data-00000-of-00001\n",
            "        │   │               └── variables.index\n",
            "        │   └── model_run\n",
            "        │       └── 11\n",
            "        │           ├── train\n",
            "        │           │   └── events.out.tfevents.1661262219.911eea53f077.750.0.v2\n",
            "        │           └── validation\n",
            "        │               └── events.out.tfevents.1661262222.911eea53f077.750.1.v2\n",
            "        └── Transform\n",
            "            ├── post_transform_anomalies\n",
            "            │   └── 12\n",
            "            │       └── SchemaDiff.pb\n",
            "            ├── post_transform_schema\n",
            "            │   └── 12\n",
            "            │       └── schema.pbtxt\n",
            "            ├── post_transform_stats\n",
            "            │   └── 12\n",
            "            │       └── FeatureStats.pb\n",
            "            ├── pre_transform_schema\n",
            "            │   └── 12\n",
            "            │       └── schema.pbtxt\n",
            "            ├── pre_transform_stats\n",
            "            │   └── 12\n",
            "            │       └── FeatureStats.pb\n",
            "            ├── transformed_examples\n",
            "            │   └── 12\n",
            "            │       ├── Split-eval\n",
            "            │       │   └── transformed_examples-00000-of-00001.gz\n",
            "            │       └── Split-train\n",
            "            │           └── transformed_examples-00000-of-00001.gz\n",
            "            ├── transform_graph\n",
            "            │   └── 12\n",
            "            │       ├── metadata\n",
            "            │       │   └── schema.pbtxt\n",
            "            │       ├── transformed_metadata\n",
            "            │       │   └── schema.pbtxt\n",
            "            │       └── transform_fn\n",
            "            │           ├── assets\n",
            "            │           ├── saved_model.pb\n",
            "            │           └── variables\n",
            "            │               ├── variables.data-00000-of-00001\n",
            "            │               └── variables.index\n",
            "            └── updated_analyzer_cache\n",
            "                └── 12\n",
            "                    └── pipeline_beam_penguin_01-CsvExampleGen-examples-1-Split-train-STAR-5c3a85cf7279f56586bc8ba04d8e849b160a6066bde4e3baa559994d7cbfa01a\n",
            "                        ├── 0-00000-of-00001.gz\n",
            "                        ├── 1-00000-of-00001.gz\n",
            "                        ├── 2-00000-of-00001.gz\n",
            "                        ├── 3-00000-of-00001.gz\n",
            "                        └── MANIFEST\n",
            "\n",
            "74 directories, 57 files\n",
            "\n"
          ]
        }
      ],
      "source": [
        "proc = subprocess.Popen(\n",
        "    [\"tree\", OUTPUT_ARTIFACTS_DIR, \"-I\", \"__pycache__|__init__.py\"], \n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "tree_03_tfx_artifacts = proc.communicate()[0]\n",
        "print(tree_03_tfx_artifacts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60jPDRMIyFHB",
        "outputId": "03dc5597-b51b-47ac-a5a9-358026bd75b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLI\n",
            "Listing all pipelines\n",
            "No pipelines to display.\n"
          ]
        }
      ],
      "source": [
        "!tfx pipeline list --engine=local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEiSvqpcyFHC",
        "outputId": "6ca9d7c7-56dc-4a36-923d-4d0180b8ce90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLI\n",
            "Listing all pipelines\n",
            "------------------------------\n",
            "pipeline_beam_penguin_01\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "!tfx pipeline list --engine=beam"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "template_orchestrator_beam.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}